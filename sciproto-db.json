{
  "analyses": {
    "bdfaa68d8984f0dc02beaca527b76f207d99b666d31d1da728ee0728182df697": {
      "hash": "bdfaa68d8984f0dc02beaca527b76f207d99b666d31d1da728ee0728182df697",
      "filename": "1706.03762v7.pdf",
      "raw_text": "Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\n\n# Attention Is All You Need\n\nAshish Vaswani*\nGoogle Brain\navaswani@google.com\n\nNoam Shazeer*\nGoogle Brain\nnoam@google.com\n\nNiki Parmar*\nGoogle Research\nnikip@google.com\n\nJakob Uszkoreit*\nGoogle Research\nusz@google.com\n\nLlion Jones*\nGoogle Research\nllion@google.com\n\nAidan N. Gomez* †\nUniversity of Toronto\naidan@cs.toronto.edu\n\nŁukasz Kaiser*\nGoogle Brain\nlukaszkaiser@google.com\n\nIllia Polosukhin* ‡\nillia.polosukhin@gmail.com\n\n### Abstract\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n\n---\n\\* Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\n† Work performed while at Google Brain.\n‡ Work performed while at Google Research.\n\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7 [cs.CL] 2 Aug 2023\n\n---\n## 1 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].\n\nRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h_t, as a function of the previous hidden state h_{t-1} and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\n\nAttention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.\n\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n\n## 2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\n\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\n\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].\n\n## 3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x_1, ..., x_n) to a sequence of continuous representations z = (z_1, ..., z_n). Given z, the decoder then generates an output sequence (y_1, ..., y_m) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.\n\n2\n\n---\nFigure 1: The Transformer - model architecture.\n\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\n\n### 3.1 Encoder and Decoder Stacks\n\n**Encoder:** The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d_model = 512.\n\n**Decoder:** The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n\n### 3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n\n3\n\n---\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n\nof the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n\n#### 3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension d_k, and values of dimension d_v. We compute the dot products of the query with all keys, divide each by sqrt(d_k), and apply a softmax function to obtain the weights on the values.\n\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. We compute the matrix of outputs as:\n\nAttention(Q, K, V) = softmax( (Q * K^T) / sqrt(d_k) ) * V    (1)\n\nThe two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1 / sqrt(d_k). Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n\nWhile for small values of d_k the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d_k [3]. We suspect that for large values of d_k, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients^4. To counteract this effect, we scale the dot products by 1 / sqrt(d_k).\n\n#### 3.2.2 Multi-Head Attention\nInstead of performing a single attention function with d_model-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d_k, d_k and d_v dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d_v-dimensional\n\n---\n^4 To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q · k = sum_{i=1}^{d_k} q_i * k_i, has mean 0 and variance d_k.\n\n4\n\n---\noutput values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\n\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n\nMultiHead(Q, K, V) = Concat(head_1, ..., head_h) * W^O\nwhere head_i = Attention(Q * W_i^Q, K * W_i^K, V * W_i^V)\n\nWhere the projections are parameter matrices W_i^Q ∈ R^{d_model × d_k}, W_i^K ∈ R^{d_model × d_k}, W_i^V ∈ R^{d_model × d_v} and W^O ∈ R^{h * d_v × d_model}.\n\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use d_k = d_v = d_model / h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n\n#### 3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n* In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\n* The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n* Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to -∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\n\n### 3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n\nFFN(x) = max(0, x * W_1 + b_1) * W_2 + b_2    (2)\n\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is d_model = 512, and the inner-layer has dimensionality d_ff = 2048.\n\n### 3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d_model. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by sqrt(d_model).\n\n5\n\n---\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.\n\n| Layer Type | Complexity per Layer | Sequential Operations | Maximum Path Length |\n| :--- | :--- | :--- | :--- |\n| Self-Attention | O(n^2 · d) | O(1) | O(1) |\n| Recurrent | O(n · d^2) | O(n) | O(n) |\n| Convolutional | O(k · n · d^2) | O(1) | O(log_k(n)) |\n| Self-Attention (restricted) | O(r · n · d) | O(1) | O(n/r) |\n\n### 3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d_model as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\n\nIn this work, we use sine and cosine functions of different frequencies:\n\nPE_{(pos,2i)} = sin(pos / 10000^{2i/d_model})\nPE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_model})\n\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PE_{pos+k} can be represented as a linear function of PE_{pos}.\n\nWe also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n\n## 4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x_1, ..., x_n) to another sequence of equal length (z_1, ..., z_n), with x_i, z_i ∈ R^d, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\n\nOne is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\n\nThe third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\n\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\n\n6\n\n---\nlength n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.\n\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(log_k(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k · n · d + n · d^2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\n\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\n\n## 5 Training\nThis section describes the training regime for our models.\n\n### 5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n\n### 5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\n\n### 5.3 Optimizer\nWe used the Adam optimizer [20] with β_1 = 0.9, β_2 = 0.98 and ε = 10^{-9}. We varied the learning rate over the course of training, according to the formula:\n\nlrate = d_{model}^{-0.5} · min(step_num^{-0.5}, step_num · warmup_steps^{-1.5})    (3)\n\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\n\n### 5.4 Regularization\nWe employ three types of regularization during training:\n\n7\n\n---\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n\n| Model | BLEU EN-DE | BLEU EN-FR | Training Cost (FLOPs) EN-DE | Training Cost (FLOPs) EN-FR |\n| :--- | :---: | :---: | :---: | :---: |\n| ByteNet [18] | 23.75 | | | |\n| Deep-Att + PosUnk [39] | | 39.2 | | 1.0 · 10^{20} |\n| GNMT + RL [38] | 24.6 | 39.92 | 2.3 · 10^{19} | 1.4 · 10^{20} |\n| ConvS2S [9] | 25.16 | 40.46 | 9.6 · 10^{18} | 1.5 · 10^{20} |\n| MoE [32] | 26.03 | 40.56 | 2.0 · 10^{19} | 1.2 · 10^{20} |\n| Deep-Att + PosUnk Ensemble [39] | | 40.4 | | 8.0 · 10^{20} |\n| GNMT + RL Ensemble [38] | 26.30 | 41.16 | 1.8 · 10^{20} | 1.1 · 10^{21} |\n| ConvS2S Ensemble [9] | 26.36 | **41.29** | 7.7 · 10^{19} | 1.2 · 10^{21} |\n| Transformer (base model) | 27.3 | 38.1 | **3.3 · 10^{18}** | |\n| Transformer (big) | **28.4** | **41.8** | 2.3 · 10^{19} | |\n\n**Residual Dropout** We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P_{drop} = 0.1.\n\n**Label Smoothing** During training, we employed label smoothing of value ε_{ls} = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n\n## 6 Results\n### 6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate P_{drop} = 0.1, instead of 0.3.\n\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38].\n\nTable 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU^5.\n\n### 6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\n\n---\n^5 We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n\n8\n\n---\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\n\n| | N | d_{model} | d_{ff} | h | d_k | d_v | P_{drop} | ε_{ls} | train steps | PPL (dev) | BLEU (dev) | params ×10^6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| base | 6 | 512 | 2048 | 8 | 64 | 64 | 0.1 | 0.1 | 100K | 4.92 | 25.8 | 65 |\n| (A) | | | | 1 | 512 | 512 | | | | 5.29 | 24.9 | |\n| | | | | 4 | 128 | 128 | | | | 5.00 | 25.5 | |\n| | | | | 16 | 32 | 32 | | | | 4.91 | 25.8 | |\n| | | | | 32 | 16 | 16 | | | | 5.01 | 25.4 | |\n| (B) | | | | | 16 | | | | | 5.16 | 25.1 | 58 |\n| | | | | | 32 | | | | | 5.01 | 25.4 | 60 |\n| (C) | 2 | | | | | | | | | 6.11 | 23.7 | 36 |\n| | 4 | | | | | | | | | 5.19 | 25.3 | 50 |\n| | 8 | | | | | | | | | 4.88 | 25.5 | 80 |\n| | | 256 | | | 32 | 32 | | | | 5.75 | 24.5 | 28 |\n| | | 1024 | | | 128 | 128 | | | | 4.66 | 26.0 | 168 |\n| | | | 1024 | | | | | | | 5.12 | 25.4 | 53 |\n| | | | 4096 | | | | | | | 4.75 | 26.2 | 90 |\n| (D) | | | | | | | 0.0 | | | 5.77 | 24.6 | |\n| | | | | | | | 0.2 | | | 4.95 | 25.5 | |\n| | | | | | | | | 0.0 | | 4.67 | 25.3 | |\n| | | | | | | | | 0.2 | | 5.47 | 25.7 | |\n| (E) | | positional embedding instead of sinusoids | | | | | | 4.92 | 25.7 | |\n| big | 6 | 1024 | 4096 | 16 | | | 0.3 | | 300K | **4.33** | **26.4** | 213 |\n\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\n\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n\nIn Table 3 rows (B), we observe that reducing the attention key size d_k hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.\n\n### 6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].\n\nWe trained a 4-layer transformer with d_model = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\n\nWe performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\n\n9\n\n---\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\n\n| Parser | Training | WSJ 23 F1 |\n| :--- | :--- | :---: |\n| Vinyals & Kaiser el al. (2014) [37] | WSJ only, discriminative | 88.3 |\n| Petrov et al. (2006) [29] | WSJ only, discriminative | 90.4 |\n| Zhu et al. (2013) [40] | WSJ only, discriminative | 90.4 |\n| Dyer et al. (2016) [8] | WSJ only, discriminative | 91.7 |\n| Transformer (4 layers) | WSJ only, discriminative | 91.3 |\n| Zhu et al. (2013) [40] | semi-supervised | 91.3 |\n| Huang & Harper (2009) [14] | semi-supervised | 91.3 |\n| McClosky et al. (2006) [26] | semi-supervised | 92.1 |\n| Vinyals & Kaiser el al. (2014) [37] | semi-supervised | 92.1 |\n| Transformer (4 layers) | semi-supervised | **92.7** |\n| Luong et al. (2015) [23] | multi-task | 93.0 |\n| Dyer et al. (2016) [8] | generative | 93.3 |\n\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3 for both WSJ only and the semi-supervised setting.\n\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\n\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-Parser [29] even when training only on the WSJ training set of 40K sentences.\n\n## 7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n\nFor translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\n\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\n\nThe code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.\n\n**Acknowledgements** We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\n\n## References\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.\n\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017.\n\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016.\n\n10\n\n---\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\n\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.\n\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016.\n\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\n\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.\n\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\n\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\n\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832–841. ACL, August 2009.\n\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016.\n\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016.\n\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.\n\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017.\n\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017.\n\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.\n\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n\n11\n\n---\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\n\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159. ACL, June 2006.\n\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.\n\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.\n\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006.\n\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.\n\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.\n\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\n\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958, 2014.\n\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\n\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\n\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015.\n\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\n\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013.\n\n12\n\n---\n## Attention Visualizations\n\n[Figure showing attention heads for the sentence: \"It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult.\"]\n\nFigure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.\n\n13\n\n---\n[Figures showing attention heads for the sentence: \"The Law will never be perfect, but its application should be just - this is what we are missing, in my opinion.\"]\n\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word.\n\n14\n\n---\n[Figures showing attention heads for the sentence: \"The Law will never be perfect, but its application should be just - this is what we are missing, in my opinion.\"]\n\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\n\n15",
      "analysis_json": "{\"title\":\"Attention Is All You Need\",\"authors\":[\"Ashish Vaswani\",\"Noam Shazeer\",\"Niki Parmar\",\"Jakob Uszkoreit\",\"Llion Jones\",\"Aidan N. Gomez\",\"Łukasz Kaiser\",\"Illia Polosukhin\"],\"publication_year\":\"2017\",\"summary\":\"This seminal paper introduces the Transformer, a novel neural network architecture based entirely on attention mechanisms, dispensing with recurrence and convolutions. By utilizing self-attention to compute representations of input and output sequences, the Transformer allows for significantly greater parallelization and captures global dependencies between sequence elements regardless of their distance. The authors demonstrate that this architecture achieves state-of-the-art results on English-to-German and English-to-French translation tasks while requiring a fraction of the training time of the best existing recurrent or convolutional models.\",\"breakthrough_score\":100,\"breakthrough_reasoning\":\"This work is a paradigm shift in deep learning. It moved the field away from Recurrent Neural Networks (RNNs/LSTMs), which were the dominant standard for sequence modeling, to the Transformer architecture. This architecture forms the foundation of modern Large Language Models (including BERT, GPT series, and Claude), enabling massive parallelization and scaling that was previously impossible. It is arguably the most influential AI paper of the decade.\",\"key_claims\":[\"Sequence transduction can be performed efficiently without recurrence or convolution, using only attention mechanisms.\",\"The Transformer architecture allows for significantly more parallelization during training compared to recurrent models.\",\"Self-attention reduces the path length between any two positions in a sequence to O(1), making it easier to learn long-range dependencies.\",\"The model achieves state-of-the-art BLEU scores on WMT 2014 translation tasks at a fraction of the training cost of previous best models.\"],\"testable_hypotheses\":[{\"hypothesis\":\"The Transformer trains faster than recurrent architectures for the same task performance due to parallelization.\",\"how_to_test\":\"Train a Transformer and a competitive RNN (e.g., GNMT) on the same dataset (WMT 2014) and hardware, measuring the wall-clock time required to reach a specific BLEU score.\",\"expected_outcome\":\"The Transformer will reach the target BLEU score in significantly less time (paper cites 3.5 days on 8 GPUs vs weeks for competitors).\"},{\"hypothesis\":\"Self-attention mechanisms can capture long-distance linguistic dependencies better than recurrent models.\",\"how_to_test\":\"Evaluate the model on specific linguistic probes requiring long-term memory (e.g., subject-verb agreement across long clauses) and visualize attention weights to see if heads attend to the relevant distant tokens.\",\"expected_outcome\":\"Attention heads will show strong weights connecting related words regardless of the distance separating them in the sentence.\"},{\"hypothesis\":\"Sinusoidal positional encodings are sufficient to allow an attention-only model to utilize sequence order.\",\"how_to_test\":\"Train a Transformer with the proposed sinusoidal encodings versus one with no positional information and one with learned positional embeddings.\",\"expected_outcome\":\"The model with sinusoidal encodings will perform nearly identically to learned embeddings and significantly better than the model with no positional information.\"}],\"key_equations\":[{\"name\":\"Scaled Dot-Product Attention\",\"latex\":\"Attention(Q, K, V) = softmax(Q K^T / sqrt(d_k)) V\",\"description\":\"The core mechanism computing the weighted sum of values (V) based on the compatibility of queries (Q) and keys (K). The scaling factor prevents gradients from vanishing in the softmax.\",\"variables\":[{\"name\":\"Q\",\"description\":\"Matrix of Queries\",\"typical_range\":\"Matrix of size (sequence_length, d_k)\"},{\"name\":\"K\",\"description\":\"Matrix of Keys\",\"typical_range\":\"Matrix of size (sequence_length, d_k)\"},{\"name\":\"d_k\",\"description\":\"Dimension of the keys/queries\",\"typical_range\":\"64 (in base model)\"}]},{\"name\":\"Multi-Head Attention\",\"latex\":\"MultiHead(Q, K, V) = Concat(head_1, ..., head_h) W^O\",\"description\":\"Allows the model to jointly attend to information from different representation subspaces at different positions.\",\"variables\":[{\"name\":\"head_i\",\"description\":\"Individual attention head output\",\"typical_range\":\"Output of Attention function\"},{\"name\":\"h\",\"description\":\"Number of parallel attention heads\",\"typical_range\":\"8 or 16\"},{\"name\":\"W^O\",\"description\":\"Output projection matrix\",\"typical_range\":\"Learnable weights\"}]},{\"name\":\"Positional Encoding\",\"latex\":\"PE(pos, 2i) = sin(pos / 10000^(2i/d_model)); PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\",\"description\":\"Injects information about the relative or absolute position of tokens in the sequence since the model has no recurrence.\",\"variables\":[{\"name\":\"pos\",\"description\":\"Position of the token in the sequence\",\"typical_range\":\"0 to max_sequence_length\"},{\"name\":\"i\",\"description\":\"Dimension index\",\"typical_range\":\"0 to d_model/2\"},{\"name\":\"d_model\",\"description\":\"Dimensionality of the embeddings\",\"typical_range\":\"512 or 1024\"}]}],\"simulation_possibilities\":[{\"title\":\"Interactive Scaled Dot-Product Attention\",\"description\":\"A playground where users can input a small sentence (e.g., 'The cat sat'). The simulation visualizes the Query, Key, and Value vectors for each word. Users can manipulate the vectors or toggle the scaling factor (sqrt(d_k)) to see how it affects the Softmax distribution (sharpening vs. smoothing) and the final Attention output.\",\"complexity\":\"Medium\",\"variables\":[\"Input Sentence\",\"Vector Dimensionality (d_k)\",\"Scaling (On/Off)\"],\"expected_insights\":\"Users will intuitively grasp how dot products measure similarity and why scaling is crucial for gradient stability in high dimensions.\",\"visualization_type\":\"interactive\"},{\"title\":\"Positional Encoding Waveforms\",\"description\":\"A visualization of the sinusoidal positional encodings. The x-axis represents the dimension index, and the y-axis represents the token position. Users can hover over specific positions to see the unique vector added to that word embedding, illustrating how the model distinguishes 'dog bites man' from 'man bites dog'.\",\"complexity\":\"Low\",\"variables\":[\"Sequence Length\",\"d_model (Embedding Dimension)\"],\"expected_insights\":\"Shows that specific positions have unique, deterministic signatures and how low frequencies handle long distances while high frequencies handle local order.\",\"visualization_type\":\"chart\"},{\"title\":\"Multi-Head Attention Map\",\"description\":\"An interactive diagram showing a source sentence and a target sentence. Lines connect words based on attention weights. Users can toggle between different 'Heads' (1 through 8) to see that different heads focus on different linguistic features (e.g., one head links verbs to objects, another links pronouns to their referents).\",\"complexity\":\"High\",\"variables\":[\"Head Index (1-8)\",\"Layer Depth\"],\"expected_insights\":\"Demonstrates that the 'Multi-Head' mechanism allows the model to learn various types of linguistic relationships simultaneously (syntax, semantics, anaphora).\",\"visualization_type\":\"diagram\"},{\"title\":\"Parallelism vs. Recurrence Race\",\"description\":\"A side-by-side animation of an RNN processing a sentence token-by-token versus a Transformer processing the whole sentence at once. A counter tracks 'Sequential Operations' to highlight the O(n) vs O(1) difference.\",\"complexity\":\"Low\",\"variables\":[\"Sentence Length\"],\"expected_insights\":\"Visually proves why Transformers are vastly more efficient to train on modern GPU hardware compared to RNNs.\",\"visualization_type\":\"animation\"}],\"field\":\"Natural Language Processing\",\"related_fields\":[\"Deep Learning\",\"Machine Translation\",\"Sequence Modeling\",\"Computer Vision (Vision Transformers)\"],\"limitations\":[\"Computational complexity is quadratic O(n^2) with respect to sequence length, making it expensive for very long sequences.\",\"Potential loss of fine-grained resolution due to averaging attention-weighted positions (mitigated by Multi-Head Attention).\",\"Requires large amounts of data to train effectively compared to some inductive-bias heavy models.\"],\"difficulty_to_understand\":\"Advanced\",\"prerequisites\":[\"Neural Networks\",\"Matrix Multiplication\",\"Softmax Function\",\"Encoder-Decoder Architectures\",\"Word Embeddings\"]}",
      "created_at": 1770641444918
    },
    "arxiv-2602.06859": {
      "hash": "arxiv-2602.06859",
      "filename": "2602.06859.pdf",
      "raw_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\n\n# Zero-shot Generalizable Graph Anomaly Detection with Mixture of Riemannian Experts\n\nXinyu Zhao, Qingyun Sun, *Member, IEEE*, Jiayi Luo, Xingcheng Fu, Jianxin Li, *Senior Member, IEEE*\n\n**Abstract**—Graph Anomaly Detection (GAD) aims to identify irregular patterns in graph data, and recent works have explored zero-shot generalist GAD to enable generalization to unseen graph datasets. However, existing zero-shot GAD methods largely ignore intrinsic geometric differences across diverse anomaly patterns, substantially limiting their cross-domain generalization. In this work, we reveal that anomaly detectability is highly dependent on the underlying geometric properties and that embedding graphs from different domains into a single static curvature space can distort the structural signatures of anomalies. To address the challenge that a single curvature space cannot capture geometry-dependent graph anomaly patterns, we propose **GAD-MoRE**, a novel framework for zero-shot Generalizable Graph Anomaly Detection with a Mixture of Riemannian Experts architecture. Specifically, to ensure that each anomaly pattern is modeled in the Riemannian space where it is most detectable, GAD-MoRE employs a set of specialized Riemannian expert networks, each operating in a distinct curvature space. To align raw node features with curvature-specific anomaly characteristics, we introduce an anomaly-aware multi-curvature feature alignment module that projects inputs into parallel Riemannian spaces, enabling the capture of diverse geometric characteristics. Finally, to facilitate better generalization beyond seen patterns, we design a memory-based dynamic router that adaptively assigns each input to the most compatible expert based on historical reconstruction performance on similar anomalies. Extensive experiments in the zero-shot setting demonstrate that GAD-MoRE significantly outperforms state-of-the-art generalist GAD baselines, and even surpasses strong competitors that are few-shot fine-tuned with labeled data from the target domain.\n\n**Index Terms**—Graph Anomaly Detection, Mixture of Experts, Riemannian Graph Learning, Zero-shot Learning.\n\n## I. INTRODUCTION\n\nGRAPH Anomaly Detection (GAD) [1], [2] aims to identify irregular patterns in graph-structured data. Detecting such anomaly patterns is of significant practical importance in many real-world applications [3], as they often correspond to detrimental events and abnormal behaviors, such as financial fraud rings characterized by dense clique formations [4], [5] or misinformation campaigns exhibiting star-like spreading patterns [6], [7], [8], which can ultimately lead to severe financial losses and negative social consequences. Benefiting from the powerful representation learning capability of Graph Neural Networks (GNNs), numerous GAD methods incorporate GNNs into their frameworks [9], [10]. However, the majority of existing methods are designed under a restrictive assumption: training and inference are performed on the same graph. This paradigm inherently limits practical applicability, where the structural and attribute patterns that characterize anomalous behavior are not universal across graphs [2]. For example, the topology of a spam-bot community in a social network [6] can be entirely different from that of a financial fraud ring in a transaction graph [5]. As a result, models trained within a single domain tend to overfit to their specific anomaly patterns, failing to generalize to unseen graphs without costly retraining and labeling. This inability to transfer learned anomaly patterns across varying distributions constitutes a critical generalization challenge [2], [10].\n\nTo enable cross-graph generalization, recent works have begun to explore zero-shot or few-shot GAD models [16], [17], [18], [19]. While these approaches have made promising progress, they often overlook the critical issue that different anomaly patterns exhibit substantial geometric heterogeneity across graphs. As a result, their ability to generalize to new, unseen graph domains remains limited. Under this heterogeneity, representing all graphs in a single fixed-curvature space can introduce systematic embedding distortion, where certain structural relations are over-compressed or over-stretched, reducing separability and making subtle anomalies harder to distinguish from normal variability. Geometry acts as an inductive bias that shapes which structural properties are explicitly captured. For example, in hierarchical graphs, anomalous bridge nodes linking distant branches are naturally exposed as outliers in hyperbolic space with negative curvature [20]. By contrast, graphs with dense structures are better captured in spherical space, where anomalies typically manifest as isolated nodes deviating from clusters [13], [21]. Therefore, imposing a single fixed-curvature across all graphs ignores the geometric heterogeneity of anomaly patterns, potentially distorting anomaly representations. To examine the geometric heterogeneity of graph anomalies, we conduct a preliminary study on representative datasets, with the results shown in Figure 1. Specifically, we use a standard Graph Autoencoder (GAE) [22] to learn node embeddings in constant-curvature spaces with different curvature values, and then evaluate anomaly detection performance using classical unsupervised detectors, including K-Means [11], k-NN [12], and Isolation Forest [13]. As shown in Figure 1, performance varies substantially with the choice of curvature. For example, the Weibo [14] dataset shows a clear performance peak in Euclidean space (κ = 0.0), whereas the Citeseer and Cora [15] datasets consistently prefer positive curvature spaces, and the Amazon [7] dataset exhibits a preference for negative curvature. These results motivate that \n\n---\n*X. Zhao, Q. Sun, J. Luo, and J. Li are with the School of Computer Science and Engineering, Beihang University, Beijing 100191, China (Email: {xyzhao, sunqy, luojy, lijx}@buaa.edu.cn).*\n*X. Fu is with the Key Lab of Education Blockchain and Intelligent Technology, Ministry of Education, Guangxi Normal University, Guilin 541004, China (E-mail: fuxc@gxnu.edu.cn).*\n*Manuscript received December 1, 2025.*\n\narXiv:2602.06859v1 [cs.LG] 6 Feb 2026\n\n---\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\n\n![Fig. 1: The left part illustrates the projection of graph structures into Hyperbolic (κ < 0), Euclidean (κ = 0), and Spherical (κ > 0) spaces. It highlights that anomalous nodes (red), which are intrinsically hidden within the original graph topology, become significantly more separable and detectable from normal nodes (blue) once mapped into these Riemannian manifolds. The right part quantifies this effect by showing the average AUROC performance of representative GAD methods (including K-Means [11], k-NN [12], and Isolation Forest [13]) across varying curvature values κ ∈ [−1.0, 1.0] on four datasets: Amazon [7], Weibo [14], Cora [15], and CiteSeer [15].](figure_1.png)\n\ndifferent anomalies are more effectively detected in different Riemannian spaces, making fixed-curvature embeddings suboptimal for anomaly detection. Consequently, it is essential to develop an adaptive framework that can dynamically align the Riemannian representation with the anomaly patterns of each graph. To overcome the limitation that a single fixed Riemannian space cannot capture curvature-dependent anomaly patterns across different graphs, we propose **GAD-MoRE**, a novel framework for zero-shot generalizable Graph Anomaly Detection with a Mixture of Riemannian Experts architecture, explicitly designed to model geometric heterogeneity in graph anomaly patterns. Specifically, to ensure each anomaly pattern is modeled in the geometry where it is most detectable, we first employ a collection of specialized Riemannian expert networks within the MoE architecture. Unlike single-geometry models that distort structural patterns, each expert operates in a distinct curvature space, enabling distinct anomaly patterns to be captured in their suitable Riemannian space. Second, to transform raw node features into geometry-sensitive representations that are suitable for these Riemannian experts, we introduce an anomaly-aware multi-curvature feature alignment module. This component bridges the gap between geometry-agnostic attributes and Riemannian manifolds, producing robust representations compatible with diverse experts. Finally, to route each node to its most appropriate Riemannian expert, we design a memory-based dynamic router that adaptively assigns current input nodes according to each expert’s historical anomaly detection performance, thereby enhancing generalization to unseen data. Extensive experiments demonstrate that our proposed GAD-MoRE achieves state-of-the-art zero-shot GAD performance, notably surpassing strong competitors even when they are fine-tuned with labeled target domain data.\n\nOur contributions can be summarized as follows:\n* We show that graph anomalies exhibit geometric heterogeneity, where different anomaly patterns have different detectability in distinct Riemannian spaces, whereas a single fixed Riemannian space is inherently insufficient to capture them.\n* We propose a novel zero-shot generalizable graph anomaly detection framework named GAD-MoRE, which explicitly models the graph anomaly geometric heterogeneity via a mixture of Riemannian experts coupled with an anomaly-aware multi-curvature feature alignment module and a memory-based dynamic router.\n* Extensive experiments demonstrate that GAD-MoRE significantly outperforms state-of-the-art baselines in challenging zero-shot settings, even against competitors fine-tuned with target-domain labels.\n\n*Organization*: The subsequent sections of this paper are structured as follows. We begin by surveying the relevant literature in Section II. Next, Section III provides the formal problem definition and essential background. The technical architecture of the proposed GAD-MoRE is detailed in Section IV, followed by a thorough performance evaluation in Section V. We provide concluding remarks and future outlooks in Section VI.\n\n## II. RELATED WORK\n\n### A. Graph Anomaly Detection\nOur work focuses on node-level graph anomaly detection (GAD), which aims to identify anomalous nodes in graph-structured data [2]. Existing methods can be broadly categorized into supervised, unsupervised, and general methods.\n\n(1) *Supervised Methods*. Supervised methods formulate GAD as a node classification problem using graph neural networks (GNNs). These approaches fall into two main categories: discriminative and generative GNNs [10]. Discriminative models such as CARE-GNN [23] and GHRN [24] improve neighborhood aggregation and feature transformation to separate normal and anomalous nodes, leveraging techniques \n\n---\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\n\nlike edge selection [23] and edge synthesis [24]. Generative models such as GraphSMOTE [25], GraphMixup [26], and GODM [27] generate synthetic anomalies. They typically rely on feature interpolation [25], [26] or noise perturbation [27] to augment training data and mitigate label scarcity. *While these methods perform well when labeled data is available, they are constrained by the high cost of anomaly labels.*\n\n(2) *Unsupervised Methods*. Unsupervised methods do not require labeled anomalies and instead rely on proxy tasks to guide model learning [10]. Common strategies include graph reconstruction, graph contrastive learning, graph knowledge distillation, adversarial training, and score prediction. Reconstruction-based models such as DOMINANT [28], AnomalyDAE [29], and GUIDE [30] reconstruct node features or graph structures, with reconstruction errors serving as anomaly scores. Contrastive learning methods such as DGI [31], InfoGraph [32], and CoLA [21] train models to separate similar and dissimilar samples to improve anomaly discrimination. Knowledge distillation models such as GLADST [33] transfer stable representations from teacher to student models. Adversarial methods like AEGIS [34] and GAAN [35] use generator-discriminator frameworks to simulate challenging examples and improve generalization. Score prediction models, including Meta-GDN [36] and SAD [37], directly learn node-level anomaly scores within a unified framework. These approaches perform well without supervision and have benefited from recent advances in deep learning, but still face challenges in aligning proxy tasks with anomaly objectives and modeling the structural and attribute heterogeneity inherent across graphs.\n\n(3) *Generalizable Methods*. Most existing GAD methods are designed for specific graphs and depend heavily on task-specific training, making them difficult to transfer across graph domains. They often assume consistent structural patterns and anomaly types, limiting their effectiveness in real-world scenarios where graphs differ widely in topology, semantics, and context. Moreover, such models typically require retraining on each new graph, which is computationally expensive and often impractical. To address these limitations, generalizable GAD methods [16], [17], [18], [19] aim to support anomaly detection across diverse graph domains, enabling few-shot or zero-shot transfer. ARC [16] applies contextual learning to adapt models with limited target labels, but struggles in fully zero-shot scenarios. UNPrompt [17] introduces a unified neighborhood prompting mechanism to detect anomalies without labels, although its performance is highly sensitive to the source graph structure. More recently, AnomalyGFM [18] proposes a graph foundation architecture. This approach distinguishes between normal and anomalous categories by mapping node residual errors into a common embedding space to match domain-invariant prototypes. Parallelly, IA-GGAD [19] advances this frontier by learning invariant representations and structure-insensitive affinities, aiming to mitigate the impact of domain-specific structural noise. These approaches offer promising generalization capability but remain vulnerable to structural inconsistencies and distribution shifts across domains. *However, none of these approaches explicitly address the intrinsic geometric heterogeneity across graphs, which we find is essential for generalizable GAD.*\n\n### B. Mixture of Experts for Graph Learning\nMixture-of-Experts (MoE) architecture [38], [39] leverages a gating network to dynamically assign inputs to specialized expert models, thereby enabling conditional computation and scalable model capacity [40]. This design allows different experts to focus on distinct data characteristics while maintaining computational efficiency. While MoE has achieved notable success in large-scale language and vision models [41], [42], [43], its adoption in graph learning remains relatively limited, though recent efforts suggest a growing interest in this direction. Recent studies [44], [45], [46] in graph learning have explored MoE frameworks to address structural and distributional heterogeneity inherent in graph data. For example, GraphDIVE [47] and G-FAME [46] route minority or under-represented nodes to dedicated experts, aiming to improve performance under data imbalance and fairness constraints. Other works implicitly adopt the MoE philosophy by varying message-passing depths or spectral filters, effectively treating different configurations as expert modules to diversify receptive fields and capture multi-scale graph structures. Beyond structural routing, MoW [45] decouples feature and structure modalities to enable specialized processing, whereas GraphMoRE [44] introduces curvature-aware experts to adapt to geometric variation across graph representations. In the context of anomaly detection, MoEGAD [48] presents the first exploration of MoE architecture in Graph-Level Anomaly Detection, leveraging a latent MoE module and iterative \n\nTABLE I: Summary of Key Notations.\n| Symbol | Definition |\n| :--- | :--- |\n| **Graph Structure & Attributes** | |\n| G | An attributed graph. |\n| V | The set of nodes. |\n| E | The set of edges. |\n| **A** | Adjacency matrix, **A** ∈ {0, 1}NxN. |\n| N | The number of nodes. |\n| D | The dimension of the unified node features. |\n| **X0** | The raw node feature matrix with original dimensions. |\n| **X** | The unified node feature matrix, **X** ∈ RNxD. |\n| **Anomaly Detection Task** | |\n| Va | The set of anomalous nodes. |\n| Vn | The set of normal nodes. |\n| **y** | The ground-truth label vector, **y** ∈ {0, 1}N. |\n| f | An anomaly scoring function, f : V → R. |\n| Dtrain | The set of source graphs for training. |\n| Dtest | The set of unseen target graphs for testing. |\n| **Model Concepts** | |\n| M | A Riemannian manifold. |\n| κ | The constant sectional curvature of a manifold. |\n| TpM | The tangent space at point p on the manifold M. |\n| K | The number of experts in the MoE model. |\n| Ei | The i-th expert network in the MoE model. |\n| G | The gating network in the MoE model. |\n| **Riemannian Geometry** | |\n| dE(p, q) | The Euclidean distance between points p and q. |\n| dH(p, q) | The hyperbolic distance between points p and q. |\n| dS(p, q) | The spherical distance between points p and q. |\n| expp | The exponential map at point p on the manifold. |\n| logp | The logarithmic map at point p on the manifold. |\n\n---\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\n\nanomalous graph generation to overcome the scarcity of labeled anomalies. Despite these advances, existing MoE-based graph methods primarily focus on supervised tasks such as node classification within a single graph domain. In contrast, the potential of a geometry-aware MoE framework for the distinct and more challenging problem of domain-general graph anomaly detection remains largely unexplored. In this work, we investigate how expert specialization and gating mechanisms can be leveraged to bridge distribution shifts across domains and enable zero-shot GAD without requiring exhaustive retraining.\n\n### C. Riemannian Graph Learning\nMany real-world graphs exhibit hierarchical or tree-like organizations, which are inherently difficult to model in Euclidean space [49]. As a result, Riemannian graph learning [50], [51] has emerged as a powerful and principled paradigm that embeds graph data into curved manifolds, most notably hyperbolic spaces, to capture these intrinsic geometric properties with significantly lower distortion [52], [53], [43]. By generalizing GNN operations, such as message passing and aggregation, to these curved spaces, hyperbolic GNNs [54], [55] are able to more faithfully encode structural priors and relational patterns present in complex graphs. While these geometric methods have proven effective, their application has predominantly focused on supervised learning within a single graph domain. The potential of leveraging diverse geometric priors from multiple curvature spaces for domain-general anomaly detection remains largely unexplored. In this work, we propose a mixture of Riemannian experts architecture for generalizable GAD, where each expert operates in a distinct Riemannian manifold, enabling the model to effectively identify anomalies across graphs.\n\n## III. PRELIMINARIES\n\nThis section formally defines the core problem and notations in this paper, and briefly reviews the necessary background on curvature manifolds.\n\n### A. Notations and Problem Formulation\n*Key Notations*: Our research focuses on the task of anomaly detection at the node level. To facilitate a clear understanding of the framework, Table I organizes the essential symbols employed throughout this study. Formally, we represent an attributed graph as G = (V, E, X), where the components V, E, and **A** correspond to the node set, the set of edges, and the adjacency matrix, respectively. The node set V is partitioned into anomalous nodes Va and normal nodes Vn, where |Va| ≪ |Vn|. The goal of Graph Anomaly Detection (GAD) is to learn a scoring function f : V → R such that f(va) > f(vn) for va ∈ Va, vn ∈ Vn.\n\n*Problem Formulation*: Our work focuses on the generalizable GAD paradigm. A model is trained on a set of source graphs Dtrain = {G(1), . . . , G(M)} and evaluated on a set of unseen target graphs Dtest = {G(M+1), . . . , G(P)}, where Dtrain ∩ Dtest = ∅. We address the zero-shot setting, where no labeled data or fine-tuning on the target graphs is permitted. To ensure a unified model input, raw feature matrices **X0(i)** with varying dimensionalities across different datasets are projected into a shared D-dimensional feature space **X** ∈ RNxD.\n\n### B. Constant Curvature Manifolds\nOur method leverages Riemannian manifolds with constant curvature (κ) to model diverse graph structures. Specifically, we consider three types of manifolds, as described below:\n* Euclidean Space (κ = 0): A flat Riemannian space equipped with the ℓ2-norm distance, defined as:\ndE(p, q) = ||p − q||2. (1)\n\n* Hyperbolic Space (κ < 0): A negatively curved hyperbolic space is well suited for modeling hierarchical and tree-like structures. Under the Poincaré ball model, the distance between two points is defined as:\ndH(p, q) = (1 / sqrt(-κ)) * arccosh(1 + (2 * ||p - q||^2) / ((1 + κ * ||p||^2) * (1 + κ * ||q||^2))). (2)\n\n* Spherical Space (κ > 0): A positively curved spherical space that is well-suited for modeling cyclical or periodic structures. The distance is defined as:\ndS(p, q) = (1 / sqrt(κ)) * arccos(<p, q>). (3)\n\nThe choice of curvature κ fundamentally influences the geometric properties of the embedding space and its suitability for modeling different graph structures.\n\n## IV. THE PROPOSED GAD-MORE FRAMEWORK\n\nGraph Anomaly Detection (GAD) faces a critical generalization challenge: models trained on specific graphs struggle to transfer learned anomaly patterns to unseen graphs due to substantial shifts in topology and anomaly characteristics. As demonstrated in Section I, graph anomaly patterns exhibit geometric heterogeneity, i.e., variability in underlying curvature across graph domains. Enforcing diverse graph structures into a single fixed-curvature space inevitably distorts the distinct structural patterns that define anomalies, hindering discrimination between normal and anomalous nodes. Geometry acts as a critical inductive bias: performance is highly sensitive to the chosen Riemannian space, with different datasets exhibiting distinct curvature preferences, and no single geometry proves universally optimal.\n\nTo overcome the limitations of single-curvature representation, we propose GAD-MoRE, a novel Geometric-aware framework for zero-shot Generalizable Graph Anomaly Detection with a Mixture-of-Experts architecture that integrates three coordinated components to effectively resolve the conflicts arising from geometric heterogeneity. The overall architecture is illustrated in Figure 2, which is coupled with three critic modules:\n* **Anomaly-aware Multi-curvature Feature Alignment**: As the foundational step, this module projects and aligns raw node features from various sources and dimensions into a unified geometry-aware feature space. By processing features in\n\n---\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\n\n![Fig. 2: The overall architecture of the GAD-MoRE framework. (1) The Anomaly-aware Multi-curvature Feature Alignment module projects raw features into a unified, geometry-aware representation. (2) The Mixture of Riemannian Experts Scorer utilizes a GNN backbone to encode structural context and employs specialized Riemannian expert networks. (3) The Memory-based Dynamic Router adaptively assigns nodes to these experts based on historical reconstruction quality to compute the final anomaly score.](figure_2.png)\n\nparallel across multiple curvature manifolds, it produces representations enriched with diverse geometric information.\n* **Memory-based Dynamic Router**: This module assigns each node to the most suitable Riemannian expert based on current structural features together with previously learned patterns, thereby improving expert assignment on unseen data and enhancing overall generalization.\n* **Mixture of Riemannian Experts Scorer**: This module utilizes a lightweight graph encoder to capture structural context and employs a collection of specialized Riemannian expert networks. Each expert operates in a unique curvature space, enabling the model to capture disparate anomaly patterns without geometric distortion.\n\nTogether, these modules enable GAD-MoRE to dynamically adapt its geometric lens to the specific anomaly characteristics of each node, enabling effective zero-shot generalizable graph anomaly detection across diverse domains.\n\n### A. Anomaly-aware Multi-curvature Feature Alignment\nRaw node features extracted from real-world graphs are inherently geometry-agnostic, as they exist in a flat Euclidean space without explicit geometric information. Yet, to effectively capture diverse structural patterns without distortion, our Riemannian expert networks must operate in distinct constant-curvature spaces.\n\nThis creates a fundamental mismatch: the Riemannian experts in MoE require manifold-aware inputs, while raw features lack geometric semantics. Simply feeding Euclidean features directly to curved-space experts is geometrically inconsistent and fails to leverage the representational advantages of non-Euclidean geometries. Moreover, different graph domains exhibit preferences for different geometric curvatures, as demonstrated empirically in Section I.\n\nTo construct robust and generalizable inputs for Riemannian experts, we explicitly embed signals from multiple Riemannian spaces into the initial node features, preserving both the rich attribute information and global structural consistency.\n\nTo address this challenge, we introduce a feature alignment module that explicitly projects raw features into diverse Riemannian spaces before feeding them to the experts. Given a raw feature matrix **X0**, the module performs a parallel transformation into a set of diverse Riemannian spaces, {Mκc}Cc=1. This set includes Euclidean space (κ = 0) alongside multiple curved spaces characterized by different constant negative (κ < 0) and positive curvatures (κ > 0). The total target feature dimension D is partitioned across these C spaces, such that D = ΣCc=1 Dc, where Dc is the final dimensionality assigned to the c-th space. For curved spaces, the raw features are first mapped into the tangent space at the origin before further processing. To ensure that the features selected for each Riemannian space are both informative and dimensionally consistent, we employ a two-stage selection process:\n\n**Stage 1: Dimensionality Reduction.** To construct geometry-specific feature candidates for each target manifold Mκc, we first project the raw features **X0** into C parallel constant-curvature manifolds {Mκc}Cc=1. Specifically, for curved spaces, the raw attributes are mapped into the tangent space at the origin ToMκc via the logarithmic map, thereby transforming geometry-agnostic features into manifold-aware representations. Then, we perform Principal Component Analysis (PCA) [56], [57] on these tangent-space features to extract \n\n---\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\n\nthe principal components corresponding to each specific c-th curvature space. This step identifies the principal components of the feature space, capturing global high-variance attribute signals that are most informative for node discrimination.\n\n**Stage 2: Laplacian Feature Selection.** We then compute the Laplacian score for each feature fj in the intermediate set **X'c**:\nL(fj) = f_j^T (I - DG^-1/2 A DG^-1/2) f_j, (4)\nwhere DG = diag(**A1**). Note that we omit the denominator term f_j^T DG fj commonly used in the classical definition, consistent with our implementation. We then select the top Dc features with the lowest scores to form the final feature set for that space, **X_c^final** ∈ RNxDc. This selection ensures that the chosen features are smooth and consistent with the local graph topology, thus preserving crucial structural information essential for anomaly detection.\n\nIdeally, PCA extracts high-variance attribute signals, while the subsequent Laplacian Score selection enforces structural consistency, yielding features that are both informative and geometrically meaningful. Finally, the selected features from all Riemannian spaces are concatenated to form a unified geometry-aware feature matrix **X** = CONCAT(**X_1^final**, . . . , **X_C^final**) ∈ RNxD.\n\nThe multi-curvature feature alignment module bridges the gap between geometry-agnostic raw features and the geometry-aware inputs required by the Riemannian expert networks. The resulting unified feature representation explicitly encodes signals from multiple curvature spaces while preserving both attribute richness and structural coherence, providing a robust foundation for the subsequent Riemannian experts to capture diverse graph anomaly patterns.\n\n### B. Mixture of Riemannian Experts Scorer\nDifferent graph domains exhibit fundamentally different structural characteristics that are best captured in distinct Riemannian spaces. For instance, hierarchical graphs with tree-like structures are naturally suited for hyperbolic geometry, while graphs with dense, cyclical communities align better with positively curved spherical spaces. When anomalies are defined as deviations from these underlying structural patterns, enforcing all graphs into a single fixed-curvature space induces critical distortions. In a hyperbolic space, dense communities may explode and distort, making anomalous isolated nodes indistinguishable from the distorted normal nodes. Conversely, in a spherical space, hierarchical structures may be compressed, obscuring bridge nodes that improperly connect distant branches. This geometric mismatch makes it extremely difficult to distinguish subtle structural anomalies from normal variations. Effectively capturing diverse structural patterns without distortion requires an adaptive mechanism that represents nodes in their most suitable Riemannian space. This necessitates a unified architecture capable of accommodating multiple, potentially conflicting Riemannian representations. In the zero-shot setting, anomaly patterns across graph domains prefer different Riemannian representations, so a single learned curvature becomes a compromise, whereas a MoE formulation enables adaptive geometric selection.\n\n**Algorithm 1** The overall training pipeline of GAD-MoRE.\n**Require:** Source graphs Dtrain, number of experts K, active experts k, GNN hops L, loss weights {λj}5j=1, cold-start epochs Ecold.\n**Ensure:** Trained model parameters Θ and expert memories {Bj}Kj=1.\n1: /* Offline Preprocessing */\n2: **for** i = 1, . . . , M **do**\n3: Align features: **X(i)** ← MCFA(**X0(i)**, **A(i)**).\n4: Propagate structure: {**H(l)**}Ll=0 ← Prop(**X(i)**, **A(i)**, L).\n5: **end for**\n6: /* Model Training */\n7: Initialize expert memories Bj ← ∅ for j = 1, . . . , K.\n8: **for** epoch = 1, . . . , Etotal **do**\n9: **for each** G(i) ∈ Dtrain **do**\n10: /* Forward Pass */\n11: Encode structural context: **Hres** ← GNN({**H(l)**}Ll=0).\n12: Compute raw logits **s** for all v ∈ V by Eq. (8).\n13: Apply training noise: s~ ← **s** + Noise(·).\n14: Select top-k experts S(v) and weights **wv** by Eq. (6).\n15: Reconstruct embeddings **H^** by Eq. (7).\n16: Decode outputs: **X^** ← Dec(**H^**), **A^** ← σ(**H^ H^T**).\n17: /* Compute Losses */\n18: Compute reconstruction losses Lembed, Lfeat, Lstruct.\n19: Compute Lcon by Eq. (17).\n20: Compute Lgate by Eq. (18).\n21: Ltotal ← Σ5j=1 λj Lj by Eq. (19).\n22: /* Optimization */\n23: Update parameters: Θ ← Θ − η∇Θ Ltotal.\n24: /* Dynamic Memory Update */\n25: **if** epoch > Ecold **then**\n26: **for** j = 1, . . . , K **do**\n27: Compute quality scores q(v, j) by Eq. (11).\n28: Add high-quality samples to Bj by Eq. (12).\n29: **end for**\n30: **end if**\n31: **end for**\n32: **end for**\n\nNode embeddings, which encode both multi-curvature attributes and structural context from the GNN backbone, are fed into the central innovation of GAD-MoRE: the mixture of Riemannian experts scorer. The GNN backbone first produces multi-hop feature matrices {**H(0)**, . . . , **H(Khops)**} where **H(0)** = **X**. A shared MLP Φ transforms {**H(0)**, . . . , **H(Khops)**} into {**X'k** = Φ(**H(k)**)}. The final node embedding **hv** is then constructed using a residual formulation to capture structural deviations, defined as **hv** = CONCAT(**X'1(v)** − **X'0(v)**, . . . , **X'Khops(v)** − **X'0(v)**). This residual formulation is designed to explicitly capture how a node’s representation evolves as structural information is aggregated from progressively larger neighborhoods. Large deviations between a node’s initial features (**X'0**) and its aggregated features (**X'k**) indicate structural irregularity, providing a strong signal for \n\n---\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\n\nanomaly detection. This embedding **hv** is then input into the MoE scorer.\n\nTo simultaneously accommodate multiple Riemannian representations, we employ a Mixture-of-Experts (MoE) architecture with specialized Riemannian Expert Networks. The MoE model consists of a set of K expert networks, {Ei}Ki=1. Each expert is a Riemannian expert, a small neural network designed to operate within a specific Riemannian manifold Mκi with learnable curvature κi. Each expert Ei takes a node embedding **hv** from the Euclidean tangent space and computes its reconstruction **h^(i)v**. This process involves mapping the input onto the manifold, processing it through manifold-aware layers, and mapping the result back to the tangent space:\nh^(i)v = log_oi (Ei(exp_oi(hv))), (5)\nwhere exp_oi and log_oi are the exponential and logarithmic maps at the origin oi of the manifold Mκi. This design allows different experts to specialize in reconstructing nodes that conform to different geometric priors, effectively capturing the diverse structural patterns present across graph domains. Following the practice of modern MoE, we employ sparse top-k routing to enable efficient conditional computation. For each node, the router first computes the gating weights **g(hv)** from the raw logits **s(hv)**:\ng(hv) = Softmax(s(hv) / τ), (6)\nwhere the temperature is controlled by the parameter τ. From the computed probabilities, the top-k experts with maximal influence are selected for the active set S(v). The final output **h^v** is computed as a weighted linear combination of outputs from these chosen experts, re-normalized among the k candidates:\nh^v = Σ_{i∈S(v)} (exp(si(hv)/τ) / Σ_{j∈S(v)} exp(sj(hv)/τ)) * h^(i)v. (7)\n\nThis mechanism allows for expanded model capacity while keeping the per-node resource consumption manageable. As a result, GAD-MoRE is well-suited for large-scale applications, offering a scalable solution that still captures intricate geometric patterns effectively.\n\nBy deploying experts in distinct curvature spaces and adaptively selecting among them, GAD-MoRE represents nodes in their optimal geometric configuration, avoiding distortions inherent to fixed-geometry approaches and enabling accurate and generalizable detection of structural anomalies across diverse graph domains.\n\n### C. Memory-based Dynamic Router\nWith specialized Riemannian Expert Networks, each tailored to distinct structural patterns under its optimal curvature, the central routing challenge is to assign each node to the most suitable Riemannian expert for accurate anomaly scoring. Conventional routing mechanisms in MoE models typically rely on learned gating networks that predict expert assignments based on input features alone. For anomaly detection, however, the routing policy must be tightly coupled to the reconstruction \n\n**Algorithm 2** Inference Procedure of GAD-MoRE\n**Require:** Test graph Gtest(**X0^test**, **A^test**), trained model Θ, expert memories {Bj}Kj=1.\n**Ensure:** Anomaly scores S ∈ RNtest.\n1: /* Preprocessing */\n2: Align features: **X^test** ← MCFA(**X0^test**, **A^test**) by Eq. (4).\n3: Propagate structure: {**H(l)**}Ll=0 ← Prop(**X^test**, **A^test**, L).\n4: /* Encoding & Routing */\n5: Encode structural context: **Hres** ← GNN({**H(l)**}Ll=0).\n6: **for each** node v ∈ Vtest **do**\n7: Compute logits **sv**: sv,j ← − min_{m∈Bj} dMj(...) by Eq. (8).\n8: Calculate gating weights **gv** and select top-k set S(v) by Eq. (6).\n9: **end for**\n10: /* Reconstruction */\n11: Reconstruct embeddings: **H^** ← Σv Σ_{j∈S(v)} wv,j * Ej(hv,res) by Eq. (7).\n12: /* Anomaly Scoring */\n13: **for each** node v ∈ Vtest **do**\n14: Calculate score: S(v) ← ||hv − h^v||2 by Eq. (20).\n15: **end for**\n16: **return** S.\n\nobjective: nodes should be directed to experts who can reconstruct their patterns with high fidelity, as reconstruction error directly determines the anomaly score. Without this alignment, the router may assign nodes to geometrically inappropriate experts, leading to unreliable anomaly scores and degraded detection performance. Moreover, a node’s optimal Riemannian space depends on its historical reconstruction quality across different experts, which is not captured by feature-based routing alone. This motivates a routing mechanism capable of adaptively selecting experts based on their demonstrated ability to reconstruct similar patterns, thereby ensuring that the routing policy directly aligns with the anomaly detection objective.\n\nTo enable reconstruction quality-based routing, we propose a memory-based dynamic router that makes routing decisions based on historical performance patterns. Each expert Ei is equipped with an expert memory bank Bi, which stores a collection of node embeddings {mi,1, mi,2, . . . } that the expert has previously reconstructed with high fidelity. These memory banks serve as geometric prototypes, representing the patterns that each expert specializes in handling. For a given node embedding **hv**, the router computes its similarity to each memory bank. The raw routing logit si(hv) for expert i is defined as the negative squared minimum manifold distance from **hv** to any embedding in the memory bank Bi:\nsi(hv) = − min_{m∈Bi} dMi(exp_oi(hv), exp_oi(m)), (8)\nwhere dMi(·, ·) is the distance metric on the manifold of expert i. This score measures how close node v is to the patterns that expert i is specialized in handling based on its historical reconstruction performance.\n\nExpert Memory Banks are not static but are populated and refined dynamically throughout the training process to store a representative set of node embeddings that each expert reconstructs with high fidelity. The update mechanism is governed by a multi-stage strategy involving a cold-start period, a \n\n---\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\n\ndynamic quality gate, and a strategic replacement policy. The update process is triggered after each training batch. To allow the expert networks to stabilize before populating memory banks, we implement a cold-start period during which memory banks remain inactive. This prevents the storage of low-quality reconstructions from poorly initialized experts. After the cold-start period, for each expert Ei, we calculate a quality score q(v, i) for each candidate node embedding **hv** assigned to it. Let Lrecon(v, i) = ||hv − h^(i)v||2^2 be the reconstruction error, where h^(i)v is the reconstruction by expert Ei. We define the minimum and maximum errors within the current batch Bbatch as:\nLmin(i) = min_{v'∈Bbatch} Lrecon(v', i), (9)\nLmax(i) = max_{v''∈Bbatch} Lrecon(v'', i). (10)\nThe normalized quality score is:\nq(v, i) = 1 − (Lrecon(v, i) - Lmin(i)) / (Lmax(i) - Lmin(i) + ε). (11)\n\nA dynamic quality gate is then applied to filter these candidates and select a high-quality subset Ci* exceeding a progressively stricter threshold τq(e) defined as a function of the current training epoch e:\nCi* = {hv | q(v, i) ≥ τq(e)}. (12)\nThe threshold τq(e) increases linearly as training progresses, ensuring that the memory banks are populated with examples of progressively higher quality:\nτq(e) = τmin + (τmax − τmin) * min(1, (e - Ecold) / (Etotal - Ecold)), (13)\nwhere τmin and τmax are the start and end thresholds, and Ecold and Etotal are the number of cold-start and total training epochs, respectively. To handle batches with universally low quality, the gate adaptively falls back to a percentile-based threshold. Once high-quality candidates in Ci* are selected, they are added to the memory bank Bi. If a bank is full, a hysteretic replacement policy is enacted: a new candidate will only replace the existing embedding with the lowest quality score if its own quality surpasses it by a significant margin. This prevents noisy fluctuations and ensures memory stability. To avoid domination by any single batch, the number of embeddings added per update is also capped.\n\nBy adaptively assigning nodes based on historical reconstruction quality rather than feature similarity alone, the routing policy aligns expert selection with the anomaly detection objective, directing each node to the Riemannian space where it is best modeled.\n\nFrom a modeling perspective, the proposed memory-based dynamic router provides a principled way to select experts based on reconstruction behavior, rather than relying on feature-based gating alone. Each Riemannian expert can be viewed as capturing a specific notion of normality under a given curvature prior, while its memory bank summarizes the regions of the embedding space where this expert has consistently achieved low reconstruction error. By routing nodes according to historical reconstruction fidelity, the router effectively favors experts who are better suited to model similar structural patterns. This design naturally aligns the routing strategy with the anomaly detection objective, in which reconstruction error serves as the primary signal for identifying abnormal nodes.\n\n### D. Model Training and Optimization\nGAD-MoRE is trained end-to-end in an unsupervised manner, as detailed in Algorithm 1. The total loss function Ltotal is a weighted sum of multiple components:\n* Embedding Reconstruction Loss (Lembed): The primary reconstruction objective, defined as the Mean Squared Error:\nLembed = (1 / |V|) Σ_{v∈V} ||hv − h^v||2^2. (14)\n\n* Feature Reconstruction Loss (Lfeat): This loss enforces the reconstructed embedding to preserve attribute information. A linear decoder maps the reconstructed embedding **h^v** back to the original feature space, and the Mean Squared Error is computed against the original features **Xv**:\nLfeat = (1 / |V|) Σ_{v∈V} ||Xv − Decoder_feat(h^v)||2^2. (15)\n\n* Structure Reconstruction Loss (Lstruct): This loss preserves topological information. The adjacency matrix is reconstructed via an inner product of the reconstructed embeddings. Let **H^** be the matrix formed by stacking all reconstructed node embeddings **h^v**. The Binary Cross-Entropy (BCE) loss is then computed against the original adjacency matrix **A**:\nLstruct = BCE(σ(**H^ H^T**), **A**). (16)\n\n* Structure-Contrastive Loss (Lcon): This structure-aware InfoNCE loss encourages connected nodes to have similar representations. For a node v, the loss is:\nLcon(v) = − log (Σ_{u∈Pv} exp(sim(hv, hu)/τc) / Σ_{j∈V} exp(sim(hv, hj)/τc)), (17)\nwhere Pv = {u | Avu = 1} is the set of positive neighbors of v, and τc is a temperature parameter. For large graphs, the denominator is computed over a randomly sampled subset of nodes, consistent with our implementation.\n\n* Gate Entropy Regularization (Lgate): This entropy-based loss encourages balanced router assignments and prevents expert collapse. To maximize entropy, we minimize its negative, which we define as the gate loss term:\nLgate = −(1 / |V|) Σ_{v∈V} H(g(hv)) = (1 / |V|) Σ_{v∈V} Σ^K_{i=1} gi(g(hv)) log gi(g(hv)). (18)\n\n---\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\n\nTABLE II: Statistics of the benchmark datasets.\n| Dataset | #Nodes | #Edges | #Feat. | #Anom. (%) |\n| :--- | :--- | :--- | :--- | :--- |\n| **Training Datasets** | | | | |\n| PubMed | 19,717 | 44,338 | 500 | 600 (3.04) |\n| Flickr | 7,575 | 239,738 | 12,047 | 450 (5.94) |\n| Reddit | 10,984 | 168,016 | 64 | 366 (3.33) |\n| YelpChi | 23,831 | 49,315 | 32 | 1,217 (5.10) |\n| **Test Datasets** | | | | |\n| ACM | 16,484 | 71,980 | 8,337 | 597 (3.62) |\n| Amazon | 10,244 | 175,608 | 25 | 693 (6.76) |\n| BlogCatalog | 5,196 | 171,743 | 8,189 | 300 (5.77) |\n| Citeseer | 3,327 | 4,732 | 3,703 | 150 (4.50) |\n| Cora | 2,708 | 5,429 | 1,433 | 150 (5.53) |\n| Facebook | 1,081 | 55,104 | 576 | 25 (2.31) |\n| Weibo | 8,405 | 407,963 | 400 | 868 (10.30) |\n\nBy integrating these modules with specific hyperparameters, the total training loss is established as follows:\nLtotal = λ1 Lembed + λ2 Lfeat + λ3 Lstruct + λ4 Lcon + λ5 Lgate, (19)\nwhere λ1, λ2, λ3, λ4, λ5 are the hyperparameters that control the relative contributions of each loss term.\n\nFor the structure-contrastive objective, we use a memory-efficient implementation with mini-batch sampling and chunked computations to approximate InfoNCE on large graphs without exceeding GPU memory. Specifically, the similarity function sim(**hv**, **hu**) operates on the node embedding vectors.\n\n### E. Anomaly Inference\nDuring the inference procedure, as detailed in Algorithm 2, GAD-MoRE is deployed on unseen graphs in a zero-shot manner. For each node v, we compute the anomaly score S(v) as the L2 norm of the residual between the input representation **hv** and its MoE-reconstructed counterpart **h^v**:\nS(v) = ||hv − h^v||2. (20)\nHigher scores indicate greater deviation from curvature-aware normal patterns learned by the Riemannian experts, reflecting reduced reconstruction fidelity and thus stronger evidence of anomalous structure.\n\n## V. EXPERIMENTS\n\nIn this section, we conduct comprehensive experiments to systematically evaluate our proposed GAD-MoRE framework. The evaluation is designed to answer the following key research questions:\n* *RQ1: Overall Performance.* How does GAD-MoRE perform against a wide range of supervised, unsupervised, and state-of-the-art generalist baselines in the challenging zero-shot cross-domain setting?\n* *RQ2: Key Component Analysis.* What are the precise contributions of the core innovations in GAD-MoRE, namely the anomaly-aware multi-curvature feature alignment, the mixture of Riemannian experts scorer, and the memory-based dynamic router, to overall model performance?\n* *RQ3: Hyperparameter Sensitivity.* How sensitive is GAD-MoRE to its key hyperparameters, such as the embedding dimension and the number of experts, assessing its robustness and practical applicability?\n* *RQ4: Comparison with Few-shot Learners.* How does our strictly zero-shot GAD-MoRE model compare against strong generalist competitors that are fine-tuned with a small number of labeled samples from the target graph?\n\n### A. Experimental Setup\n1) *Datasets*: To evaluate the generalization capability of our model, we conduct experiments on 11 widely used benchmark datasets spanning citation, social, and co-review networks. Following a rigorous protocol, all models are trained on four source graphs (PubMed [15], Flickr [28], [58], Reddit [14], YelpChi [7], [59]) and evaluated on seven unseen target graphs (ACM [60], Amazon [7], [59], BlogCatalog [28], [58], Citeseer [15], Cora [15], Facebook [61], Weibo [14]). Dataset statistics are summarized in Table II.\n\n2) *Baselines*: To ensure a rigorous and fair experimental validation, we benchmark the proposed GAD-MoRE against an extensive collection of competitive methods:\n* **Supervised Baselines**: These methods are trained on the source graphs with full label access. We include *GCN* [62], a foundational model that aggregates neighborhood features; *GAT* [63], which uses attention mechanisms for weighted neighbor aggregation; *BWGNN* [64], which applies band-pass filters to amplify high-frequency signals associated with anomalies; and *GHRN* [24], which is designed to handle heterophily in graphs.\n* **Unsupervised Baselines**: These methods are trained on the source graphs without any labels. We include *AnomalyDAE* [29], a classic reconstruction-based method using a dual autoencoder for structure and attributes; *CoLA* [21], a contrastive learning framework that learns representations by distinguishing nodes from their corrupted counterparts; and *TAM* [37], which optimizes node embeddings based on a local affinity assumption.\n* **Zero-shot Generalist Methods**: These are the current state-of-the-art models designed for zero-shot cross-domain graph anomaly detection. We include *UNPrompt* [17], which utilizes a unified neighborhood prompting mechanism; *AnomalyGFM* [18], a graph foundation model that learns graph-agnostic prototypes; and the newest *IA-GGAD* [19] framework, which learns invariant representations and structure-insensitive affinities.\n\n3) *Evaluation Metrics*: To evaluate anomaly detection performance comprehensively, we adopt two standard metrics: AUROC and AUPRC, which respectively assess overall discriminative ability and performance on the minority class. To ensure the stability of our findings, we conduct 5 independent runs for every experiment using various random seeds, providing both average results and their corresponding standard deviations.\n\n4) *Implementation Details*: For the detailed hyperparameter settings, the unified feature dimension D after alignment is set to 32. The GNN backbone for structure encoding consists of 4 \n\n---\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\n\nTABLE III: AUROC results on seven target Graph Anomaly Detection (GAD) datasets. All models are trained exclusively on four source graphs (PubMed, Flickr, Reddit, YelpChi) and evaluated in a zero-shot cross-domain setting. The performance results highlight the optimal outcomes in **bold** and the subsequent best in <ins>underline</ins> for every dataset. Furthermore, the “Average” column indicates the calculated mean efficacy across the entire suite of benchmarks.\n\n| | Dataset | | Citation | | | Social | | Co-review | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | Method | ACM | Citeseer | Cora | BlogCatalog | Facebook | Weibo | Amazon | Average |\n| **Supervised** | GCN | 47.69±1.48 | 53.83±4.01 | 51.99±3.20 | 43.83±2.39 | 46.09±18.88 | 37.64±13.84 | 41.10±4.24 | 46.02 |\n| | GAT | 46.54±4.71 | 44.58±8.25 | 44.65±6.20 | 49.32±1.70 | 53.96±26.14 | 35.50±6.91 | 55.35±5.69 | 47.13 |\n| | BWGNN | 67.43±2.48 | 42.58±2.86 | 39.75±3.16 | 69.91±4.66 | 64.84±4.17 | 50.08±0.03 | 50.01±0.04 | 54.94 |\n| | GHRN | 52.72±1.01 | 54.11±0.88 | 47.65±2.03 | 62.28±2.42 | 54.04±3.81 | 17.79±5.00 | 63.52±1.92 | 50.30 |\n| **Unsupervised**| AnomalyDAE | 77.60±1.03 | <ins>82.00±2.78</ins> | 84.01±1.19 | 57.53±0.30 | 10.87±0.70 | 19.77±1.14 | 58.38±0.25 | 55.74 |\n| | CoLA | 49.30±1.00 | 52.56±1.36 | 49.35±1.52 | 49.50±1.27 | 50.70±5.49 | 45.25±2.39 | 47.57±3.02 | 49.17 |\n| | TAM | 50.39±0.08 | 46.44±0.15 | 48.78±0.17 | 33.03±0.28 | 54.56±0.36 | 21.50±0.30 | 52.37±0.12 | 43.87 |\n| **Generalist** | UNPrompt | 73.20±0.36 | 59.53±0.53 | 63.18±1.29 | 70.85±0.50 | 60.64±1.94 | 66.29±1.38 | 72.14±2.94 | 66.55 |\n| | AnomalyGFM | 38.91±1.21 | 40.37±1.37 | 40.93±1.09 | 40.93±1.22 | 74.45±2.36 | 44.43±4.89 | 61.24±5.21 | 48.75 |\n| | IA-GGAD | <ins>78.44±0.46</ins> | **91.54±0.32** | <ins>85.69±0.77</ins> | <ins>73.03±1.20</ins> | <ins>69.59±1.39</ins> | <ins>87.35±0.40</ins> | <ins>53.38±1.76</ins> | <ins>77.00</ins> |\n| | **GAD-MoRE** | **81.17±0.05** | <ins>90.28±0.13</ins> | **86.39±0.20** | **73.09±0.06** | **75.75±0.95** | **91.03±0.11** | **76.90±1.80** | **82.09** |\n\nTABLE IV: AUPRC results on seven target Graph Anomaly Detection (GAD) datasets. All models are trained exclusively on four source graphs (PubMed, Flickr, Reddit, YelpChi) and evaluated in a zero-shot cross-domain setting. The performance results highlight the optimal outcomes in **bold** and the subsequent best in <ins>underline</ins> for every dataset. Furthermore, the “Average” column indicates the calculated mean efficacy across the entire suite of benchmarks.\n\n| | Dataset | | Citation | | | Social | | Co-review | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | Method | ACM | Citeseer | Cora | BlogCatalog | Facebook | Weibo | Amazon | Average |\n| **Supervised** | GCN | 3.68±0.16 | 5.21±0.89 | 5.85±0.55 | 5.71±0.38 | 4.55±3.06 | 25.97±15.84 | 5.54±0.51 | 8.07 |\n| | GAT | 3.38±0.45 | 4.35±0.82 | 4.93±0.75 | 6.64±0.71 | 6.18±5.83 | 15.86±5.83 | 8.10±1.24 | 7.06 |\n| | BWGNN | 19.48±2.20 | 4.02±0.43 | 5.30±0.87 | 28.37±4.31 | 5.71±1.60 | 10.47±0.05 | 6.79±0.03 | 11.45 |\n| | GHRN | 9.21±2.94 | 5.37±0.14 | 5.23±0.19 | 24.57±3.60 | 3.61±0.68 | 8.35±2.44 | 11.18±1.10 | 9.65 |\n| **Unsupervised**| AnomalyDAE | 11.67±0.38 | <ins>31.37±1.09</ins> | <ins>32.65±1.46</ins> | 7.17±0.04 | 1.32±0.01 | 6.10±0.08 | 9.26±1.51 | 14.22 |\n| | CoLA | 3.55±0.12 | 4.76±0.21 | 5.86±0.34 | 6.11±0.36 | 3.41±1.38 | 9.67±0.50 | 6.55±0.44 | 5.70 |\n| | TAM | 3.74±0.01 | 4.15±0.02 | 5.58±0.08 | 5.66±0.17 | 5.37±0.51 | 6.20±0.02 | 7.90±0.05 | 5.51 |\n| **Generalist** | UNPrompt | <ins>24.02±1.38</ins> | 9.47±0.91 | 10.26±0.52 | <ins>35.29±0.90</ins> | 3.44±0.31 | 28.41±2.25 | <ins>12.28±1.63</ins> | 17.59 |\n| | AnomalyGFM | 3.10±0.17 | 3.69±0.20 | 4.36±0.09 | 4.85±0.40 | 6.00±0.52 | 9.05±1.03 | 9.18±1.69 | 5.75 |\n| | IA-GGAD | **36.33±0.16** | **46.96±1.16** | <ins>45.03±2.06</ins> | **36.64±0.66** | <ins>8.65±1.74</ins> | <ins>60.16±0.93</ins> | 6.90±0.29 | <ins>34.38</ins> |\n| | **GAD-MoRE** | **37.17±0.07** | <ins>40.15±1.31</ins> | **45.25±0.88** | <ins>35.31±0.42</ins> | **9.00±1.12** | **60.26±0.30** | **31.55±4.84** | **36.96** |\n\nlayers and aggregates information from a 2-hop neighborhood. The final node embeddings are constructed using residual connections to capture deviations from the initial node features. For the MoE scorer, the number of experts K is set to 5, and we use a top-k routing strategy with k = 2. The Riemannian experts are initialized with a diverse set of curvatures to cover different Riemannian spaces; specifically, the initial curvatures are set to [0.0, -0.5, -1.0, 0.5, 1.0], allowing experts to specialize in Euclidean, Hyperbolic, and Spherical geometries. The gating mechanism’s softmax temperature is set to 0.7. Then we use the Adam optimizer with a learning rate of 5e-5 and a weight decay of 5e-5, training for 40 epochs on the combined source graphs. The weights for the components in our final loss function (Eq. (19)) are set as λ1 = 1.0 (Lembed), λ2 = 0.5 (Lfeat), λ3 = 0.1 (Lstruct), λ4 = 0.1 (Lcon), and λ5 = 0.01 (Lgate). For all baselines, we adopt the optimal hyperparameter settings reported in their original papers or tune them on the source graphs. All experiments are conducted on a single NVIDIA Tesla A100 with 40GB of memory.\n\n### B. Overall Performance (RQ1)\nThe results, as detailed in Table III and Table IV, demonstrate the superiority of our proposed GAD-MoRE framework. Across seven test datasets, GAD-MoRE consistently achieves state-of-the-art performance with an average AUROC of 82.09% and AUPRC of 36.96%, surpassing all baseline categories. Compared to the strongest generalist competitor, IA-GGAD, our model achieves an absolute improvement of 5.09% in average AUROC, validating the effectiveness of our design. Specifically, it establishes a remarkable lead on the structurally complex Weibo dataset, demonstrating the necessity of our specialized Riemannian experts. Notably, GAD-MoRE outperforms even the fully supervised baselines, which have access to anomaly labels on the source graphs. This highlights the limitation of traditional supervised learning in generalizing to new domains and underscores the effectiveness of \n\n---\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\n\n![Fig. 3: AUROC performance of GAD-MoRE and its ablation variants on target GAD datasets. All models are trained exclusively on four source graphs (PubMed, Flickr, Reddit, YelpChi) and evaluated in a zero-shot cross-domain setting.](figure_3.png)\n\n![Fig. 4: Hyperparameter sensitivity analysis of GAD-MoRE. The left two subplots show the impact of the unified embedding dimension D on AUROC and AUPRC, while the right two subplots illustrate the sensitivity to the number of experts K.](figure_4.png)\n\nGAD-MoRE’s unsupervised, geometry-aware approach. When compared to other generalist methods like UNPrompt and AnomalyGFM, GAD-MoRE shows a substantial improvement in both average metrics. This validates our core hypothesis that explicitly modeling the geometric heterogeneity of graphs is crucial for effective domain generalization. The strong AUPRC score is particularly significant, as it confirms that our proposed GAD-MoRE is highly effective at identifying the rare anomalous nodes in an imbalanced setting.\n\nFurthermore, the effectiveness of GAD-MoRE in distinguishing normal from anomalous patterns is visualized in Figure 5, which shows the anomaly score distributions. In both datasets, normal nodes (blue) receive lower scores, while anomalous nodes (red) are assigned higher scores, highlighting GAD-MoRE’s ability to accurately identify anomalies.\n\n### C. Key Component Analysis (RQ2)\nTo answer our second research question and quantify the precise contributions of the central innovations within GAD-MoRE, we conduct a thorough ablation study. We design several variants of our model, each disabling or replacing a key component, and evaluate their AUROC performance against the full model on representative datasets. The components under investigation are: We define the following variants for comparison, with their acronyms matching Figure 3:\n* **w/o MCFA**: In this variant, we bypass the MCFA module and use a standard PCA for dimensionality reduction, removing the multi-geometry signals.\n* **w/o MoE**: We replace the MoE scorer with an attention-based reconstruction scorer over reference embeddings.\n* **w/o MDR**: In this version, we replace our Memory-based Dynamic Router with a simpler linear router.\n\n![Fig. 5: The anomaly score distributions for Citeseer and Cora show a clear distinction between normal and anomalous nodes. This visual evidence confirms that our approach can effectively detect anomalies in these graph datasets.](figure_5.png)\n\nFigure 3 presents the results of our ablation study. These findings clearly confirm the critical importance of each model component. The most significant performance degradation is observed in the w/o MoE variant, with the average AUROC dropping to 75.32%. This provides the strongest empirical evidence for our central hypothesis: a single geometric lens is insufficient for generalizable GAD, and a collection of specialized Riemannian experts is essential for robust performance. By contrast, removing the Multi-curvature Fea-\n\n---\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\n\nTABLE V: AUROC performance comparison (in %) of a zero-shot model against few-shot competitors on seven target Graph Anomaly Detection (GAD) datasets. The few-shot methods are fine-tuned with 10 labeled samples (10-shot). The performance results highlight the optimal outcomes in **bold** and the subsequent best in <ins>underline</ins> for every dataset. Furthermore, the “Average” column indicates the calculated mean efficacy across the entire suite of benchmarks.\n\n| | Dataset | | Citation | | | Social | | Co-review | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | Method | ACM | Citeseer | Cora | BlogCatalog | Facebook | Weibo | Amazon | Average |\n| **10-shot** | ARC | 78.33±1.13 | 82.31±2.89 | 82.22±2.34 | 72.59±1.10 | 64.94±3.72 | <ins>88.65±0.82</ins> | 57.95±6.13 | 75.28 |\n| | AnomalyGFM | 41.88±0.73 | 46.60±0.61 | 43.56±1.04 | 38.19±0.34 | **81.96±3.08** | 49.68±3.43 | <ins>66.03±1.91</ins> | 52.56 |\n| **0-shot** | **GAD-MoRE** | **81.17±0.05** | **90.28±0.13** | **86.39±0.20** | **73.09±0.06** | <ins>75.75±0.95</ins> | **91.03±0.11** | **76.90±1.80** | **82.09** |\n\nTABLE VI: AUPRC performance comparison (in %) of a zero-shot model against few-shot competitors on seven target Graph Anomaly Detection (GAD) datasets. The few-shot methods are fine-tuned with 10 labeled samples (10-shot). The performance results highlight the optimal outcomes in **bold** and the subsequent best in <ins>underline</ins> for every dataset. Furthermore, the “Average” column indicates the calculated mean efficacy across the entire suite of benchmarks.\n\n| | Dataset | | Citation | | | Social | | Co-review | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | Method | ACM | Citeseer | Cora | BlogCatalog | Facebook | Weibo | Amazon | Average |\n| **10-shot** | ARC | <ins>36.53±0.24</ins> | <ins>30.93±4.24</ins> | <ins>39.23±4.40</ins> | **37.77±0.35** | 4.92±1.85 | <ins>59.47±2.38</ins> | 7.98±1.18 | <ins>30.98</ins> |\n| | AnomalyGFM | 2.89±0.08 | 4.13±0.09 | 4.30±0.10 | 4.17±0.06 | **16.53±5.41** | 9.79±0.99 | <ins>10.88±1.50</ins> | 7.53 |\n| **0-shot** | **GAD-MoRE** | **37.17±0.07** | **40.15±1.31** | **45.25±0.88** | <ins>35.31±0.42</ins> | <ins>9.00±1.12</ins> | **60.26±0.30** | **31.55±4.84** | **36.96** |\n\nture Alignment (w/o MCFA) also leads to a substantial performance drop, particularly on the Facebook dataset. This demonstrates that enriching initial node representations with signals from diverse Riemannian spaces provides a more robust foundation, enabling the model to adapt more effectively to unseen graph structures. Lastly, replacing the Memory-based Dynamic Router (w/o MDR) with a simpler linear router results in a slight but consistent performance decrease across most datasets. This highlights the effectiveness of the design of our proposed dynamic routing strategy, as history-aware assignments from the MDR lead to more precise final reconstruction.\n\n### D. Hyperparameter Sensitivity (RQ3)\nIn this section, we investigate the sensitivity of GAD-MoRE to its two most critical hyperparameters: the unified embedding dimension D and the number of experts K. This analysis helps to understand the model’s robustness and guides practical application. Figure 4 illustrates the performance of GAD-MoRE with respect to these two parameters on four representative test datasets, including ACM, Citeseer, Cora, and Weibo.\n\nFor the embedding dimension D (left column of Figure 4), we observe that performance for both AUROC and AUPRC generally improves as the dimension increases from 16 to 32. This indicates that a larger dimension allows for more expressive representations to capture complex patterns. However, the performance then stabilizes or slightly decreases for dimension 48, suggesting that overly large dimensions may lead to overfitting on the source datasets without improving generalization. Our chosen value of D = 32 offers the best trade-off between performance and model efficiency.\n\nFor the number of experts K (right column of Figure 4), performance steadily increases as we add more experts from K = 2 up to K = 5. This suggests that a richer set of geometric lenses is beneficial for handling the diverse graph structures encountered in the test sets. Beyond K = 5, the performance plateaus, indicating that five experts are sufficient to cover the geometric diversity in our datasets without becoming redundant or introducing unnecessary complexity. This confirms our choice of K = 5 as optimal for the MoE scorer. Overall, the results show that GAD-MoRE is robust to variations in these hyperparameters within a reasonable range.\n\n### E. Comparison with Few-shot Learners (RQ4)\nTo further demonstrate the strong performance of our proposed GAD-MoRE on the GAD task, we conduct a comparison against the competitive methods that are fine-tuned using labeled data from the target domain. In particular, we still rigorously evaluate the strictly zero-shot capability of GAD-MoRE in comparing it with few-shot learning approaches, which are allowed access to a limited amount of target-domain labeled data. Specifically, we compare GAD-MoRE with ARC [16] and AnomalyGFM [18], two representative state-of-the-art generalist GAD frameworks designed for cross-domain transfer. These methods are selected as strong baselines due to their demonstrated effectiveness under few-shot adaptation settings. The baselines are detailed as follows:\n* **ARC** [16] utilizes an in-context learning mechanism to adapt to target graphs using limited target labels.\n* **AnomalyGFM** [18] is a graph foundation model that aligns node representations with class-agnostic prototypes to achieve generalization.\n\nThese two methods are chosen because most traditional GAD models are graph-specific and lack the architectural support for cross-domain fine-tuning. To simulate a competitive few-shot scenario, we fine-tune these baselines on 10 labeled nodes \n\n---\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13\n\nfrom each test graph (10-shot) and report performance with our model, which receives no such target-specific advantage.\n\nThe quantitative results for AUROC and AUPRC are detailed in Table V and Table VI. Despite operating in a zero-shot setting, GAD-MoRE demonstrates remarkable superiority, achieving an average AUROC of 82.09% and AUPRC of 36.96%. These results substantially surpass the 10-shot fine-tuned ARC (75.28% / 30.98%) and AnomalyGFM (52.56% / 7.53%), proving that GAD-MoRE outperforms competitors on most datasets without access to target domain labels.\n\nThis comparison reveals a crucial insight: GAD-MoRE’s architecture is highly effective at learning generalizable, geometry-aware patterns from source domains, enabling it to surpass models that benefit from direct, albeit limited, supervision. This showcases the power of our approach and demonstrates that GAD-MoRE is a more practical and scalable solution for real-world GAD applications where labels for new domains are scarce or entirely unavailable.\n\n## VI. CONCLUSION\n\nIn this paper, we introduce GAD-MoRE, a novel framework for generalizable graph anomaly detection that addresses the challenge of geometric heterogeneity. Motivated by our empirical finding that no single curvature is universally optimal, we propose a novel MoE architecture, featuring a multi-curvature feature alignment module, a mixture of Riemannian experts scorer, and a memory-based dynamic router. Specifically, the anomaly-aware multi-curvature feature alignment module bridges the gap between geometry-agnostic raw features and manifold representations by projecting inputs into parallel Riemannian spaces. Subsequently, the mixture of Riemannian experts scorer employs specialized Riemannian experts operating in distinct curvature spaces, ensuring that complex anomaly patterns are modeled in suitable geometric environments without distortion. Furthermore, the memory-based dynamic router facilitates adaptive inference using historical reconstruction feedback to align expert selection with the anomaly detection objective. Extensive experiments demonstrate that GAD-MoRE establishes a new state-of-the-art in zero-shot settings and consistently outperforms strong generalist competitors, even those fine-tuned with target domain labels. Promising research directions include extending GAD-MoRE to dynamic graphs, exploring hierarchical MoE architectures, and integrating with graph foundation models.\n\n## ACKNOWLEDGMENT\nThe corresponding author is Qingyun Sun. The authors are supported in part by the National Natural Science Foundation of China through grants No.62225202 and No.62427808, and the Fundamental Research Funds for the Central Universities.\n\n## REFERENCES\n[1] Z. Yuan, Q. Sun, H. Zhou, M. Shao, and X. Fu, “A comprehensive survey on gnn-based anomaly detection: taxonomy, methods, and the role of large language models,” *Int. J. Mach. Learn. Cybern.*, pp. 1–26, 2025.\n[2] X. Ma, J. Wu, S. Xue, J. Yang, C. Zhou, Q. Z. Sheng, H. Xiong, and L. Akoglu, “A comprehensive survey on graph anomaly detection with deep learning,” *IEEE TKDE*, vol. 35, no. 12, pp. 12 012–12 038, 2021.\n[3] C. C. Aggarwal, “An introduction to outlier analysis,” in *Outlier analysis*. Springer, 2016, pp. 1–34.\n[4] W. Hilal, S. A. Gadsden, and J. Yawney, “Financial fraud: a review of anomaly detection techniques and recent advances,” *ESWA*, vol. 193, p. 116429, 2022.\n[5] D. Wang, J. Lin, P. Cui, Q. Jia, Z. Wang, Y. Fang, Q. Yu, J. Zhou, S. Yang, and Y. Qi, “A semi-supervised graph attentive network for financial fraud detection,” in *ICDM*. IEEE, 2019, pp. 598–607.\n[6] Y. Yang, Y. Xu, Y. Sun, Y. Dong, F. Wu, and Y. Zhuang, “Mining fraudsters and fraudulent strategies in large-scale mobile social networks,” *IEEE TKDE*, vol. 33, no. 1, pp. 169–179, 2019.\n[7] S. Rayana and L. Akoglu, “Collective opinion spam detection: Bridging review networks and metadata,” in *KDD*, 2015, pp. 985–994.\n[8] S. Kumar, B. Hooi, D. Makhija, M. Kumar, C. Faloutsos, and V. Subrahmanian, “Rev2: Fraudulent user prediction in rating platforms,” in *WSDM*, 2018, pp. 333–341.\n[9] A. B. Nassif, M. A. Talib, Q. Nasir, and F. M. Dakalbab, “Machine learning for anomaly detection: A systematic review,” *IEEE Access*, vol. 9, pp. 78 658–78 700, 2021.\n[10] H. Qiao, H. Tong, B. An, I. King, C. Aggarwal, and G. Pang, “Deep graph anomaly detection: A survey and new perspectives,” *IEEE TKDE*, 2025.\n[11] J. A. Hartigan and M. A. Wong, “Algorithm as 136: A k-means clustering algorithm,” *J. R. Stat. Soc. Ser. C (Appl. Stat.)*, vol. 28, no. 1, pp. 100–108, 1979.\n[12] T. Cover, “Estimation by the nearest neighbor rule,” *IEEE TIT*, vol. 14, no. 1, pp. 50–55, 1968.\n[13] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation forest,” in *ICDM*. IEEE, 2008, pp. 413–422.\n[14] S. Kumar, X. Zhang, and J. Leskovec, “Predicting dynamic embedding trajectory in temporal interaction networks,” in *KDD*, 2019, pp. 1269–1278.\n[15] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad, “Collective classification in network data,” *AI Magazine*, vol. 29, no. 3, pp. 93–93, 2008.\n[16] Y. Liu, S. Li, Y. Zheng, Q. Chen, C. Zhang, and S. Pan, “Arc: A generalist graph anomaly detector with in-context learning,” *NeurIPS*, vol. 37, pp. 50 772–50 804, 2024.\n[17] C. Niu, H. Qiao, C. Chen, L. Chen, and G. Pang, “Zero-shot generalist graph anomaly detection with unified neighborhood prompts,” *ArXiv*, 2024.\n[18] H. Qiao, C. Niu, L. Chen, and G. Pang, “Anomalygfm: Graph foundation model for zero/few-shot anomaly detection,” in *KDD*, 2025, pp. 2326–2337.\n[19] X. Zhang, Z. He, C. Fu, and C. Xie, “Ia-ggad: Zero-shot generalist graph anomaly detection via invariant and affinity learning,” in *NeurIPS*, 2025.\n[20] K. Grover, G. J. Gordon, and C. Faloutsos, “Curvgad: Leveraging curvature for enhanced graph anomaly detection,” in *ICML*, 2025.\n[21] Y. Liu, Z. Li, S. Pan, C. Gong, C. Zhou, and G. Karypis, “Anomaly detection on attributed networks via contrastive self-supervised learning,” *IEEE TNNLS*, vol. 33, no. 6, pp. 2378–2392, 2021.\n[22] T. N. Kipf and M. Welling, “Variational graph auto-encoders,” *ArXiv*, 2016.\n[23] Y. Dou, Z. Liu, L. Sun, Y. Deng, H. Peng, and P. S. Yu, “Enhancing graph neural network-based fraud detectors against camouflaged fraudsters,” in *CIKM*, 2020, pp. 315–324.\n[24] Y. Gao, X. Wang, X. He, Z. Liu, H. Feng, and Y. Zhang, “Addressing heterophily in graph anomaly detection: A perspective of graph spectrum,” in *WWW*, 2023, pp. 1528–1538.\n[25] T. Zhao, X. Zhang, and S. Wang, “Graphsmote: Imbalanced node classification on graphs with graph neural networks,” in *WSDM*, 2021, pp. 833–841.\n[26] L. Wu, J. Xia, Z. Gao, H. Lin, C. Tan, and S. Z. Li, “Graphmixup: Improving class-imbalanced node classification by reinforcement mixup and self-supervised context prediction,” in *ECML-PKDD*, 2022, pp. 519–535.\n[27] K. Liu, H. Zhang, Z. Hu, F. Wang, and P. S. Yu, “Data augmentation for supervised graph outlier detection with latent diffusion models,” *ArXiv*, 2023.\n[28] K. Ding, J. Li, R. Bhanushali, and H. Liu, “Deep anomaly detection on attributed networks,” in *SDM*. SIAM, 2019, pp. 594–602.\n[29] H. Fan, F. Zhang, and Z. Li, “Anomalydae: Dual autoencoder for anomaly detection on attributed networks,” in *ICASSP*. IEEE, 2020, pp. 5685–5689.\n[30] X. Yuan, N. Zhou, S. Yu, H. Huang, Z. Chen, and F. Xia, “Higher-order structure based anomaly detection on attributed networks,” in *IEEE BigData*. IEEE, 2021, pp. 2691–2700.\n\n---\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14\n\n[31] P. Velickovic, W. Fedus, W. L. Hamilton, P. Lio, Y. Bengio, and R. D. Hjelm, “Deep graph infomax,” *ArXiv*, 2018.\n[32] F.-Y. Sun, J. Hoffmann, V. Verma, and J. Tang, “Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization,” *ArXiv*, 2019.\n[33] F. Lin, X. Luo, J. Wu, J. Yang, S. Xue, Z. Wang, and H. Gong, “Discriminative graph-level anomaly detection via dual-students-teacher model,” in *ADMA*. Springer, 2023, pp. 261–276.\n[34] Z. Chen, B. Liu, M. Wang, P. Dai, J. Lv, and L. Bo, “Generative adversarial attributed network anomaly detection,” in *CIKM*, 2020, pp. 1989–1992.\n[35] K. Ding, J. Li, N. Agarwal, and H. Liu, “Inductive anomaly detection on attributed networks,” in *IJCAI*, 2021, pp. 1288–1294.\n[36] K. Ding, Q. Zhou, H. Tong, and H. Liu, “Few-shot network anomaly detection via cross-network meta-learning,” in *WWW*, 2021, pp. 2448–2456.\n[37] S. Tian, J. Dong, J. Li, W. Zhao, X. Xu, B. Wang, B. Song, C. Meng, T. Zhang, and L. Chen, “Sad: semi-supervised anomaly detection on dynamic graphs,” in *IJCAI*, 2023, pp. 2306–2314.\n[38] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton, “Adaptive mixtures of local experts,” *Neural Comput.*, vol. 3, no. 1, pp. 79–87, 1991.\n[39] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, “Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,” in *ICLR*, 2017.\n[40] Z. Mi, P. Yin, X. Xiao, and D. Xu, “Learning heterogeneous mixture of scene experts for large-scale neural radiance fields,” *IEEE TPAMI*, 2025.\n[41] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity,” *JMLR*, vol. 23, no. 120, pp. 1–39, 2022.\n[42] C. Riquelme, J. Puigcerver, B. Mustafa, M. Neumann, R. Jenatton, A. Susano Pinto, D. Keysers, and N. Houlsby, “Scaling vision with sparse mixture of experts,” *NeurIPS*, vol. 34, pp. 8583–8595, 2021.\n[43] L. Liu, P. Wang, G. Wu, J. Jiang, and H. Yang, “Towards optimal mixture of experts system for 3d object detection: A game of accuracy, efficiency and adaptivity,” *IEEE TPAMI*, 2025.\n[44] Z. Guo, Q. Sun, H. Yuan, X. Fu, M. Zhou, Y. Gao, and J. Li, “Graphmore: Mitigating topological heterogeneity via mixture of riemannian experts,” in *AAAI*, vol. 39, no. 11, 2025, pp. 11 754–11 762.\n[45] H. Zeng, H. Lyu, D. Hu, Y. Xia, and J. Luo, “Mixture of weak & strong experts on graphs,” *ArXiv*, 2023.\n[46] H. Wang, Z. Jiang, Y. You, Y. Han, G. Liu, J. Srinivasa, R. Kompella, Z. Wang *et al.*, “Graph mixture of experts: Learning on large-scale graphs with explicit diversity modeling,” *NeurIPS*, vol. 36, pp. 50 825–50 837, 2023.\n[47] F. Hu, L. Wang, S. Wu, L. Wang, and T. Tan, “Graph classification by mixture of diverse experts,” *ArXiv*, 2021.\n[48] J. Cai, Y. Zhang, P. Wang, and S.-K. Ng, “Moegad: A mixture-of-experts framework with pseudo-anomaly generation for graph-level anomaly detection.” *IEEE TPAMI*, 2025.\n[49] I. Chami, Z. Ying, C. Re, and J. Leskovec, “Hyperbolic graph convolutional neural networks,” *NeurIPS*, vol. 32, 2019.\n[50] X. Fu, J. Li, J. Wu, Q. Sun, C. Ji, S. Wang, J. Tan, H. Peng, and P. S. Yu, “Ace-hgnn: Adaptive curvature exploration hyperbolic graph neural network,” in *ICDM*. IEEE, 2021, pp. 111–120.\n[51] J. Li, X. Fu, Q. Sun, C. Ji, J. Tan, J. Wu, and H. Peng, “Curvature graph generative adversarial networks,” in *WWW*, 2022, pp. 1528–1537.\n[52] R. C. Wilson, E. R. Hancock, E. Pekalska, and R. P. Duin, “Spherical and hyperbolic embeddings of data,” *IEEE TPAMI*, vol. 36, no. 11, pp. 2255–2269, 2014.\n[53] M. Nickel and D. Kiela, “Poincare embeddings for learning hierarchical representations,” *NeurIPS*, vol. 30, 2017.\n[54] Q. Liu, M. Nickel, and D. Kiela, “Hyperbolic graph neural networks,” *NeurIPS*, vol. 32, 2019.\n[55] Y. Zhang, X. Wang, C. Shi, X. Jiang, and Y. Ye, “Hyperbolic graph attention network,” *IEEE TBD*, vol. 8, no. 6, pp. 1690–1701, 2021.\n[56] K. Pearson, “Liii. on lines and planes of closest fit to systems of points in space,” *The London, Edinburgh, and Dublin philosophical magazine and journal of science*, vol. 2, no. 11, pp. 559–572, 1901.\n[57] H. Hotelling, “Analysis of a complex of statistical variables into principal components.” *J. Educ. Psychol.*, vol. 24, no. 6, p. 417, 1933.\n[58] L. Tang and H. Liu, “Relational learning via latent social dimensions,” in *KDD*, 2009, pp. 817–826.\n[59] J. J. McAuley and J. Leskovec, “From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews,” in *WWW*, 2013, pp. 897–908.\n[60] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su, “Arnetminer: extraction and mining of academic social networks,” in *KDD*, 2008, pp. 990–998.\n[61] Z. Xu, X. Huang, Y. Zhao, Y. Dong, and J. Li, “Contrastive attributed network anomaly detection with data augmentation,” in *PAKDD*. Springer, 2022, pp. 444–457.\n[62] T. Kipf, “Semi-supervised classification with graph convolutional networks,” *Arxiv*, 2016.\n[63] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio, “Graph attention networks,” *ArXiv*, 2017.\n[64] J. Tang, J. Li, Z. Gao, and J. Li, “Rethinking graph neural networks for anomaly detection,” in *ICML*, 2022, pp. 21 076–21 089.\n\n**Xinyu Zhao** is currently a master’s candidate at the School of Computer Science and Engineering, Beihang University. His research interests include graph anomaly detection and graph foundation models.\n\n**Qingyun Sun** is currently an Associate Professor at the School of Computer Science and Engineering, Beihang University. Her research interests include machine learning and graph mining. She has published several papers on IEEE TPAMI, IEEE TKDE, Web Conference, AAAI, ICDM, CIKM, etc..\n\n**Jiayi Luo** is currently a Ph.D. candidate at the School of Computer Science and Engineering, Beihang University. His research interests include robust graph learning and graph foundation models.\n\n**Xingcheng Fu** is currently an associate professor at the Key Lab of Education Blockchain and Intelligent Technology at Guangxi Normal University. His research interests include graph representation learning, complex networks, and social network analysis. He has published several papers on IEEE TKDE, Web Conference, AAAI, ICDM, CIKM, etc..\n\n**Jianxin Li** is currently a Professor with the School of Computer Science and Engineering, Beihang University. His current research interests include social networks, machine learning, big data, and trustworthy computing. Dr. Li has published research papers in top-tier journals and conferences, including the IEEE TKDE, TDSC, JAIR, ACM TOIS, TKDD, KDD, AAAI, and WWW.",
      "analysis_json": "{\"title\":\"Zero-shot Generalizable Graph Anomaly Detection with Mixture of Riemannian Experts\",\"authors\":[\"Xinyu Zhao\",\"Qingyun Sun\",\"Jiayi Luo\",\"Xingcheng Fu\",\"Jianxin Li\"],\"publication_year\":\"2026\",\"summary\":\"This paper addresses the challenge of detecting anomalies in graphs that the model has never seen before (zero-shot generalization). The authors argue that existing methods fail because they force all graphs into a single geometric space (usually Euclidean), which distorts the specific structural patterns of anomalies. Different anomalies are better detected in different geometric spaces (e.g., tree-like anomalies in Hyperbolic space, cyclical anomalies in Spherical space). To solve this, they propose GAD-MoRE, a framework using a Mixture of Experts (MoE) where each expert operates in a different Riemannian manifold (different curvature). A memory-based router dynamically assigns nodes to the expert that best reconstructs their specific pattern, allowing the model to adapt to diverse anomaly types across different domains without retraining.\",\"breakthrough_score\":78,\"breakthrough_reasoning\":\"The paper introduces a significant conceptual leap by combining Mixture of Experts (MoE) architectures with Riemannian Geometric Deep Learning specifically for the problem of domain generalization in anomaly detection. While MoEs and Hyperbolic GNNs exist separately, using curvature-specific experts to solve the 'geometric heterogeneity' problem in zero-shot transfer is a novel and scientifically grounded approach. It addresses a fundamental theoretical limitation of fixed-curvature embeddings, making it a high-impact contribution to graph learning.\",\"key_claims\":[\"Graph anomalies exhibit geometric heterogeneity, meaning their detectability varies significantly depending on the curvature of the embedding space.\",\"Forcing diverse graph data into a single fixed-curvature space causes embedding distortion that hides anomalies.\",\"A Mixture of Riemannian Experts (MoE) can dynamically adapt to the optimal geometry for each node instance.\",\"Memory-based routing based on historical reconstruction quality outperforms standard feature-based routing for anomaly detection tasks.\",\"GAD-MoRE outperforms state-of-the-art baselines in zero-shot settings, even surpassing some methods fine-tuned on target data.\"],\"testable_hypotheses\":[{\"hypothesis\":\"Anomalies in hierarchical/tree-like graphs are statistically easier to separate from normal nodes when embedded in Hyperbolic space (negative curvature) compared to Euclidean space.\",\"how_to_test\":\"Take a dataset with known hierarchical structure (e.g., citation networks). Embed nodes using a Hyperbolic GAE and a Euclidean GAE. Measure the AUROC of anomaly detection using simple distances in both spaces.\",\"expected_outcome\":\"The Hyperbolic embedding should yield a significantly higher AUROC score for hierarchical datasets.\"},{\"hypothesis\":\"Routing nodes to experts based on reconstruction error (memory) yields higher accuracy than routing based purely on input feature similarity.\",\"how_to_test\":\"Train two variants of GAD-MoRE: one using the proposed Memory-based Dynamic Router and one using a standard learned gating network (MLP on input features). Compare performance on unseen graphs.\",\"expected_outcome\":\"The Memory-based router variant will achieve higher detection accuracy because it routes based on the model's actual ability to handle the data pattern.\"}],\"key_equations\":[{\"name\":\"Hyperbolic Distance (Poincaré Ball)\",\"latex\":\"d_H(p, q) = (1 / sqrt(-κ)) * arccosh(1 + (2 * ||p - q||^2) / ((1 + κ * ||p||^2) * (1 + κ * ||q||^2)))\",\"description\":\"Calculates the distance between two points in a negatively curved Hyperbolic space, which expands exponentially, making it ideal for trees.\",\"variables\":[{\"name\":\"κ\",\"description\":\"Curvature parameter (negative for Hyperbolic)\",\"typical_range\":\"-1.0 to -0.1\"},{\"name\":\"p, q\",\"description\":\"Node embedding vectors\",\"typical_range\":\"Vectors in unit ball\"}]},{\"name\":\"MoE Weighted Reconstruction\",\"latex\":\"h^v = Σ (exp(s_i(hv)/τ) / Σ exp(s_j(hv)/τ)) * h^(i)_v\",\"description\":\"The final node representation is a weighted sum of the reconstructions provided by the selected top-k Riemannian experts.\",\"variables\":[{\"name\":\"s_i\",\"description\":\"Routing logit/score for expert i\",\"typical_range\":\"Real number\"},{\"name\":\"τ\",\"description\":\"Temperature parameter controlling gating sharpness\",\"typical_range\":\"0.1 to 1.0\"}]},{\"name\":\"Anomaly Score\",\"latex\":\"S(v) = ||h_v - h^v||_2\",\"description\":\"The anomaly score is defined as the Euclidean distance between the original node embedding and the reconstructed embedding from the MoE.\",\"variables\":[{\"name\":\"h_v\",\"description\":\"Original node embedding\",\"typical_range\":\"Vector\"},{\"name\":\"h^v\",\"description\":\"Reconstructed embedding\",\"typical_range\":\"Vector\"}]}],\"simulation_possibilities\":[{\"title\":\"The Geometric Lens Comparator\",\"description\":\"A visualization showing a cluster of nodes (normal and anomalous) projected simultaneously into three viewports: Hyperbolic (Poincaré Disk), Euclidean (2D Plane), and Spherical (3D Globe). Users can select a dataset type (Tree-like, Cycle-heavy, Random).\",\"complexity\":\"High\",\"variables\":[\"Graph Topology Type\",\"Curvature Value (κ)\"],\"expected_insights\":\"Users will visually see how anomalies 'pop out' (become outliers) in the correct geometry while remaining hidden inside clusters in the wrong geometry.\",\"visualization_type\":\"3d\"},{\"title\":\"Expert Router Flow\",\"description\":\"An interactive sankey diagram or flow animation where incoming graph nodes flow into different 'Expert Buckets' (Negative, Zero, Positive Curvature). As the user adjusts the structure of the incoming node (e.g., adding edges to make it a clique vs. a star), the node dynamically reroutes to a different expert.\",\"complexity\":\"Medium\",\"variables\":[\"Node Connectivity Pattern (Star, Clique, Chain)\",\"Router Temperature\"],\"expected_insights\":\"Demonstrates that the model intelligently sorts data based on structure, sending star-like patterns to Hyperbolic experts and dense cliques to Spherical experts.\",\"visualization_type\":\"interactive\"},{\"title\":\"Reconstruction Error Heatmap\",\"description\":\"A side-by-side view of a graph. Left: The ground truth graph. Right: A heatmap of the 'Anomaly Score' (Reconstruction Error). A slider allows the user to switch between using a 'Single Fixed Expert' vs. 'MoE'.\",\"complexity\":\"Medium\",\"variables\":[\"Model Type (Fixed Curvature vs. MoE)\",\"Anomaly Injection Intensity\"],\"expected_insights\":\"Users will see that the Single Expert model produces high error (false positives) on normal nodes that don't fit its geometry, while the MoE model keeps the background 'cool' (low error) and only highlights true anomalies.\",\"visualization_type\":\"chart\"}],\"field\":\"Graph Representation Learning\",\"related_fields\":[\"Anomaly Detection\",\"Riemannian Geometry\",\"Mixture of Experts (MoE)\",\"Zero-shot Learning\"],\"limitations\":[\"The complexity of computing Riemannian operations (exp/log maps) is higher than standard Euclidean operations.\",\"Requires tuning the number of experts and specific curvature initializations.\",\"Structure-contrastive loss on large graphs requires approximation (sampling).\",\"The paper focuses on node-level anomalies, not graph-level or edge-level specifically.\"],\"difficulty_to_understand\":\"Advanced\",\"prerequisites\":[\"Graph Neural Networks (GNNs)\",\"Autoencoders\",\"Basic Differential Geometry (Manifolds, Curvature)\",\"Mixture of Experts Architecture\"]}",
      "created_at": 1770642661400
    }
  },
  "prototypes": {
    "bdfaa68d8984f0dc02beaca527b76f207d99b666d31d1da728ee0728182df697-0-interactive-scaled-dot-product-attention": {
      "id": "bdfaa68d8984f0dc02beaca527b76f207d99b666d31d1da728ee0728182df697-0-interactive-scaled-dot-product-attention",
      "paper_hash": "bdfaa68d8984f0dc02beaca527b76f207d99b666d31d1da728ee0728182df697",
      "title": "Interactive Scaled Dot-Product Attention",
      "description": "A playground where users can input a small sentence (e.g., 'The cat sat'). The simulation visualizes the Query, Key, and Value vectors for each word. Users can manipulate the vectors or toggle the scaling factor (sqrt(d_k)) to see how it affects the Softmax distribution (sharpening vs. smoothing) and the final Attention output.",
      "code": "import React, { useState, useMemo, useEffect } from 'react';\nimport { motion, AnimatePresence } from 'framer-motion';\nimport { \n  Zap, \n  Settings2, \n  Type, \n  ArrowRight, \n  Info,\n  RefreshCcw\n} from 'lucide-react';\nimport { BarChart, Bar, XAxis, YAxis, Tooltip, ResponsiveContainer, Cell } from 'recharts';\n\n// Utility for Softmax calculation\nconst softmax = (arr) => {\n  const max = Math.max(...arr);\n  const exps = arr.map(x => Math.exp(x - max));\n  const sum = exps.reduce((a, b) => a + b, 0);\n  return exps.map(x => x / sum);\n};\n\n// Dot product of two vectors\nconst dotProduct = (a, b) => a.reduce((sum, val, i) => sum + val * (b[i] || 0), 0);\n\n// Generate random vectors for words\nconst generateVectors = (words, dim) => {\n  const vectors = {};\n  words.forEach(word => {\n    vectors[word] = {\n      q: Array.from({ length: dim }, () => (Math.random() * 2 - 1).toFixed(2)),\n      k: Array.from({ length: dim }, () => (Math.random() * 2 - 1).toFixed(2)),\n      v: Array.from({ length: dim }, () => (Math.random() * 2 - 1).toFixed(2)),\n    };\n  });\n  return vectors;\n};\n\nexport default function ScaledAttentionPlayground() {\n  const [inputText, setInputText] = useState(\"The cat sat\");\n  const [dk, setDk] = useState(64);\n  const [useScaling, setUseScaling] = useState(true);\n  const [selectedWordIdx, setSelectedWordIdx] = useState(0);\n  const [vectors, setVectors] = useState({});\n\n  const words = useMemo(() => inputText.split(/\\s+/).filter(w => w.length > 0), [inputText]);\n  const vectorDim = 4;\n\n  // Initialize or update vectors when words change\n  useEffect(() => {\n    setVectors(prev => {\n      const newVectors = { ...prev };\n      let updated = false;\n      words.forEach(word => {\n        if (!newVectors[word]) {\n          newVectors[word] = {\n            q: Array.from({ length: vectorDim }, () => (Math.random() * 2 - 1).toFixed(2)),\n            k: Array.from({ length: vectorDim }, () => (Math.random() * 2 - 1).toFixed(2)),\n            v: Array.from({ length: vectorDim }, () => (Math.random() * 2 - 1).toFixed(2)),\n          };\n          updated = true;\n        }\n      });\n      return updated ? newVectors : prev;\n    });\n    \n    if (selectedWordIdx >= words.length) setSelectedWordIdx(0);\n  }, [words]);\n\n  // Calculation Logic with safety checks to prevent 'undefined' errors\n  const attentionStats = useMemo(() => {\n    if (!words.length || words.length === 0) return null;\n    \n    const queryWord = words[selectedWordIdx];\n    // Safety check: ensure the query word exists in the vectors dictionary\n    if (!vectors[queryWord]) return null;\n\n    const q = vectors[queryWord].q.map(Number);\n    \n    // 1. Calculate Dot Products (Q * K^T) - using safety checks for each key word\n    const dotProducts = words.map(w => {\n      const keyVec = vectors[w]?.k;\n      return keyVec ? dotProduct(q, keyVec.map(Number)) : 0;\n    });\n    \n    // 2. Apply Scaling\n    const scaleFactor = useScaling ? Math.sqrt(dk) : 1;\n    const scaledScores = dotProducts.map(s => s / scaleFactor);\n    \n    // 3. Softmax\n    const weights = softmax(scaledScores);\n\n    // 4. Output (Weighted sum of V)\n    const outputVector = Array(vectorDim).fill(0);\n    weights.forEach((weight, i) => {\n      const v = vectors[words[i]]?.v;\n      if (v) {\n        v.forEach((val, j) => {\n          outputVector[j] += weight * Number(val);\n        });\n      }\n    });\n\n    return {\n      queryWord,\n      dotProducts,\n      scaledScores,\n      weights,\n      outputVector,\n      scaleFactor\n    };\n  }, [words, vectors, selectedWordIdx, dk, useScaling]);\n\n  const handleVectorChange = (word, type, index, value) => {\n    setVectors(prev => ({\n      ...prev,\n      [word]: {\n        ...prev[word],\n        [type]: prev[word][type].map((v, i) => i === index ? value : v)\n      }\n    }));\n  };\n\n  const chartData = (attentionStats && words.length > 0) ? words.map((w, i) => ({\n    name: w,\n    weight: ((attentionStats.weights[i] || 0) * 100).toFixed(1),\n    score: (attentionStats.scaledScores[i] || 0).toFixed(2)\n  })) : [];\n\n  return (\n    <div className=\"min-h-screen bg-gray-950 text-slate-200 p-4 md:p-8 font-sans\">\n      <header className=\"max-w-6xl mx-auto mb-8\">\n        <div className=\"flex items-center gap-3 mb-2\">\n          <div className=\"bg-blue-600 p-2 rounded-lg\">\n            <Zap className=\"text-white w-6 h-6\" />\n          </div>\n          <h1 className=\"text-3xl font-bold tracking-tight text-white\">Scaled Dot-Product Attention</h1>\n        </div>\n        <p className=\"text-slate-400 max-w-2xl text-sm md:text-base\">\n          Interactive visualization of the core mechanism from \"Attention Is All You Need\". \n          See how scaling by √d_k prevents dot products from growing too large and keeps gradients stable.\n        </p>\n      </header>\n\n      <main className=\"max-w-6xl mx-auto grid grid-cols-1 lg:grid-cols-12 gap-6\">\n        \n        {/* Left Column: Configuration & Inputs */}\n        <div className=\"lg:col-span-4 space-y-6\">\n          <section className=\"bg-gray-900/50 border border-white/10 rounded-2xl p-5 backdrop-blur-sm\">\n            <h2 className=\"flex items-center gap-2 text-sm font-semibold text-blue-400 uppercase tracking-wider mb-4\">\n              <Type size={16} /> Input Sequence\n            </h2>\n            <input \n              type=\"text\"\n              value={inputText}\n              onChange={(e) => setInputText(e.target.value)}\n              className=\"w-full bg-gray-800 border border-white/10 rounded-lg px-4 py-2 text-white focus:outline-none focus:ring-2 focus:ring-blue-500 mb-4\"\n              placeholder=\"Type a sentence...\"\n            />\n            \n            <div className=\"space-y-4\">\n              <div>\n                <div className=\"flex justify-between mb-2\">\n                  <label className=\"text-xs text-slate-400\">Key Dimension (d_k)</label>\n                  <span className=\"text-xs font-mono text-blue-400\">{dk}</span>\n                </div>\n                <input \n                  type=\"range\" min=\"1\" max=\"1024\" step=\"1\"\n                  value={dk}\n                  onChange={(e) => setDk(parseInt(e.target.value))}\n                  className=\"w-full h-1.5 bg-gray-800 rounded-lg appearance-none cursor-pointer accent-blue-500\"\n                />\n              </div>\n\n              <div className=\"flex items-center justify-between p-3 bg-gray-800/50 rounded-xl border border-white/5\">\n                <div className=\"flex items-center gap-2\">\n                  <Settings2 size={16} className=\"text-slate-400\" />\n                  <span className=\"text-sm\">Apply Scaling (1/√d_k)</span>\n                </div>\n                <button \n                  onClick={() => setUseScaling(!useScaling)}\n                  className={`w-12 h-6 rounded-full transition-colors relative ${useScaling ? 'bg-blue-600' : 'bg-gray-700'}`}\n                >\n                  <div className={`absolute top-1 w-4 h-4 rounded-full bg-white transition-all ${useScaling ? 'right-1' : 'left-1'}`} />\n                </button>\n              </div>\n            </div>\n          </section>\n\n          <section className=\"bg-gray-900/50 border border-white/10 rounded-2xl p-5\">\n            <div className=\"flex justify-between items-center mb-4\">\n              <h2 className=\"text-sm font-semibold text-purple-400 uppercase tracking-wider\">Vector Matrix (Q, K, V)</h2>\n              <button onClick={() => setVectors(generateVectors(words, vectorDim))} className=\"p-1 hover:bg-white/10 rounded\">\n                <RefreshCcw size={14} />\n              </button>\n            </div>\n            <div className=\"space-y-3 overflow-y-auto max-h-[400px] pr-2 custom-scrollbar\">\n              {words.map((word, wIdx) => (\n                <div key={`${word}-${wIdx}`} className={`p-3 rounded-xl border transition-all ${selectedWordIdx === wIdx ? 'bg-blue-500/10 border-blue-500/50' : 'bg-gray-800/30 border-white/5'}`}>\n                  <div className=\"flex justify-between items-center mb-2\">\n                    <button \n                      onClick={() => setSelectedWordIdx(wIdx)}\n                      className=\"text-sm font-bold text-white truncate hover:text-blue-400\"\n                    >\n                      {word}\n                    </button>\n                    {selectedWordIdx === wIdx && <span className=\"text-[10px] bg-blue-600 px-1.5 rounded text-white font-mono uppercase\">Query</span>}\n                  </div>\n                  <div className=\"grid grid-cols-4 gap-1\">\n                    {['q', 'k', 'v'].map(type => (\n                      <div key={type} className=\"flex flex-col gap-1\">\n                        <span className=\"text-[9px] uppercase text-slate-500 font-bold text-center\">{type}</span>\n                        {vectors[word]?.[type]?.map((val, i) => (\n                          <input \n                            key={i}\n                            type=\"number\"\n                            step=\"0.1\"\n                            value={val}\n                            onChange={(e) => handleVectorChange(word, type, i, e.target.value)}\n                            className=\"bg-black/40 text-[10px] text-center rounded py-0.5 border border-white/5 text-slate-300 w-full\"\n                          />\n                        ))}\n                      </div>\n                    ))}\n                  </div>\n                </div>\n              ))}\n            </div>\n          </section>\n        </div>\n\n        {/* Right Column: Visualization */}\n        <div className=\"lg:col-span-8 space-y-6\">\n          <div className=\"grid grid-cols-1 md:grid-cols-2 gap-6\">\n            {/* Attention Distribution */}\n            <section className=\"bg-gray-900/50 border border-white/10 rounded-2xl p-6 min-h-[350px] flex flex-col\">\n              <div className=\"flex justify-between items-center mb-6\">\n                <h2 className=\"text-sm font-semibold text-emerald-400 uppercase tracking-wider\">Softmax Weights (α)</h2>\n                <div className=\"text-[10px] text-slate-500 bg-white/5 px-2 py-1 rounded-md\">\n                  Σ = 100%\n                </div>\n              </div>\n              <div className=\"flex-1 min-h-[200px]\">\n                <ResponsiveContainer width=\"100%\" height=\"100%\">\n                  <BarChart data={chartData}>\n                    <XAxis dataKey=\"name\" axisLine={false} tickLine={false} tick={{fill: '#94a3b8', fontSize: 12}} />\n                    <Tooltip \n                      cursor={{fill: 'rgba(255,255,255,0.05)'}}\n                      contentStyle={{backgroundColor: '#0f172a', border: '1px solid #1e293b', borderRadius: '8px'}}\n                    />\n                    <Bar dataKey=\"weight\" radius={[4, 4, 0, 0]} name=\"Weight (%)\">\n                      {chartData.map((entry, index) => (\n                        <Cell key={`cell-${index}`} fill={index === selectedWordIdx ? '#3b82f6' : '#10b981'} fillOpacity={0.8} />\n                      ))}\n                    </Bar>\n                  </BarChart>\n                </ResponsiveContainer>\n              </div>\n              <p className=\"mt-4 text-xs text-slate-500 leading-relaxed italic\">\n                {words[selectedWordIdx] ? `How much the query \"${words[selectedWordIdx]}\" focuses on other words.` : \"Enter text to see attention weights.\"}\n              </p>\n            </section>\n\n            {/* Math Breakdown */}\n            <section className=\"bg-gray-900/50 border border-white/10 rounded-2xl p-6 flex flex-col\">\n              <h2 className=\"text-sm font-semibold text-orange-400 uppercase tracking-wider mb-6\">Step-by-Step Logic</h2>\n              <div className=\"space-y-4 flex-1\">\n                <div className=\"p-3 bg-black/30 rounded-lg border border-white/5\">\n                  <div className=\"text-[10px] text-slate-500 uppercase font-bold mb-1\">1. Dot Product (Similarity)</div>\n                  <div className=\"text-sm font-mono text-slate-300\">\n                    q · k_i = <span className=\"text-orange-300\">{attentionStats?.dotProducts?.[selectedWordIdx]?.toFixed(2) || \"0.00\"}</span>\n                  </div>\n                </div>\n\n                <div className=\"p-3 bg-black/30 rounded-lg border border-white/5\">\n                  <div className=\"text-[10px] text-slate-500 uppercase font-bold mb-1\">2. Scaling Factor (√d_k)</div>\n                  <div className=\"text-sm font-mono text-slate-300\">\n                    {useScaling ? (\n                      <>Score ÷ √{dk} = <span className=\"text-orange-300\">{attentionStats?.scaledScores?.[selectedWordIdx]?.toFixed(2) || \"0.00\"}</span></>\n                    ) : (\n                      <span className=\"text-slate-500\">Scaling Disabled (Factor = 1.0)</span>\n                    )}\n                  </div>\n                </div>\n\n                <div className=\"p-3 bg-black/30 rounded-lg border border-white/5\">\n                  <div className=\"text-[10px] text-slate-500 uppercase font-bold mb-1\">3. Softmax Distribution</div>\n                  <div className=\"flex flex-wrap gap-1 mt-2\">\n                    {attentionStats?.weights ? attentionStats.weights.map((w, i) => (\n                      <div key={i} className=\"h-6 flex items-center bg-emerald-500/20 px-2 rounded border border-emerald-500/30 text-[10px] font-mono\">\n                        {words[i]}: {(w * 100).toFixed(0)}%\n                      </div>\n                    )) : <span className=\"text-xs text-slate-600\">Waiting for input...</span>}\n                  </div>\n                </div>\n              </div>\n\n              <div className=\"mt-6 p-4 bg-blue-600/10 border border-blue-500/20 rounded-xl\">\n                <div className=\"flex items-start gap-3\">\n                  <Info size={18} className=\"text-blue-400 mt-0.5\" />\n                  <p className=\"text-[11px] text-slate-400 leading-relaxed\">\n                    <strong>The Scaling Rule:</strong> As d_k grows, dot products increase in magnitude. This pushes Softmax into regions with extremely small gradients. Dividing by √d_k \"sharpens\" or \"smoothes\" the distribution to keep training stable.\n                  </p>\n                </div>\n              </div>\n            </section>\n          </div>\n\n          {/* Attention Output Visualization */}\n          <section className=\"bg-gray-900/50 border border-white/10 rounded-2xl p-6 overflow-hidden\">\n            <h2 className=\"text-sm font-semibold text-white uppercase tracking-wider mb-6\">Final Attention Output (Context Vector)</h2>\n            \n            <div className=\"flex flex-col md:flex-row items-center gap-8\">\n              <div className=\"flex flex-col gap-2 w-full md:w-auto\">\n                <div className=\"text-xs text-slate-500 mb-2\">Linear Weighted Sum: ∑ (α_i × v_i)</div>\n                <div className=\"flex gap-2 justify-center md:justify-start\">\n                  {attentionStats?.outputVector ? attentionStats.outputVector.map((val, i) => (\n                    <motion.div \n                      key={i}\n                      initial={{ scale: 0.8 }}\n                      animate={{ scale: 1 }}\n                      className=\"w-16 h-16 bg-gradient-to-br from-blue-600 to-indigo-700 rounded-xl flex items-center justify-center text-white font-mono font-bold border border-white/10 shadow-lg shadow-blue-500/20\"\n                    >\n                      {val.toFixed(2)}\n                    </motion.div>\n                  )) : Array(4).fill(0).map((_, i) => (\n                    <div key={i} className=\"w-16 h-16 bg-gray-800 rounded-xl border border-white/5\" />\n                  ))}\n                </div>\n              </div>\n\n              <div className=\"hidden md:block\">\n                <ArrowRight className=\"text-slate-700\" size={32} />\n              </div>\n\n              <div className=\"flex-1 bg-black/40 p-4 rounded-xl border border-white/5 w-full\">\n                <div className=\"text-xs text-slate-500 mb-3\">Interpretation</div>\n                <p className=\"text-sm text-slate-300 leading-relaxed\">\n                  The final vector is a <strong>semantic blend</strong> of the input sentence, weighted by its relevance to the word \n                  <span className=\"text-blue-400 font-bold mx-1\">\"{words[selectedWordIdx] || \"selected word\"}\"</span>.\n                  Notice how toggling \"Scaling\" makes the distribution flatter (more uniform) or peakier (sharper focus).\n                </p>\n              </div>\n            </div>\n          </section>\n        </div>\n      </main>\n\n      <footer className=\"max-w-6xl mx-auto mt-12 pt-8 border-t border-white/5 text-center\">\n        <p className=\"text-slate-500 text-xs\">\n          Interactive Transformer Lab — Based on Equation 1: Attention(Q, K, V) = softmax(QKᵀ / √dₖ)V\n        </p>\n      </footer>\n\n      <style dangerouslySetInnerHTML={{ __html: `\n        .custom-scrollbar::-webkit-scrollbar {\n          width: 4px;\n        }\n        .custom-scrollbar::-webkit-scrollbar-track {\n          background: rgba(255, 255, 255, 0.02);\n        }\n        .custom-scrollbar::-webkit-scrollbar-thumb {\n          background: rgba(255, 255, 255, 0.1);\n          border-radius: 10px;\n        }\n      `}} />\n    </div>\n  );\n}\n",
      "algorithm_info": "",
      "history": [
        {
          "role": "user",
          "parts": [
            {
              "text": "Create an interactive prototype for \"Interactive Scaled Dot-Product Attention\". Description: A playground where users can input a small sentence (e.g., 'The cat sat'). The simulation visualizes the Query, Key, and Value vectors for each word. Users can manipulate the vectors or toggle the scaling factor (sqrt(d_k)) to see how it affects the Softmax distribution (sharpening vs. smoothing) and the final Attention output.\n\nPAPER CONTENT:\nProvided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\n\n# Attention Is All You Need\n\nAshish Vaswani*\nGoogle Brain\navaswani@google.com\n\nNoam Shazeer*\nGoogle Brain\nnoam@google.com\n\nNiki Parmar*\nGoogle Research\nnikip@google.com\n\nJakob Uszkoreit*\nGoogle Research\nusz@google.com\n\nLlion Jones*\nGoogle Research\nllion@google.com\n\nAidan N. Gomez* †\nUniversity of Toronto\naidan@cs.toronto.edu\n\nŁukasz Kaiser*\nGoogle Brain\nlukaszkaiser@google.com\n\nIllia Polosukhin* ‡\nillia.polosukhin@gmail.com\n\n### Abstract\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n\n---\n\\* Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\n† Work performed while at Google Brain.\n‡ Work performed while at Google Research.\n\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7 [cs.CL] 2 Aug 2023\n\n---\n## 1 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].\n\nRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h_t, as a function of the previous hidden state h_{t-1} and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\n\nAttention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.\n\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n\n## 2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\n\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\n\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].\n\n## 3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x_1, ..., x_n) to a sequence of continuous representations z = (z_1, ..., z_n). Given z, the decoder then generates an output sequence (y_1, ..., y_m) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.\n\n2\n\n---\nFigure 1: The Transformer - model architecture.\n\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\n\n### 3.1 Encoder and Decoder Stacks\n\n**Encoder:** The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d_model = 512.\n\n**Decoder:** The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n\n### 3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n\n3\n\n---\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n\nof the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n\n#### 3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension d_k, and values of dimension d_v. We compute the dot products of the query with all keys, divide each by sqrt(d_k), and apply a softmax function to obtain the weights on the values.\n\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. We compute the matrix of outputs as:\n\nAttention(Q, K, V) = softmax( (Q * K^T) / sqrt(d_k) ) * V    (1)\n\nThe two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1 / sqrt(d_k). Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n\nWhile for small values of d_k the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d_k [3]. We suspect that for large values of d_k, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients^4. To counteract this effect, we scale the dot products by 1 / sqrt(d_k).\n\n#### 3.2.2 Multi-Head Attention\nInstead of performing a single attention function with d_model-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d_k, d_k and d_v dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d_v-dimensional\n\n---\n^4 To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q · k = sum_{i=1}^{d_k} q_i * k_i, has mean 0 and variance d_k.\n\n4\n\n---\noutput values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\n\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n\nMultiHead(Q, K, V) = Concat(head_1, ..., head_h) * W^O\nwhere head_i = Attention(Q * W_i^Q, K * W_i^K, V * W_i^V)\n\nWhere the projections are parameter matrices W_i^Q ∈ R^{d_model × d_k}, W_i^K ∈ R^{d_model × d_k}, W_i^V ∈ R^{d_model × d_v} and W^O ∈ R^{h * d_v × d_model}.\n\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use d_k = d_v = d_model / h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n\n#### 3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n* In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\n* The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n* Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to -∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\n\n### 3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n\nFFN(x) = max(0, x * W_1 + b_1) * W_2 + b_2    (2)\n\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is d_model = 512, and the inner-layer has dimensionality d_ff = 2048.\n\n### 3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d_model. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by sqrt(d_model).\n\n5\n\n---\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.\n\n| Layer Type | Complexity per Layer | Sequential Operations | Maximum Path Length |\n| :--- | :--- | :--- | :--- |\n| Self-Attention | O(n^2 · d) | O(1) | O(1) |\n| Recurrent | O(n · d^2) | O(n) | O(n) |\n| Convolutional | O(k · n · d^2) | O(1) | O(log_k(n)) |\n| Self-Attention (restricted) | O(r · n · d) | O(1) | O(n/r) |\n\n### 3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d_model as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\n\nIn this work, we use sine and cosine functions of different frequencies:\n\nPE_{(pos,2i)} = sin(pos / 10000^{2i/d_model})\nPE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_model})\n\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PE_{pos+k} can be represented as a linear function of PE_{pos}.\n\nWe also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n\n## 4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x_1, ..., x_n) to another sequence of equal length (z_1, ..., z_n), with x_i, z_i ∈ R^d, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\n\nOne is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\n\nThe third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\n\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\n\n6\n\n---\nlength n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.\n\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(log_k(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k · n · d + n · d^2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\n\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\n\n## 5 Training\nThis section describes the training regime for our models.\n\n### 5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n\n### 5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\n\n### 5.3 Optimizer\nWe used the Adam optimizer [20] with β_1 = 0.9, β_2 = 0.98 and ε = 10^{-9}. We varied the learning rate over the course of training, according to the formula:\n\nlrate = d_{model}^{-0.5} · min(step_num^{-0.5}, step_num · warmup_steps^{-1.5})    (3)\n\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\n\n### 5.4 Regularization\nWe employ three types of regularization during training:\n\n7\n\n---\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n\n| Model | BLEU EN-DE | BLEU EN-FR | Training Cost (FLOPs) EN-DE | Training Cost (FLOPs) EN-FR |\n| :--- | :---: | :---: | :---: | :---: |\n| ByteNet [18] | 23.75 | | | |\n| Deep-Att + PosUnk [39] | | 39.2 | | 1.0 · 10^{20} |\n| GNMT + RL [38] | 24.6 | 39.92 | 2.3 · 10^{19} | 1.4 · 10^{20} |\n| ConvS2S [9] | 25.16 | 40.46 | 9.6 · 10^{18} | 1.5 · 10^{20} |\n| MoE [32] | 26.03 | 40.56 | 2.0 · 10^{19} | 1.2 · 10^{20} |\n| Deep-Att + PosUnk Ensemble [39] | | 40.4 | | 8.0 · 10^{20} |\n| GNMT + RL Ensemble [38] | 26.30 | 41.16 | 1.8 · 10^{20} | 1.1 · 10^{21} |\n| ConvS2S Ensemble [9] | 26.36 | **41.29** | 7.7 · 10^{19} | 1.2 · 10^{21} |\n| Transformer (base model) | 27.3 | 38.1 | **3.3 · 10^{18}** | |\n| Transformer (big) | **28.4** | **41.8** | 2.3 · 10^{19} | |\n\n**Residual Dropout** We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P_{drop} = 0.1.\n\n**Label Smoothing** During training, we employed label smoothing of value ε_{ls} = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n\n## 6 Results\n### 6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate P_{drop} = 0.1, instead of 0.3.\n\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38].\n\nTable 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU^5.\n\n### 6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\n\n---\n^5 We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n\n8\n\n---\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\n\n| | N | d_{model} | d_{ff} | h | d_k | d_v | P_{drop} | ε_{ls} | train steps | PPL (dev) | BLEU (dev) | params ×10^6 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| base | 6 | 512 | 2048 | 8 | 64 | 64 | 0.1 | 0.1 | 100K | 4.92 | 25.8 | 65 |\n| (A) | | | | 1 | 512 | 512 | | | | 5.29 | 24.9 | |\n| | | | | 4 | 128 | 128 | | | | 5.00 | 25.5 | |\n| | | | | 16 | 32 | 32 | | | | 4.91 | 25.8 | |\n| | | | | 32 | 16 | 16 | | | | 5.01 | 25.4 | |\n| (B) | | | | | 16 | | | | | 5.16 | 25.1 | 58 |\n| | | | | | 32 | | | | | 5.01 | 25.4 | 60 |\n| (C) | 2 | | | | | | | | | 6.11 | 23.7 | 36 |\n| | 4 | | | | | | | | | 5.19 | 25.3 | 50 |\n| | 8 | | | | | | | | | 4.88 | 25.5 | 80 |\n| | | 256 | | | 32 | 32 | | | | 5.75 | 24.5 | 28 |\n| | | 1024 | | | 128 | 128 | | | | 4.66 | 26.0 | 168 |\n| | | | 1024 | | | | | | | 5.12 | 25.4 | 53 |\n| | | | 4096 | | | | | | | 4.75 | 26.2 | 90 |\n| (D) | | | | | | | 0.0 | | | 5.77 | 24.6 | |\n| | | | | | | | 0.2 | | | 4.95 | 25.5 | |\n| | | | | | | | | 0.0 | | 4.67 | 25.3 | |\n| | | | | | | | | 0.2 | | 5.47 | 25.7 | |\n| (E) | | positional embedding instead of sinusoids | | | | | | 4.92 | 25.7 | |\n| big | 6 | 1024 | 4096 | 16 | | | 0.3 | | 300K | **4.33** | **26.4** | 213 |\n\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\n\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n\nIn Table 3 rows (B), we observe that reducing the attention key size d_k hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.\n\n### 6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].\n\nWe trained a 4-layer transformer with d_model = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\n\nWe performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\n\n9\n\n---\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\n\n| Parser | Training | WSJ 23 F1 |\n| :--- | :--- | :---: |\n| Vinyals & Kaiser el al. (2014) [37] | WSJ only, discriminative | 88.3 |\n| Petrov et al. (2006) [29] | WSJ only, discriminative | 90.4 |\n| Zhu et al. (2013) [40] | WSJ only, discriminative | 90.4 |\n| Dyer et al. (2016) [8] | WSJ only, discriminative | 91.7 |\n| Transformer (4 layers) | WSJ only, discriminative | 91.3 |\n| Zhu et al. (2013) [40] | semi-supervised | 91.3 |\n| Huang & Harper (2009) [14] | semi-supervised | 91.3 |\n| McClosky et al. (2006) [26] | semi-supervised | 92.1 |\n| Vinyals & Kaiser el al. (2014) [37] | semi-supervised | 92.1 |\n| Transformer (4 layers) | semi-supervised | **92.7** |\n| Luong et al. (2015) [23] | multi-task | 93.0 |\n| Dyer et al. (2016) [8] | generative | 93.3 |\n\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3 for both WSJ only and the semi-supervised setting.\n\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\n\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-Parser [29] even when training only on the WSJ training set of 40K sentences.\n\n## 7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n\nFor translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\n\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\n\nThe code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.\n\n**Acknowledgements** We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\n\n## References\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.\n\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017.\n\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016.\n\n10\n\n---\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\n\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.\n\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016.\n\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\n\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.\n\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\n\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\n\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832–841. ACL, August 2009.\n\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016.\n\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016.\n\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.\n\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017.\n\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017.\n\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.\n\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n\n11\n\n---\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\n\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159. ACL, June 2006.\n\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.\n\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.\n\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006.\n\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.\n\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.\n\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\n\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958, 2014.\n\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\n\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\n\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015.\n\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\n\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013.\n\n12\n\n---\n## Attention Visualizations\n\n[Figure showing attention heads for the sentence: \"It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult.\"]\n\nFigure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.\n\n13\n\n---\n[Figures showing attention heads for the sentence: \"The Law will never be perfect, but its application should be just - this is what we are missing, in my opinion.\"]\n\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word.\n\n14\n\n---\n[Figures showing attention heads for the sentence: \"The Law will never be perfect, but its application should be just - this is what we are missing, in my opinion.\"]\n\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\n\n15"
            }
          ]
        },
        {
          "role": "model",
          "parts": [
            {
              "functionCall": {
                "name": "render_prototype",
                "args": {
                  "title": "Interactive Scaled Dot-Product Attention Playground",
                  "code": "import React, { useState, useMemo, useEffect } from 'react';\nimport { motion, AnimatePresence } from 'framer-motion';\nimport { \n  Zap, \n  Settings2, \n  Type, \n  ArrowRight, \n  Info,\n  RefreshCcw,\n  Sliders\n} from 'lucide-react';\nimport { BarChart, Bar, XAxis, YAxis, Tooltip, ResponsiveContainer, Cell } from 'recharts';\n\n// Utility for Softmax calculation\nconst softmax = (arr) => {\n  const max = Math.max(...arr);\n  const exps = arr.map(x => Math.exp(x - max));\n  const sum = exps.reduce((a, b) => a + b, 0);\n  return exps.map(x => x / sum);\n};\n\n// Dot product of two vectors\nconst dotProduct = (a, b) => a.reduce((sum, val, i) => sum + val * (b[i] || 0), 0);\n\n// Generate random vectors for words\nconst generateVectors = (words, dim) => {\n  const vectors = {};\n  words.forEach(word => {\n    vectors[word] = {\n      q: Array.from({ length: dim }, () => (Math.random() * 2 - 1).toFixed(2)),\n      k: Array.from({ length: dim }, () => (Math.random() * 2 - 1).toFixed(2)),\n      v: Array.from({ length: dim }, () => (Math.random() * 2 - 1).toFixed(2)),\n    };\n  });\n  return vectors;\n};\n\nexport default function ScaledAttentionPlayground() {\n  const [inputText, setInputText] = useState(\"The cat sat\");\n  const [dk, setDk] = useState(64);\n  const [useScaling, setUseScaling] = useState(true);\n  const [selectedWordIdx, setSelectedWordIdx] = useState(0);\n  const [vectors, setVectors] = useState({});\n  const [isAutoPlaying, setIsAutoPlaying] = useState(false);\n\n  const words = useMemo(() => inputText.split(/\\s+/).filter(w => w.length > 0), [inputText]);\n  const vectorDim = 4; // Visual dimension for the UI (Paper uses 64)\n\n  // Initialize or update vectors when words change\n  useEffect(() => {\n    setVectors(generateVectors(words, vectorDim));\n    if (selectedWordIdx >= words.length) setSelectedWordIdx(0);\n  }, [words]);\n\n  // Calculation Logic\n  const attentionStats = useMemo(() => {\n    if (!words.length || !vectors[words[0]]) return null;\n\n    const queryWord = words[selectedWordIdx];\n    const q = vectors[queryWord].q.map(Number);\n    \n    // 1. Calculate Dot Products (Q * K^T)\n    const dotProducts = words.map(w => dotProduct(q, vectors[w].k.map(Number)));\n    \n    // 2. Apply Scaling\n    const scaleFactor = useScaling ? Math.sqrt(dk) : 1;\n    const scaledScores = dotProducts.map(s => s / scaleFactor);\n    \n    // 3. Softmax\n    const weights = softmax(scaledScores);\n\n    // 4. Output (Weighted sum of V)\n    const outputVector = Array(vectorDim).fill(0);\n    weights.forEach((weight, i) => {\n      const v = vectors[words[i]].v.map(Number);\n      v.forEach((val, j) => {\n        outputVector[j] += weight * val;\n      });\n    });\n\n    return {\n      queryWord,\n      dotProducts,\n      scaledScores,\n      weights,\n      outputVector,\n      scaleFactor\n    };\n  }, [words, vectors, selectedWordIdx, dk, useScaling]);\n\n  const handleVectorChange = (word, type, index, value) => {\n    setVectors(prev => ({\n      ...prev,\n      [word]: {\n        ...prev[word],\n        [type]: prev[word][type].map((v, i) => i === index ? value : v)\n      }\n    }));\n  };\n\n  const chartData = attentionStats ? words.map((w, i) => ({\n    name: w,\n    weight: (attentionStats.weights[i] * 100).toFixed(1),\n    score: attentionStats.scaledScores[i].toFixed(2)\n  })) : [];\n\n  return (\n    <div className=\"min-h-screen bg-gray-950 text-slate-200 p-4 md:p-8 font-sans\">\n      <header className=\"max-w-6xl mx-auto mb-8\">\n        <div className=\"flex items-center gap-3 mb-2\">\n          <div className=\"bg-blue-600 p-2 rounded-lg\">\n            <Zap className=\"text-white w-6 h-6\" />\n          </div>\n          <h1 className=\"text-3xl font-bold tracking-tight text-white\">Scaled Dot-Product Attention</h1>\n        </div>\n        <p className=\"text-slate-400 max-w-2xl text-sm md:text-base\">\n          Interactive visualization of the core mechanism from \"Attention Is All You Need\". \n          See how scaling by √d_k prevents dot products from growing too large and keeps gradients stable.\n        </p>\n      </header>\n\n      <main className=\"max-w-6xl mx-auto grid grid-cols-1 lg:grid-cols-12 gap-6\">\n        \n        {/* Left Column: Configuration & Inputs */}\n        <div className=\"lg:col-span-4 space-y-6\">\n          <section className=\"bg-gray-900/50 border border-white/10 rounded-2xl p-5 backdrop-blur-sm\">\n            <h2 className=\"flex items-center gap-2 text-sm font-semibold text-blue-400 uppercase tracking-wider mb-4\">\n              <Type size={16} /> Input Sequence\n            </h2>\n            <input \n              type=\"text\"\n              value={inputText}\n              onChange={(e) => setInputText(e.target.value)}\n              className=\"w-full bg-gray-800 border border-white/10 rounded-lg px-4 py-2 text-white focus:outline-none focus:ring-2 focus:ring-blue-500 mb-4\"\n              placeholder=\"Type a sentence...\"\n            />\n            \n            <div className=\"space-y-4\">\n              <div>\n                <div className=\"flex justify-between mb-2\">\n                  <label className=\"text-xs text-slate-400\">Key Dimension (d_k)</label>\n                  <span className=\"text-xs font-mono text-blue-400\">{dk}</span>\n                </div>\n                <input \n                  type=\"range\" min=\"1\" max=\"1024\" step=\"1\"\n                  value={dk}\n                  onChange={(e) => setDk(parseInt(e.target.value))}\n                  className=\"w-full h-1.5 bg-gray-800 rounded-lg appearance-none cursor-pointer accent-blue-500\"\n                />\n              </div>\n\n              <div className=\"flex items-center justify-between p-3 bg-gray-800/50 rounded-xl border border-white/5\">\n                <div className=\"flex items-center gap-2\">\n                  <Settings2 size={16} className=\"text-slate-400\" />\n                  <span className=\"text-sm\">Apply Scaling (1/√d_k)</span>\n                </div>\n                <button \n                  onClick={() => setUseScaling(!useScaling)}\n                  className={`w-12 h-6 rounded-full transition-colors relative ${useScaling ? 'bg-blue-600' : 'bg-gray-700'}`}\n                >\n                  <div className={`absolute top-1 w-4 h-4 rounded-full bg-white transition-all ${useScaling ? 'right-1' : 'left-1'}`} />\n                </button>\n              </div>\n            </div>\n          </section>\n\n          <section className=\"bg-gray-900/50 border border-white/10 rounded-2xl p-5\">\n            <div className=\"flex justify-between items-center mb-4\">\n              <h2 className=\"text-sm font-semibold text-purple-400 uppercase tracking-wider\">Vector Matrix (Q, K, V)</h2>\n              <button onClick={() => setVectors(generateVectors(words, vectorDim))} className=\"p-1 hover:bg-white/10 rounded\">\n                <RefreshCcw size={14} />\n              </button>\n            </div>\n            <div className=\"space-y-3 overflow-y-auto max-h-[400px] pr-2 custom-scrollbar\">\n              {words.map((word, wIdx) => (\n                <div key={`${word}-${wIdx}`} className={`p-3 rounded-xl border transition-all ${selectedWordIdx === wIdx ? 'bg-blue-500/10 border-blue-500/50' : 'bg-gray-800/30 border-white/5'}`}>\n                  <div className=\"flex justify-between items-center mb-2\">\n                    <button \n                      onClick={() => setSelectedWordIdx(wIdx)}\n                      className=\"text-sm font-bold text-white truncate hover:text-blue-400\"\n                    >\n                      {word}\n                    </button>\n                    {selectedWordIdx === wIdx && <span className=\"text-[10px] bg-blue-600 px-1.5 rounded text-white font-mono uppercase\">Query</span>}\n                  </div>\n                  <div className=\"grid grid-cols-4 gap-1\">\n                    {['q', 'k', 'v'].map(type => (\n                      <div key={type} className=\"flex flex-col gap-1\">\n                        <span className=\"text-[9px] uppercase text-slate-500 font-bold text-center\">{type}</span>\n                        {vectors[word]?.[type].map((val, i) => (\n                          <input \n                            key={i}\n                            type=\"number\"\n                            step=\"0.1\"\n                            value={val}\n                            onChange={(e) => handleVectorChange(word, type, i, e.target.value)}\n                            className=\"bg-black/40 text-[10px] text-center rounded py-0.5 border border-white/5 text-slate-300 w-full\"\n                          />\n                        ))}\n                      </div>\n                    ))}\n                  </div>\n                </div>\n              ))}\n            </div>\n          </section>\n        </div>\n\n        {/* Right Column: Visualization */}\n        <div className=\"lg:col-span-8 space-y-6\">\n          <div className=\"grid grid-cols-1 md:grid-cols-2 gap-6\">\n            {/* Attention Distribution */}\n            <section className=\"bg-gray-900/50 border border-white/10 rounded-2xl p-6 min-h-[350px] flex flex-col\">\n              <div className=\"flex justify-between items-center mb-6\">\n                <h2 className=\"text-sm font-semibold text-emerald-400 uppercase tracking-wider\">Softmax Weights (α)</h2>\n                <div className=\"text-[10px] text-slate-500 bg-white/5 px-2 py-1 rounded-md\">\n                  Σ = 100%\n                </div>\n              </div>\n              <div className=\"flex-1 min-h-[200px]\">\n                <ResponsiveContainer width=\"100%\" height=\"100%\">\n                  <BarChart data={chartData}>\n                    <XAxis dataKey=\"name\" axisLine={false} tickLine={false} tick={{fill: '#94a3b8', fontSize: 12}} />\n                    <Tooltip \n                      cursor={{fill: 'rgba(255,255,255,0.05)'}}\n                      contentStyle={{backgroundColor: '#0f172a', border: '1px solid #1e293b', borderRadius: '8px'}}\n                    />\n                    <Bar dataKey=\"weight\" radius={[4, 4, 0, 0]} name=\"Weight (%)\">\n                      {chartData.map((entry, index) => (\n                        <Cell key={`cell-${index}`} fill={index === selectedWordIdx ? '#3b82f6' : '#10b981'} fillOpacity={0.8} />\n                      ))}\n                    </Bar>\n                  </BarChart>\n                </ResponsiveContainer>\n              </div>\n              <p className=\"mt-4 text-xs text-slate-500 leading-relaxed italic\">\n                How much the query \"{words[selectedWordIdx]}\" focuses on other words.\n              </p>\n            </section>\n\n            {/* Math Breakdown */}\n            <section className=\"bg-gray-900/50 border border-white/10 rounded-2xl p-6 flex flex-col\">\n              <h2 className=\"text-sm font-semibold text-orange-400 uppercase tracking-wider mb-6\">Step-by-Step Logic</h2>\n              <div className=\"space-y-4 flex-1\">\n                <div className=\"p-3 bg-black/30 rounded-lg border border-white/5\">\n                  <div className=\"text-[10px] text-slate-500 uppercase font-bold mb-1\">1. Dot Product (Similarity)</div>\n                  <div className=\"text-sm font-mono text-slate-300\">\n                    q · k_i = <span className=\"text-orange-300\">{attentionStats?.dotProducts[selectedWordIdx]?.toFixed(2)}</span>\n                  </div>\n                </div>\n\n                <div className=\"p-3 bg-black/30 rounded-lg border border-white/5\">\n                  <div className=\"text-[10px] text-slate-500 uppercase font-bold mb-1\">2. Scaling Factor (√d_k)</div>\n                  <div className=\"text-sm font-mono text-slate-300\">\n                    {useScaling ? (\n                      <>Score ÷ √{dk} = <span className=\"text-orange-300\">{attentionStats?.scaledScores[selectedWordIdx]?.toFixed(2)}</span></>\n                    ) : (\n                      <span className=\"text-slate-500\">Scaling Disabled (Factor = 1.0)</span>\n                    )}\n                  </div>\n                </div>\n\n                <div className=\"p-3 bg-black/30 rounded-lg border border-white/5\">\n                  <div className=\"text-[10px] text-slate-500 uppercase font-bold mb-1\">3. Softmax Distribution</div>\n                  <div className=\"flex flex-wrap gap-1 mt-2\">\n                    {attentionStats?.weights.map((w, i) => (\n                      <div key={i} className=\"h-6 flex items-center bg-emerald-500/20 px-2 rounded border border-emerald-500/30 text-[10px] font-mono\">\n                        {words[i]}: {(w * 100).toFixed(0)}%\n                      </div>\n                    ))}\n                  </div>\n                </div>\n              </div>\n\n              <div className=\"mt-6 p-4 bg-blue-600/10 border border-blue-500/20 rounded-xl\">\n                <div className=\"flex items-start gap-3\">\n                  <Info size={18} className=\"text-blue-400 mt-0.5\" />\n                  <p className=\"text-[11px] text-slate-400 leading-relaxed\">\n                    <strong>The Scaling Rule:</strong> As d_k grows, dot products increase in magnitude. This pushes Softmax into regions with extremely small gradients. Dividing by √d_k \"sharpens\" or \"smoothes\" the distribution to keep training stable.\n                  </p>\n                </div>\n              </div>\n            </section>\n          </div>\n\n          {/* Attention Output Visualization */}\n          <section className=\"bg-gray-900/50 border border-white/10 rounded-2xl p-6 overflow-hidden\">\n            <h2 className=\"text-sm font-semibold text-white uppercase tracking-wider mb-6\">Final Attention Output (Context Vector)</h2>\n            \n            <div className=\"flex flex-col md:flex-row items-center gap-8\">\n              <div className=\"flex flex-col gap-2 w-full md:w-auto\">\n                <div className=\"text-xs text-slate-500 mb-2\">Linear Weighted Sum: ∑ (α_i × v_i)</div>\n                <div className=\"flex gap-2 justify-center md:justify-start\">\n                  {attentionStats?.outputVector.map((val, i) => (\n                    <motion.div \n                      key={i}\n                      initial={{ scale: 0.8 }}\n                      animate={{ scale: 1 }}\n                      className=\"w-16 h-16 bg-gradient-to-br from-blue-600 to-indigo-700 rounded-xl flex items-center justify-center text-white font-mono font-bold border border-white/10 shadow-lg shadow-blue-500/20\"\n                    >\n                      {val.toFixed(2)}\n                    </motion.div>\n                  ))}\n                </div>\n              </div>\n\n              <div className=\"hidden md:block\">\n                <ArrowRight className=\"text-slate-700\" size={32} />\n              </div>\n\n              <div className=\"flex-1 bg-black/40 p-4 rounded-xl border border-white/5 w-full\">\n                <div className=\"text-xs text-slate-500 mb-3\">Interpretation</div>\n                <p className=\"text-sm text-slate-300 leading-relaxed\">\n                  The final vector is a <strong>semantic blend</strong> of the input sentence, weighted by its relevance to the word \n                  <span className=\"text-blue-400 font-bold mx-1\">\"{words[selectedWordIdx]}\"</span>.\n                  Notice how toggling \"Scaling\" makes the distribution flatter (more uniform) or peakier (sharper focus).\n                </p>\n              </div>\n            </div>\n          </section>\n        </div>\n      </main>\n\n      <footer className=\"max-w-6xl mx-auto mt-12 pt-8 border-t border-white/5 text-center\">\n        <p className=\"text-slate-500 text-xs\">\n          Interactive Transformer Lab — Based on Equation 1: Attention(Q, K, V) = softmax(QKᵀ / √dₖ)V\n        </p>\n      </footer>\n\n      <style dangerouslySetInnerHTML={{ __html: `\n        .custom-scrollbar::-webkit-scrollbar {\n          width: 4px;\n        }\n        .custom-scrollbar::-webkit-scrollbar-track {\n          background: rgba(255, 255, 255, 0.02);\n        }\n        .custom-scrollbar::-webkit-scrollbar-thumb {\n          background: rgba(255, 255, 255, 0.1);\n          border-radius: 10px;\n        }\n      `}} />\n    </div>\n  );\n}\n"
                }
              }
            }
          ]
        },
        {
          "role": "user",
          "parts": [
            {
              "functionResponse": {
                "name": "render_prototype",
                "response": {
                  "success": true,
                  "rendered": true
                }
              }
            }
          ]
        },
        {
          "role": "user",
          "parts": [
            {
              "text": "The prototype has an error: Uncaught TypeError: Cannot read properties of undefined (reading 'k') (Line 8)\n\nPlease fix it."
            }
          ]
        },
        {
          "role": "model",
          "parts": [
            {
              "functionCall": {
                "name": "render_prototype",
                "args": {
                  "title": "Interactive Scaled Dot-Product Attention Playground",
                  "code": "import React, { useState, useMemo, useEffect } from 'react';\nimport { motion, AnimatePresence } from 'framer-motion';\nimport { \n  Zap, \n  Settings2, \n  Type, \n  ArrowRight, \n  Info,\n  RefreshCcw\n} from 'lucide-react';\nimport { BarChart, Bar, XAxis, YAxis, Tooltip, ResponsiveContainer, Cell } from 'recharts';\n\n// Utility for Softmax calculation\nconst softmax = (arr) => {\n  const max = Math.max(...arr);\n  const exps = arr.map(x => Math.exp(x - max));\n  const sum = exps.reduce((a, b) => a + b, 0);\n  return exps.map(x => x / sum);\n};\n\n// Dot product of two vectors\nconst dotProduct = (a, b) => a.reduce((sum, val, i) => sum + val * (b[i] || 0), 0);\n\n// Generate random vectors for words\nconst generateVectors = (words, dim) => {\n  const vectors = {};\n  words.forEach(word => {\n    vectors[word] = {\n      q: Array.from({ length: dim }, () => (Math.random() * 2 - 1).toFixed(2)),\n      k: Array.from({ length: dim }, () => (Math.random() * 2 - 1).toFixed(2)),\n      v: Array.from({ length: dim }, () => (Math.random() * 2 - 1).toFixed(2)),\n    };\n  });\n  return vectors;\n};\n\nexport default function ScaledAttentionPlayground() {\n  const [inputText, setInputText] = useState(\"The cat sat\");\n  const [dk, setDk] = useState(64);\n  const [useScaling, setUseScaling] = useState(true);\n  const [selectedWordIdx, setSelectedWordIdx] = useState(0);\n  const [vectors, setVectors] = useState({});\n\n  const words = useMemo(() => inputText.split(/\\s+/).filter(w => w.length > 0), [inputText]);\n  const vectorDim = 4;\n\n  // Initialize or update vectors when words change\n  useEffect(() => {\n    setVectors(prev => {\n      const newVectors = { ...prev };\n      let updated = false;\n      words.forEach(word => {\n        if (!newVectors[word]) {\n          newVectors[word] = {\n            q: Array.from({ length: vectorDim }, () => (Math.random() * 2 - 1).toFixed(2)),\n            k: Array.from({ length: vectorDim }, () => (Math.random() * 2 - 1).toFixed(2)),\n            v: Array.from({ length: vectorDim }, () => (Math.random() * 2 - 1).toFixed(2)),\n          };\n          updated = true;\n        }\n      });\n      return updated ? newVectors : prev;\n    });\n    \n    if (selectedWordIdx >= words.length) setSelectedWordIdx(0);\n  }, [words]);\n\n  // Calculation Logic with safety checks to prevent 'undefined' errors\n  const attentionStats = useMemo(() => {\n    if (!words.length || words.length === 0) return null;\n    \n    const queryWord = words[selectedWordIdx];\n    // Safety check: ensure the query word exists in the vectors dictionary\n    if (!vectors[queryWord]) return null;\n\n    const q = vectors[queryWord].q.map(Number);\n    \n    // 1. Calculate Dot Products (Q * K^T) - using safety checks for each key word\n    const dotProducts = words.map(w => {\n      const keyVec = vectors[w]?.k;\n      return keyVec ? dotProduct(q, keyVec.map(Number)) : 0;\n    });\n    \n    // 2. Apply Scaling\n    const scaleFactor = useScaling ? Math.sqrt(dk) : 1;\n    const scaledScores = dotProducts.map(s => s / scaleFactor);\n    \n    // 3. Softmax\n    const weights = softmax(scaledScores);\n\n    // 4. Output (Weighted sum of V)\n    const outputVector = Array(vectorDim).fill(0);\n    weights.forEach((weight, i) => {\n      const v = vectors[words[i]]?.v;\n      if (v) {\n        v.forEach((val, j) => {\n          outputVector[j] += weight * Number(val);\n        });\n      }\n    });\n\n    return {\n      queryWord,\n      dotProducts,\n      scaledScores,\n      weights,\n      outputVector,\n      scaleFactor\n    };\n  }, [words, vectors, selectedWordIdx, dk, useScaling]);\n\n  const handleVectorChange = (word, type, index, value) => {\n    setVectors(prev => ({\n      ...prev,\n      [word]: {\n        ...prev[word],\n        [type]: prev[word][type].map((v, i) => i === index ? value : v)\n      }\n    }));\n  };\n\n  const chartData = (attentionStats && words.length > 0) ? words.map((w, i) => ({\n    name: w,\n    weight: ((attentionStats.weights[i] || 0) * 100).toFixed(1),\n    score: (attentionStats.scaledScores[i] || 0).toFixed(2)\n  })) : [];\n\n  return (\n    <div className=\"min-h-screen bg-gray-950 text-slate-200 p-4 md:p-8 font-sans\">\n      <header className=\"max-w-6xl mx-auto mb-8\">\n        <div className=\"flex items-center gap-3 mb-2\">\n          <div className=\"bg-blue-600 p-2 rounded-lg\">\n            <Zap className=\"text-white w-6 h-6\" />\n          </div>\n          <h1 className=\"text-3xl font-bold tracking-tight text-white\">Scaled Dot-Product Attention</h1>\n        </div>\n        <p className=\"text-slate-400 max-w-2xl text-sm md:text-base\">\n          Interactive visualization of the core mechanism from \"Attention Is All You Need\". \n          See how scaling by √d_k prevents dot products from growing too large and keeps gradients stable.\n        </p>\n      </header>\n\n      <main className=\"max-w-6xl mx-auto grid grid-cols-1 lg:grid-cols-12 gap-6\">\n        \n        {/* Left Column: Configuration & Inputs */}\n        <div className=\"lg:col-span-4 space-y-6\">\n          <section className=\"bg-gray-900/50 border border-white/10 rounded-2xl p-5 backdrop-blur-sm\">\n            <h2 className=\"flex items-center gap-2 text-sm font-semibold text-blue-400 uppercase tracking-wider mb-4\">\n              <Type size={16} /> Input Sequence\n            </h2>\n            <input \n              type=\"text\"\n              value={inputText}\n              onChange={(e) => setInputText(e.target.value)}\n              className=\"w-full bg-gray-800 border border-white/10 rounded-lg px-4 py-2 text-white focus:outline-none focus:ring-2 focus:ring-blue-500 mb-4\"\n              placeholder=\"Type a sentence...\"\n            />\n            \n            <div className=\"space-y-4\">\n              <div>\n                <div className=\"flex justify-between mb-2\">\n                  <label className=\"text-xs text-slate-400\">Key Dimension (d_k)</label>\n                  <span className=\"text-xs font-mono text-blue-400\">{dk}</span>\n                </div>\n                <input \n                  type=\"range\" min=\"1\" max=\"1024\" step=\"1\"\n                  value={dk}\n                  onChange={(e) => setDk(parseInt(e.target.value))}\n                  className=\"w-full h-1.5 bg-gray-800 rounded-lg appearance-none cursor-pointer accent-blue-500\"\n                />\n              </div>\n\n              <div className=\"flex items-center justify-between p-3 bg-gray-800/50 rounded-xl border border-white/5\">\n                <div className=\"flex items-center gap-2\">\n                  <Settings2 size={16} className=\"text-slate-400\" />\n                  <span className=\"text-sm\">Apply Scaling (1/√d_k)</span>\n                </div>\n                <button \n                  onClick={() => setUseScaling(!useScaling)}\n                  className={`w-12 h-6 rounded-full transition-colors relative ${useScaling ? 'bg-blue-600' : 'bg-gray-700'}`}\n                >\n                  <div className={`absolute top-1 w-4 h-4 rounded-full bg-white transition-all ${useScaling ? 'right-1' : 'left-1'}`} />\n                </button>\n              </div>\n            </div>\n          </section>\n\n          <section className=\"bg-gray-900/50 border border-white/10 rounded-2xl p-5\">\n            <div className=\"flex justify-between items-center mb-4\">\n              <h2 className=\"text-sm font-semibold text-purple-400 uppercase tracking-wider\">Vector Matrix (Q, K, V)</h2>\n              <button onClick={() => setVectors(generateVectors(words, vectorDim))} className=\"p-1 hover:bg-white/10 rounded\">\n                <RefreshCcw size={14} />\n              </button>\n            </div>\n            <div className=\"space-y-3 overflow-y-auto max-h-[400px] pr-2 custom-scrollbar\">\n              {words.map((word, wIdx) => (\n                <div key={`${word}-${wIdx}`} className={`p-3 rounded-xl border transition-all ${selectedWordIdx === wIdx ? 'bg-blue-500/10 border-blue-500/50' : 'bg-gray-800/30 border-white/5'}`}>\n                  <div className=\"flex justify-between items-center mb-2\">\n                    <button \n                      onClick={() => setSelectedWordIdx(wIdx)}\n                      className=\"text-sm font-bold text-white truncate hover:text-blue-400\"\n                    >\n                      {word}\n                    </button>\n                    {selectedWordIdx === wIdx && <span className=\"text-[10px] bg-blue-600 px-1.5 rounded text-white font-mono uppercase\">Query</span>}\n                  </div>\n                  <div className=\"grid grid-cols-4 gap-1\">\n                    {['q', 'k', 'v'].map(type => (\n                      <div key={type} className=\"flex flex-col gap-1\">\n                        <span className=\"text-[9px] uppercase text-slate-500 font-bold text-center\">{type}</span>\n                        {vectors[word]?.[type]?.map((val, i) => (\n                          <input \n                            key={i}\n                            type=\"number\"\n                            step=\"0.1\"\n                            value={val}\n                            onChange={(e) => handleVectorChange(word, type, i, e.target.value)}\n                            className=\"bg-black/40 text-[10px] text-center rounded py-0.5 border border-white/5 text-slate-300 w-full\"\n                          />\n                        ))}\n                      </div>\n                    ))}\n                  </div>\n                </div>\n              ))}\n            </div>\n          </section>\n        </div>\n\n        {/* Right Column: Visualization */}\n        <div className=\"lg:col-span-8 space-y-6\">\n          <div className=\"grid grid-cols-1 md:grid-cols-2 gap-6\">\n            {/* Attention Distribution */}\n            <section className=\"bg-gray-900/50 border border-white/10 rounded-2xl p-6 min-h-[350px] flex flex-col\">\n              <div className=\"flex justify-between items-center mb-6\">\n                <h2 className=\"text-sm font-semibold text-emerald-400 uppercase tracking-wider\">Softmax Weights (α)</h2>\n                <div className=\"text-[10px] text-slate-500 bg-white/5 px-2 py-1 rounded-md\">\n                  Σ = 100%\n                </div>\n              </div>\n              <div className=\"flex-1 min-h-[200px]\">\n                <ResponsiveContainer width=\"100%\" height=\"100%\">\n                  <BarChart data={chartData}>\n                    <XAxis dataKey=\"name\" axisLine={false} tickLine={false} tick={{fill: '#94a3b8', fontSize: 12}} />\n                    <Tooltip \n                      cursor={{fill: 'rgba(255,255,255,0.05)'}}\n                      contentStyle={{backgroundColor: '#0f172a', border: '1px solid #1e293b', borderRadius: '8px'}}\n                    />\n                    <Bar dataKey=\"weight\" radius={[4, 4, 0, 0]} name=\"Weight (%)\">\n                      {chartData.map((entry, index) => (\n                        <Cell key={`cell-${index}`} fill={index === selectedWordIdx ? '#3b82f6' : '#10b981'} fillOpacity={0.8} />\n                      ))}\n                    </Bar>\n                  </BarChart>\n                </ResponsiveContainer>\n              </div>\n              <p className=\"mt-4 text-xs text-slate-500 leading-relaxed italic\">\n                {words[selectedWordIdx] ? `How much the query \"${words[selectedWordIdx]}\" focuses on other words.` : \"Enter text to see attention weights.\"}\n              </p>\n            </section>\n\n            {/* Math Breakdown */}\n            <section className=\"bg-gray-900/50 border border-white/10 rounded-2xl p-6 flex flex-col\">\n              <h2 className=\"text-sm font-semibold text-orange-400 uppercase tracking-wider mb-6\">Step-by-Step Logic</h2>\n              <div className=\"space-y-4 flex-1\">\n                <div className=\"p-3 bg-black/30 rounded-lg border border-white/5\">\n                  <div className=\"text-[10px] text-slate-500 uppercase font-bold mb-1\">1. Dot Product (Similarity)</div>\n                  <div className=\"text-sm font-mono text-slate-300\">\n                    q · k_i = <span className=\"text-orange-300\">{attentionStats?.dotProducts?.[selectedWordIdx]?.toFixed(2) || \"0.00\"}</span>\n                  </div>\n                </div>\n\n                <div className=\"p-3 bg-black/30 rounded-lg border border-white/5\">\n                  <div className=\"text-[10px] text-slate-500 uppercase font-bold mb-1\">2. Scaling Factor (√d_k)</div>\n                  <div className=\"text-sm font-mono text-slate-300\">\n                    {useScaling ? (\n                      <>Score ÷ √{dk} = <span className=\"text-orange-300\">{attentionStats?.scaledScores?.[selectedWordIdx]?.toFixed(2) || \"0.00\"}</span></>\n                    ) : (\n                      <span className=\"text-slate-500\">Scaling Disabled (Factor = 1.0)</span>\n                    )}\n                  </div>\n                </div>\n\n                <div className=\"p-3 bg-black/30 rounded-lg border border-white/5\">\n                  <div className=\"text-[10px] text-slate-500 uppercase font-bold mb-1\">3. Softmax Distribution</div>\n                  <div className=\"flex flex-wrap gap-1 mt-2\">\n                    {attentionStats?.weights ? attentionStats.weights.map((w, i) => (\n                      <div key={i} className=\"h-6 flex items-center bg-emerald-500/20 px-2 rounded border border-emerald-500/30 text-[10px] font-mono\">\n                        {words[i]}: {(w * 100).toFixed(0)}%\n                      </div>\n                    )) : <span className=\"text-xs text-slate-600\">Waiting for input...</span>}\n                  </div>\n                </div>\n              </div>\n\n              <div className=\"mt-6 p-4 bg-blue-600/10 border border-blue-500/20 rounded-xl\">\n                <div className=\"flex items-start gap-3\">\n                  <Info size={18} className=\"text-blue-400 mt-0.5\" />\n                  <p className=\"text-[11px] text-slate-400 leading-relaxed\">\n                    <strong>The Scaling Rule:</strong> As d_k grows, dot products increase in magnitude. This pushes Softmax into regions with extremely small gradients. Dividing by √d_k \"sharpens\" or \"smoothes\" the distribution to keep training stable.\n                  </p>\n                </div>\n              </div>\n            </section>\n          </div>\n\n          {/* Attention Output Visualization */}\n          <section className=\"bg-gray-900/50 border border-white/10 rounded-2xl p-6 overflow-hidden\">\n            <h2 className=\"text-sm font-semibold text-white uppercase tracking-wider mb-6\">Final Attention Output (Context Vector)</h2>\n            \n            <div className=\"flex flex-col md:flex-row items-center gap-8\">\n              <div className=\"flex flex-col gap-2 w-full md:w-auto\">\n                <div className=\"text-xs text-slate-500 mb-2\">Linear Weighted Sum: ∑ (α_i × v_i)</div>\n                <div className=\"flex gap-2 justify-center md:justify-start\">\n                  {attentionStats?.outputVector ? attentionStats.outputVector.map((val, i) => (\n                    <motion.div \n                      key={i}\n                      initial={{ scale: 0.8 }}\n                      animate={{ scale: 1 }}\n                      className=\"w-16 h-16 bg-gradient-to-br from-blue-600 to-indigo-700 rounded-xl flex items-center justify-center text-white font-mono font-bold border border-white/10 shadow-lg shadow-blue-500/20\"\n                    >\n                      {val.toFixed(2)}\n                    </motion.div>\n                  )) : Array(4).fill(0).map((_, i) => (\n                    <div key={i} className=\"w-16 h-16 bg-gray-800 rounded-xl border border-white/5\" />\n                  ))}\n                </div>\n              </div>\n\n              <div className=\"hidden md:block\">\n                <ArrowRight className=\"text-slate-700\" size={32} />\n              </div>\n\n              <div className=\"flex-1 bg-black/40 p-4 rounded-xl border border-white/5 w-full\">\n                <div className=\"text-xs text-slate-500 mb-3\">Interpretation</div>\n                <p className=\"text-sm text-slate-300 leading-relaxed\">\n                  The final vector is a <strong>semantic blend</strong> of the input sentence, weighted by its relevance to the word \n                  <span className=\"text-blue-400 font-bold mx-1\">\"{words[selectedWordIdx] || \"selected word\"}\"</span>.\n                  Notice how toggling \"Scaling\" makes the distribution flatter (more uniform) or peakier (sharper focus).\n                </p>\n              </div>\n            </div>\n          </section>\n        </div>\n      </main>\n\n      <footer className=\"max-w-6xl mx-auto mt-12 pt-8 border-t border-white/5 text-center\">\n        <p className=\"text-slate-500 text-xs\">\n          Interactive Transformer Lab — Based on Equation 1: Attention(Q, K, V) = softmax(QKᵀ / √dₖ)V\n        </p>\n      </footer>\n\n      <style dangerouslySetInnerHTML={{ __html: `\n        .custom-scrollbar::-webkit-scrollbar {\n          width: 4px;\n        }\n        .custom-scrollbar::-webkit-scrollbar-track {\n          background: rgba(255, 255, 255, 0.02);\n        }\n        .custom-scrollbar::-webkit-scrollbar-thumb {\n          background: rgba(255, 255, 255, 0.1);\n          border-radius: 10px;\n        }\n      `}} />\n    </div>\n  );\n}\n"
                }
              }
            }
          ]
        },
        {
          "role": "user",
          "parts": [
            {
              "functionResponse": {
                "name": "render_prototype",
                "response": {
                  "success": true,
                  "rendered": true
                }
              }
            }
          ]
        }
      ],
      "created_at": 1770641575118,
      "updated_at": 1770641726285
    }
  }
}