{
  "bdfaa68d8984f0dc02beaca527b76f207d99b666d31d1da728ee0728182df697": {
    "hash": "bdfaa68d8984f0dc02beaca527b76f207d99b666d31d1da728ee0728182df697",
    "filename": "1706.03762v7.pdf",
    "raw_text": "Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani∗\nGoogle Brain\navaswani@google.com\nNoam Shazeer∗\nGoogle Brain\nnoam@google.com\nNiki Parmar∗\nGoogle Research\nnikip@google.com\nJakob Uszkoreit∗\nGoogle Research\nusz@google.com\nLlion Jones∗\nGoogle Research\nllion@google.com\nAidan N. Gomez∗ †\nUniversity of Toronto\naidan@cs.toronto.edu\nŁukasz Kaiser∗\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n†Work performed while at Google Brain.\n‡Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7 [cs.CL] 2 Aug 2023\n\n-- 1 of 15 --\n\n1 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35 , 2 , 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [ 21 ] and conditional\ncomputation [ 32 ], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [ 2, 19 ]. In all but a few cases [ 27 ], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16 ], ByteNet [ 18 ] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [12 ]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5, 2 , 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2\n\n-- 2 of 15 --\n\nFigure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [ 11 ] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3\n\n-- 3 of 15 --\n\nScaled Dot-Product Attention Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv . We compute the dot products of the\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V ) = softmax( QKT\n√dk\n)V (1)\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof 1\t√dk\n. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3 ]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1\t√dk\n.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv -dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\ni=1 qiki, has mean 0 and variance dk .\n4\n\n-- 4 of 15 --\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\nwhere headi = Attention(QW Q\ni , KW K\ni , V W V\ni )\nWhere the projections are parameter matrices W Q\ni ∈ Rdmodel×dk , W K\ni ∈ Rdmodel×dk , W V\ni ∈ Rdmodel×dv\nand W O ∈ Rhdv ×dmodel .\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndf f = 2048.\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30 ]. In the embedding layers, we multiply those weights by √dmodel.\n5\n\n-- 5 of 15 --\n\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2 · d) O(1) O(1)\nRecurrent O(n · d2) O(n) O(n)\nConvolutional O(k · n · d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nP E(pos,2i) = sin(pos/100002i/dmodel )\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\nP Epos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6\n\n-- 6 of 15 --\n\nlength n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38 ] and byte-pair [31 ] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [6 ], however, decrease the complexity\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38 ]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d−0.5\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4 Regularization\nWe employ three types of regularization during training:\n7\n\n-- 7 of 15 --\n\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel BLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\nTransformer (base model) 27.3 38.1 3.3 · 1018\nTransformer (big) 28.4 41.8 2.3 · 1019\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [ 36 ]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty α = 0.6 [ 38 ]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8\n\n-- 8 of 15 --\n\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN dmodel dff h dk dv Pdrop ϵls train PPL BLEU params\nsteps (dev) (dev) ×106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B) 16 5.16 25.1 58\n32 5.01 25.4 60\n(C)\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)\n0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37 ]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9\n\n-- 9 of 15 --\n\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al. (2013) [40] semi-supervised 91.3\nHuang & Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10\n\n-- 10 of 15 --\n\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770–778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735–1780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832–841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n11\n\n-- 11 of 15 --\n\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152–159. ACL, June 2006.\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):1929–1958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434–443. ACL, August 2013.\n12\n\n-- 12 of 15 --\n\nAttention VisualizationsInput-Input Layer5\nIt\t\nis\nin\nthis\nspirit\nthat\na\t\nmajority\t\nof\nAmerican\t\ngovernments\t\nhave\npassed\t\nnew\nlaws\nsince\n2009\nmaking\t\nthe\nregistration\t\nor\nvoting\t\nprocess\t\nmore\ndifficult\t\n.\t\n<EOS>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\nIt\t\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\n13\n\n-- 13 of 15 --\n\nInput-Input Layer5\nThe\nLaw\nwill\nnever\t\nbe\nperfect\t\n,\t\nbut\nits\napplication\t\nshould\t\nbe\njust\n-\t\nthis\nis\t\nwhat\nwe\nare\nmissing\t\n,\t\nin\nmy\nopinion\t\n.\t\n<EOS>\t\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\t\n<pad>Input-Input Layer5\nThe\nLaw\nwill\nnever\t\nbe\nperfect\t\n,\t\nbut\nits\napplication\t\nshould\t\nbe\njust\n-\t\nthis\nis\t\nwhat\nwe\nare\nmissing\t\n,\t\nin\nmy\nopinion\t\n.\t\n<EOS>\t\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\t\n<pad>\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14\n\n-- 14 of 15 --\n\nInput-Input Layer5\nThe\nLaw\nwill\nnever\t\nbe\nperfect\t\n,\t\nbut\nits\napplication\t\nshould\t\nbe\njust\n-\t\nthis\nis\t\nwhat\nwe\nare\nmissing\t\n,\t\nin\nmy\nopinion\t\n.\t\n<EOS>\t\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\t\n<pad>Input-Input Layer5\nThe\nLaw\nwill\nnever\t\nbe\nperfect\t\n,\t\nbut\nits\napplication\t\nshould\t\nbe\njust\n-\t\nthis\nis\t\nwhat\nwe\nare\nmissing\t\n,\t\nin\nmy\nopinion\t\n.\t\n<EOS>\t\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\t\n<pad>\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15\n\n-- 15 of 15 --\n\n",
    "analysis_json": "{\"title\":\"Attention Is All You Need\",\"authors\":[\"Ashish Vaswani\",\"Noam Shazeer\",\"Niki Parmar\",\"Jakob Uszkoreit\",\"Llion Jones\",\"Aidan N. Gomez\",\"Lukasz Kaiser\",\"Illia Polosukhin\"],\"publication_year\":\"2017\",\"summary\":\"This seminal paper introduces the Transformer architecture, a novel sequence transduction model that entirely replaces recurrent and convolutional layers with self-attention mechanisms. By enabling global dependencies to be drawn regardless of distance and facilitating massive parallelization, the Transformer achieved state-of-the-art results in machine translation with significantly reduced training times, laying the foundation for modern large language models.\",\"breakthrough_score\":100,\"breakthrough_reasoning\":\"The Transformer paradigm shift moved AI from sequential processing (RNNs) to parallel attention-based processing. It is the architectural foundation of almost all modern state-of-the-art AI, including GPT-4, BERT, and Claude, making it one of the most impactful papers in the history of computer science.\",\"key_claims\":[\"Self-attention can replace recurrence and convolution for sequence modeling.\",\"The Transformer allows for significantly more parallelization during training compared to RNNs.\",\"The model reduces the path length between long-range dependencies to a constant O(1) operations.\",\"Multi-head attention allows the model to jointly attend to information from different representation subspaces.\"],\"testable_hypotheses\":[{\"hypothesis\":\"Training time for a Transformer is significantly lower than an LSTM-based model for the same BLEU score performance.\",\"how_to_test\":\"Train a standard Transformer and a GNMT-style LSTM model on the same dataset (e.g., WMT 14) using identical hardware and measure total FLOPs and clock time to reach a target BLEU score.\",\"expected_outcome\":\"The Transformer reaches the target score in a fraction of the time and total floating-point operations.\"},{\"hypothesis\":\"Multi-head attention captures distinct linguistic relationships that a single-head attention mechanism would average out.\",\"how_to_test\":\"Visualize individual attention heads' weights for a sentence containing anaphoric references (e.g., 'The law... its application').\",\"expected_outcome\":\"Specific heads will show high activation for coreference resolution while others focus on syntactic structures like verb-object pairs.\"}],\"key_equations\":[{\"name\":\"Scaled Dot-Product Attention\",\"latex\":\"Attention(Q, K, V) = softmax( (Q K^T) / sqrt(d_k) ) V\",\"description\":\"Calculates the attention weights by computing the dot product of queries and keys, scaling by the square root of the key dimension, and applying a softmax before multiplying by the values.\",\"variables\":[{\"name\":\"Q\",\"description\":\"Query matrix\",\"typical_range\":\"Matrix of size n x d_k\"},{\"name\":\"d_k\",\"description\":\"Dimension of the keys\",\"typical_range\":\"64 to 128\"}]},{\"name\":\"Sinusoidal Positional Encoding\",\"latex\":\"PE(pos, 2i) = sin(pos / 10000^{2i/d_{model}})\",\"description\":\"Injects information about the relative or absolute position of tokens into the embedding by using sine and cosine functions of different frequencies.\",\"variables\":[{\"name\":\"pos\",\"description\":\"The position of the token in the sequence\",\"typical_range\":\"0 to sequence_length\"},{\"name\":\"d_model\",\"description\":\"The dimensionality of the model embeddings\",\"typical_range\":\"512 to 1024\"}]}],\"simulation_possibilities\":[{\"title\":\"Interactive Attention Weight Visualizer\",\"description\":\"A real-time interface where users type a sentence and see the 'wires' of attention connect words. Users can toggle between different heads and layers to see how 'it' resolves to a previous noun.\",\"complexity\":\"Medium\",\"variables\":[\"input_text\",\"active_head_index\",\"layer_depth\"],\"expected_insights\":\"Users will understand that attention is not just a single connection but a set of parallel 'filters' looking for different patterns.\",\"visualization_type\":\"interactive\"},{\"title\":\"Positional Encoding Waveform Explorer\",\"description\":\"A visualization of the sinusoidal positional encodings. Sliders for 'pos' and 'i' show how the frequency shifts, creating a unique 'fingerprint' for every position.\",\"complexity\":\"Low\",\"variables\":[\"position\",\"dimension_index\",\"d_model\"],\"expected_insights\":\"Demonstrates how a non-recurrent model can still 'know' where words are based on overlapping wave patterns.\",\"visualization_type\":\"chart\"},{\"title\":\"The Scaling Factor Gradient Demo\",\"description\":\"A chart showing the distribution of softmax values as d_k increases. It compares scaled vs. unscaled dot products to show why gradients vanish without the sqrt(d_k) factor.\",\"complexity\":\"Low\",\"variables\":[\"d_k\",\"vector_magnitude\"],\"expected_insights\":\"Explains the mathematical necessity of the scaling factor for stable training in high dimensions.\",\"visualization_type\":\"chart\"}],\"field\":\"Natural Language Processing\",\"related_fields\":[\"Deep Learning\",\"Computer Vision\",\"Sequential Decision Making\",\"Generative AI\"],\"limitations\":[\"Quadratic complexity O(n^2) with respect to sequence length.\",\"High memory requirements for very long sequences.\",\"Fixed maximum sequence length for trained positional encodings (though sinusoids mitigate this somewhat).\"],\"difficulty_to_understand\":\"Advanced\",\"prerequisites\":[\"Linear Algebra (Matrix Multiplication)\",\"Calculus (Gradients and Softmax)\",\"Basic Neural Network Architectures (Embeddings, Feed-Forward Networks)\",\"Concept of Sequence-to-Sequence models\"]}",
    "created_at": 1770292136786
  },
  "arxiv-2602.04883": {
    "hash": "arxiv-2602.04883",
    "filename": "2602.04883.pdf",
    "raw_text": "Protein Autoregressive Modeling via Multiscale Structure\nGeneration\nYanru Qu1,2,∗, Cheng-Yen Hsieh1,∗,†, Zaixiang Zheng1, Ge Liu2, Quanquan Gu1,‡\n1ByteDance Seed, 2University of Illinois Urbana-Champaign\n∗Equal Contributions, †Project Lead, ‡Corresponding Author\nAbstract\nWe present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for\nprotein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature\nof proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and\nrefining structural details over scales. To achieve this, PAR consists of three key components: (i)\nmulti-scale downsampling operations that represent protein structures across multiple scales during\ntraining; (ii) an autoregressive transformer that encodes multi-scale information and produces\nconditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that\ngenerates backbone atoms conditioned on these embeddings. Moreover, autoregressive models\nsuffer from exposure bias, caused by the training and the generation procedure mismatch, and\nsubstantially degrades structure generation quality. We effectively alleviate this issue by adopting\nnoisy context learning and scheduled sampling, enabling robust backbone generation. Notably,\nPAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional\ngeneration and motif scaffolding without requiring fine-tuning. On the unconditional generation\nbenchmark, PAR effectively learns protein distributions and produces backbones of high design\nquality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a\npromising framework for protein structure generation.\nDate: February 5, 2026\nCorrespondence: Quanquan Gu quanquan.gu@bytedance.com\nProject Page: https://par-protein.github.io\nNote: Work was done during Yanru Qu’s internship at ByteDance Seed\n1 Introduction\nDeep generative modeling of proteins has emerged as a way to design and model novel structures with desired\nfunctions and properties, with broad applications in biomedicine and nanotechnology [ 21, 25 ]. A widely\nadopted approach is to directly model the distribution of three-dimensional protein structures, which govern\nprotein function. Typically, structure generative models produce protein backbones without sequences or side\nchains. Prior work in this area could be broadly categorized into methods that predict the SE(3) backbone\nframe representations [ 45 , 47] and those that directly model atoms, e.g., Cα coordinates for simplicity and\nscalability [ 14, 31]. However, all these works are based on diffusion models and their variations (e.g., flow\nmatching).\n1\narXiv:2602.04883v1 [cs.LG] 4 Feb 2026\n\n-- 1 of 23 --\n\nFigure 1 Overview of PAR. PAR comprises the autoregressive (AR) transformer Tθ and the flow-based backbone\ndecoder vθ . During training, we downsample a backbone x ∈ RL×3 into multi-scale representations {x1, . . . , x}.\nAR transformer performs next-scale prediction, producing conditional embeddings (z1, . . . , zn) from (bos, . . . , xn−1).\nThe shared flow-based decoder learns to denoise backbones xi at each scale conditioned on zi. At inference, PAR\nautoregressively generates xi until the final structure x is constructed.\nOn the other hand, autoregressive (AR) modeling has emerged as a powerful paradigm for large language\nmodels [ 1, 41]. AR models employ next-token prediction to model the probability of each token based on prior\nones, showing striking empirical behaviors such as scalability [ 24] and zero-shot generalization to unseen tasks\n[6].\nDespite its success in other domains, AR modeling has received little attention in backbone modeling. We\nidentify two main reasons. (i) Extending AR models to continuous data, e.g. atomic positions in 3D, often\nrelies on data discretization [12], which can reduce structural fidelity and fine-grained details for proteins,\nlimiting generative performance [ 19]. (ii) Protein residues exhibit strong bidirectional dependencies: residues\ndistant in sequence may be spatially close and form hydrogen bonds or hydrophobic contacts. This mutual\ndependency conflicts with the unidirectional assumption of standard AR models, and thus limits the quality\nof previous attempts on autoregressive structure generation [ 13]. A natural question therefore arises: can we\napply AR modeling to protein backbone design?\nIn this paper, we answer the above question affirmatively, and propose PAR, a Protein AutoRegressive\nframework, to unlock the power of AR models for protein backbone generation. We take initiative from the\nhierarchical nature of proteins: their structures span multiple scales of granularity, from coarse 3D topology\nand tertiary fold arrangements, local secondary structures, to the finest atomic coordinates. PAR thus adopts\na multi-scale autoregressive framework via next-scale prediction, predicting each scale conditioned on prior\ncoarser scales. This strategy, inspired by advances in image generation, enabled AR models to surpass strong\ndiffusion models in image synthesis for the first time [ 40 ], and further allows multimodal LLMs to achieve\nunified text and image generation framework [28].\nBuilding on this multi-scale framework, PAR includes three key components (Fig. 1). The multi-scale\ndownsampling creates coarse-to-fine structural representations to serve as structural context and targets\nduring training. AR transformer, a stack of non-equivariant attention layers [42 ], encodes all preceding\nscales to produce a scale-wise conditional embedding following Li et al. [30] . The flow-based backbone decoder\nis conditioned on this embedding to model Cα backbone atoms directly. As a result, PAR avoids both\ndiscretization of protein structures and residue-wise unidirectional autoregressive ordering, thereby overcoming\nthe two aforementioned limitations that compromise structural fidelity and generative quality. Moreover,\ntraining on ground-truth structural context, AR models suffer from exposure bias [ 3], which is a key challenge\nsubstantially reducing structure generation quality in our preliminary study. We effectively mitigate such\nissue via noisy context learning and scheduled sampling, allowing the model to learn from corrupted context.\nThis multi-scale approach introduces several notable model behaviors. PAR generates backbones by establishing\na global topology and performing refinements, analogous to progressively sculpting a statue into a masterpiece\n(Fig. 2). For unconditional generation, PAR exhibits favorable scaling behavior, yielding competitive results on\n2\n\n-- 2 of 23 --\n\nFigure 2 Samples generated by PAR over scales. We illustrate PAR’s generation process across five scales. Much\nlike sculpting a statue, the model first formulates the global structural layout at coarse scales and progressively refines\nthe details at later scales.\ndistributional metrics like Fréchet Protein Structure Distance (FPSD). Unlike diffusion models, which operate\nat a single scale, PAR flexibly handles inputs at various granularities, and hence shows zero-shot generalization\nin tasks like prompt-based generation and motif scaffolding. The multi-scale formulation enables PAR to\norchestrate sampling strategies, achieving a 2.5x sampling speedup compared to single-scale baselines. Finally,\nPAR provides a more general framework, incorporating flow-based models as a special case when restricted to\na single scale, and thus remains compatible with techniques from flow-based models like self-conditioning [9].\nMain contributions: (i) We present PAR, the first multi-scale AR model for protein backbone generation\nthat addresses key limitations of existing AR methods. (ii) PAR comprises multi-scale downsampling, AR\ntransformer, and a flow-based decoder, to directly model Cα atom, avoiding discretization loss. (iii) We\nalleviate exposure bias through noisy context learning and scheduled sampling, effectively improving structure\ngeneration. (iv) Our model shows an interpretable generation process that forms coarse backbone topology\nand refines it progressively. (v) Benchmarking results show that PAR effectively captures protein data\ndistributions, achieving FPSD score of 161.0 against PDB dataset that further scale with training compute.\n(vi) PAR exhibits efficient sampling and zero-shot generalization potential, reflecting the versatility of AR\nlarge language models.\n2 Background and Related Work\nFlow and diffusion-based structure generative models. Flow-based and diffusion methods [18, 34] operate by\ntransforming samples from a prior distribution to the target data distribution, and have been widely applied\nto protein backbone generation. These methods either predict per-residue rotations and translations using\na frame-based Riemannian manifold representation [ 5, 22, 45, 47, 48 ] or directly model atom coordinates,\nsuch as Cα positions [14, 31 , 32], with some approaches generating fully atomistic proteins including side\nchains [ 10, 37]. Discrete diffusion methods [15, 43], trained on structure tokens, often reduce structural fidelity\nand limit generation quality [ 19]. Unlike most diffusion approaches, which are single-scale, PAR models\nprotein structures across multiple scales using a parameterized upsampling autoregressive process from short\nto long, allowing flexible handling of different structural granularities and zero-shot generalization to tasks\nlike prompt-based generation. In addition, PAR provides a more general framework, as it naturally reduces to\na flow-based model when restricted to a single scale.\n3\n\n-- 3 of 23 --\n\nAutoregressive modeling. Autoregressive (AR) modeling has been driving natural language processing and\ncomputer vision due to its strong scalability and zero-shot generalization [1, 40, 41]. The approach relies on\nnext-token prediction that predicts the distribution of the next token based on prior ones in a unidirectional\nsequence. However, adapting autoregressive models to continuous domains, like image generation, often\ninvolves tokenizers such as VQVAE [12 ], which discretizes the data for transformer training and may discard\nfine-grained details. Recently, Li et al. [30] used the AR model that produces conditioning for a diffusion\nnetwork (e.g., a small MLP) to model image latents, unlocking the operations of AR models in a continuous-\nvalued space. In addition, defining appropriate autoregressive orders that preserve data properties is crucial.\nSince next-token prediction inherently discards spatial locality by flattening the 2D image feature map into a\n1D sequence, VAR [40] introduced next-scale prediction. Leveraging a multi-scale VQVAE, the image feature\nmap is quantized into n multi-scale token maps that preserve the spatial and bidirectional correlations. To\nour knowledge, autoregressive modeling has not been widely applied to protein structure generation despite\ntheir success in other domains. The only exception is Gaujac et al. [13], which models structure tokens with\na causal transformer. In contrast, we design a multi-scale autoregressive framework that operates directly\nin continuous backbone space using a flow-based backbone decoder, thereby addressing the limitations of\ndiscrete token maps while respecting the bidirectional biophysical relations of protein structures.\n3 Protein Autoregressive Modeling\nIn this section, we introduce PAR, a multi-scale autoregressive (AR) framework for protein backbone generation.\nFormally, we want to model a protein backbone Cα structure with L residues x ∈ RL×3 in an autoregressive\nmanner as follows: pθ (x) = EX∼qdecompose(·|x)\n\u0002pθ (X = {x1, . . . , xn})\u0003\n= EX∼qdecompose(·|x)\nn\tY\ni=1\npθ (xi | X<i). (1)\nwhere qdecompose(·|x) defines a decomposition of autoregressive order for protein structure x into n scales\nX = {x1, . . . , xn} with xn = x, while pθ (xi | X<i) is the desired PAR model learning to generate x via a\nscale-wise autoregression.\nThe design space of qdecompose and pθ under this formulation (Eqn. 1) can be flexible. Recall that our goal is\nto enable AR modeling to preserve spatial dependencies and avoid discretization, as discussed in §1. To this\nend, in §3.1, we devise a non-parametric and deterministic qdecompose by multi-scale protein downsampling\n(Fig. 1) that represents protein backbones at multiple scales via hierarchical down-sampling (Eqn. 2), providing\nstructural context and training targets. In §3.2, we parameterize PAR pθ as a backbone autoregressive\nupsampling process via next-scale prediction and achieve direct Cα modeling in the continuous space (Eqn. 3).\nThis comprises two key components: (i) an autoregressive transformer (Fig. 1) that produces scale-wise\nconditional embeddings informed by preceding scales to guide generation (Eqn. 4); and (ii) a flow-based\nbackbone decoder (Fig. 1) which samples Cα backbone coordinates conditioned on the learned embeddings\n(Eqn. 5).\nFinally, in §3.3, we dedicated strategies to mitigate exposure bias [ 3], a mismatch between training on ground-\ntruth data and inference on model predictions that leads to error accumulations and degrading generation\nquality in AR models. Together, these components enable PAR to robustly generate protein backbones in a\ncoarse-to-fine manner.\n3.1 Multi-scale Protein Downsampling\nWe construct the multi-scale representations of protein structures via hierarchical downsampling to serve as\ntraining context and targets for PAR (Fig. 1). Given a protein structure x ∈ RL×3, it produces a hierarchy of\ncoarse-to-fine scales by progressively downsampling x into n scales:\nqdecompose : x 7 → X = {x1, x2, . . . , xn}\n= {Down(x, size(1)), Down(x, size(2)), . . . , x}. (2)\n4\n\n-- 4 of 23 --\n\nwhere Down(x, size(i)) ∈ Rsize(i)×3 denotes a downsampling operation that interpolates x along the sequence\ndimension, leading to size(i) 3D centroids that provide a coarse structural layout. Since qdecompose is designed\nas a deterministic mapping for every x, the likelihood of Eqn. 1 can be simplified without marginalization:\npθ (x) = Qn\ni=1 pθ (xi | X<i). We show that this downsampling strategy properly preserves pairwise spatial\nrelationships in §C.8.\nScale configurations. S = {size(1), . . . , size(n)} could be defined in two ways. When defined by length,\nscales are chosen as hyperparameters, e.g., S = {64, 128, 256}. In this case, if L lies in (size(i), size(i + 1)],\nthe protein could be generated with only i+1 autoregressive steps. When defined by ratio, scales are adaptively\ndetermined based on protein length, e.g., S = {L/4, L/2, L}. Empirically, defining scales by length yields\nslightly better results in modeling data distributions. We adopt this as the default configuration. This\ndesign enables training PAR with flexible scale configurations. In the following sections, we describe how this\nhierarchy of representations are modeled using the autoregressive transformer and backbone decoder.\n3.2 Coarse-to-Fine Backbone Autoregressive Modeling\nPreserving the inherent dependencies in data when defining the autoregressive order is crucial and affects\ngeneration performance [ 40]. Standard AR models assume unidirectional dependency, which conflicts with\nthe strong bidirectional interactions in protein sequences, e.g., spatially close residues can form hydrophobic\ncontacts or hydrogen bonds even if distant in sequence. PAR addresses this with a multi-scale AR framework\nvia next-scale prediction, capturing mutual structural dependency over each scale. Motivated by Li et al. [30] ,\nwe propose to use an AR Transformer with diffusion/flow-based regression loss to enable modeling of Cα\natoms directly in continuous space. That is, we could rewrite the likelihood as:\npθ (X = {x1, . . . , xn}) =\nn\tY\ni=1\npθ (xi|X<i)\n=\nn\tY\ni=1\npθ (xi | zi = Tθ (X<i)),\n(3)\nwhere Tθ is an AR Transformer that produces scale-wise conditioning zi while pθ (xi|zi) is optimized with a\nflow-based atomic decoder vθ with flow matching. This avoids discretizing protein structures into tokens,\npreserving structural details and generation fidelity. We describe each component below.\nAutoregressive transformer for scale-wise conditioning. To formulate the autoregressive order, we leverage\nthe hierarchical nature of proteins, where a protein structure could span various levels of representations from\ncoarse tertiary topology to the finest atomic coordinates. We adopt the next-scale prediction to model per-scale\ndistribution based on prior coarser scales, which further ensures that the bidirectional dependencies of residues\nare modeled over each scale. We train our autoregressive model (Fig. 1), a non-equivariant transformer Tθ , to\nproduce scale-wise conditioning embedding zi for scale i depending on prior scales X<i = {x1, . . . , xi−1}:\nzi = Tθ (X<i) = Tθ\n\u0010\u0002bos, Up(x1, size(2)), . . . , Up(xi−1, size(i))\u0003\u0011\n. (4)\nwhere bos ∈ Rsize(1)×3 is a learnable embedding, and Up(xi−1, size(i)) interpolates xi−1 to size(i) 3D\npoints. All inputs are concatenated along the sequence dimension before being fed into Tθ . The embedding zi\nis then used to condition the flow matching decoder to predict the backbone coordinates xi, detailed below.\nFlow-based atomic decoder. We enable PAR to directly model Cα positions x, wherein pθ (x|zi) is param-\neterized by an atomic decoder vθ with flow matching [FM, 34], which maps standard normal distribution\nto the target data distribution. We condition the vθ with scale-wise conditioning zi predicted by the AR\nTransformer Tθ at each scale i (Fig. 1). During training, we sample the noise ϵi ∼ N (0, I) and a time variable\nti ∈ [0, 1], and compute the interpolated sample as xi\nti = ti · xi + (1 − ti) · ϵi. As such, we can jointly train vθ\nand Tθ with an FM objective:\nL(θ) = Ex∼pD\n\u0014 1\nn\nn\tX\ni=1\n1\nsize(i) Eti∼p(ti),ϵi∼N (0,I) vθ (xi\nti , ti, zi) − (xi − ϵi) 2\n\u0015\n. (5)\n5\n\n-- 5 of 23 --\n\nwhere pD (x) denotes the training data distribution and p(t) denotes the t-sampling distribution in Geffner\net al. [14] . The conditioning embedding zi is injected into the atomic decoder network vθ through adaptive\nlayer norms [36]. We further concatenate a learnable scale embedding alongside zi to help the model identify\ndifferent scales and incorporate self-conditioning input as an additional condition [ 9], though we omit them in\nthe equation for simplicity. To formulate the indices for positional encoding pi at scale i, we uniformly sample\nsize(i) numbers from the interval [1, L], i.e., pi = linspace(1, L, size(i)). At coarse scales, the wide spacing\nbetween adjacent indices encourages the model to capture global structural layout, while at finer scales the\ndense indices allow the model to focus on local details. For more details, please refer to §A.1.\nLeveraging the learned flow network vθ , sampling could be performed at each scale through ordinary differential\nequation (ODE) dxt = vθ (xt, t) dt, with the scale superscript i and condition z omitted for simplicity. Moreover,\nwe could define the stochastic differential equation (SDE) for sampling:\ndxt = vθ (xt, t) dt + g(t) sθ (xt, t) dt + p2g(t)γ dWt, (6)\nwhere g(t) is a time-dependent scaling function for the score function sθ (xt, t) [2, 35] and the noise term, γ is\na noise scaling parameter, and Wt is a standard Wiener process. The score function, defined as the gradient\nof the log-probability of the noisy data distribution at time t, could be computed as sθ (xt, t) = t vθ (xt,t)−xt\n1−t .\nMulti-scale structure generation. At inference, the autoregressive transformer first produces z1 at the coarsest\nscale, which conditions the flow matching decoder to generate x1 either via ODE or SDE sampling in Eqn. 6.\nWe upsample x1 using Up(x1, size(2)) and send it back into the autoregressive transformer to predict the\nnext scale embedding z2. This coarse-to-fine process iterates n times until the flow-matching model generates\nthe full-resolution backbone x. KV cache is applied throughout the autoregressive process for efficiency.\n3.3 Mitigating Exposure Bias\nTraining AR models typically uses teacher forcing [46], where ground-truth data are fed as context to stabilize\nlearning. However, during inference the model is conditioned on its own predictions, creating a training-\ninference mismatch known as exposure bias [ 3, 16]. Errors can then accumulate across autoregressive steps,\ndegrading output quality. Our preliminary study shows that teacher forcing greatly reduces the designability\nof generated structures. To mitigate this, we adapt Noisy Context Learning (NCL) and Scheduled Sampling\n(SS), techniques from language and image AR modeling [4, 38], for PAR.\nNoisy context learning. We train PAR with noisy context, adding noise to the ground-truth prior-scale input\nduring training. This encourages the model to learn the per-scale distribution without relying on perfectly\naccurate context, improving robustness. We randomly sample n noise weights {w1\nncl, · · · , wn\nncl} ∈ [0, 1], and\ndraw n noise samples {ϵ1\nncl, · · · , ϵn\nncl} ∈ N (0, I). Each input context xi is corrupted as xi\nncl = wi\nncl · xi +\n(1 − wi\nncl) · ϵi\nncl. This perturbation is applied to the input context only during training, which updates the\nautoregressive step in Eqn. 4 as zi = Tθ\n\u0010\u0002bos, Up(x1\nncl, size(2)), . . . , Up(xi−1\nncl , size(i))\u0003\u0011\n.\nScheduled sampling. During training, we use scheduled sampling [ 4] by running the forward process\niteratively across scales. At the i-th scale, the flow-based backbone decoder predicts the clean data xi\npred =\nxi\nt + (1 − ti)vθ (xi\nt, ti, zi). With a probability of 0.5, we replace the ground truth context xi with this prediction\nxi\npred at later scales. This exposes the model to its own output and reduces the train-test gap. Notably, we\ncould combine noisy context learning with this technique by adding noises to the model predicted context\nxi\npred.\n4 Experiments\nWe begin by evaluating PAR on unconditional backbone generation and compare it with existing structure\ngenerative methods in §4.1. Next, we examine its zero-shot generalization ability in §4.2. We then study the\nscaling behavior, efficient sampling, and propose strategies to mitigate exposure bias, along with additional\nablations in §4.3. We include additional empirical analysis in §C.\n6\n\n-- 6 of 23 --\n\nTable 1 Unconditional backbone generation performance. We follow Geffner et al. [14] in adopting FPSD and fS\nto evaluate the model’s ability to capture the data distribution. PARpdb denotes the 400M model finetuned on the\nPDB subset.\nMethod \tDesignability FPSD vs. fS Diversity Sec. Struct. %\n(%)↑ sc-RMSD↓ PDB↓ AFDB↓ (C / A / T)↑ TM-Sc.↓ (α/β)\nFrameDiff (17M) 65.4 - 194.2 258.1 2.46/5.78/23.35 0.40 64.9/11.2\nRFDiffusion (60M) 94.4 - 253.7 252.4 2.25/5.06/19.83 0.42 64.3/17.2\nESM3 (1.4B) 22.0 - 933.9 855.4 3.19/6.71/17.73 0.42 64.5/8.5\nGenie2 (16M) 95.2 - 350.0 313.8 1.55/3.66/11.65 0.38 72.7/4.8\nProteina (200M) 92.8 1.14 282.3 285.6 2.17/6.22/21.48 0.37 66.3/9.2\nProteina (400M) 92.6 1.09 271.3 272.6 2.13/6.14/21.18 0.37 65.1/9.5\nPAR (200M) 87.0 1.33 252.0 237.9 2.11/6.41/19.22 0.37 64.3/8.8\nPAR (400M) 96.0 1.01 313.9 296.4 2.24/6.60/16.71 0.39 66.3/8.9\nγ=0.45 88.0 1.28 231.5 211.8 2.20/6.59/20.96 0.36 63.2/9.7\nPARpdb \t96.6 1.04 161.0 228.4 2.57/7.42/23.61 0.43 50.2/16.7\nγ=0.45 88.8 1.34 176.6 256.4 2.62/7.52/30.99 0.40 50.2/16.2\n4.1 Protein Backbone Generation\nGeneration over scales. We illustrate PAR’s backbone generation using a 5-scale model in Fig. 2 (i.e.,\nS = {L/16, L/8, L/4, L/2, L}), showing generated structures with target lengths of {50, 100, 200, 250} residues.\nGeneration proceeds in a coarse-to-fine manner, which resonates with statue sculpting: the coarser scales\nestablish a rough global layout, and finer scales progressively add local details. This multi-scale formulation\nyields a clear and interpretable generation process. We present the quantitative analysis on PAR’s backbone\ngeneration in the next paragraph.\nUnconditional generation benchmark. We compare 3-scale PAR’s (S = {64, 128, 256}) backbone generation\nperformance with other baselines in Tab. 1, following the evaluation protocol in Geffner et al. [14]. The\nbaselines span three categories: frame-based diffusion methods [ 45 , 48], multimodal protein language models\n[ 15], and diffusion/flow-based Cα generators [ 14, 32 ]. We disable the optional pair representations and triangle-\nbased modules [23] in both PAR and Proteina to align architectural capacity and improve computational\nefficiency. We train PAR using a two-stage procedure following Geffner et al. [14]: the model is first trained\nfor 200K steps on the AFDB representative dataset and subsequently fine-tuned for 5K steps on a PDB subset\nof 21K designable samples. Results for the remaining baselines are taken directly from Geffner et al. [14] .\nEvaluation metrics and baseline categories are detailed in §A.2§A.3. To better reflect the goal of unconditional\nprotein generation as modeling the full data distribution, we adopt FPSD, which jointly measures quality\nand diversity by comparing generated and reference distributions, analogous to FID in image generation\n[ 17]. As shown in Tab. 1, PAR generates samples that closely match the reference data distribution and\nmaintains competitive designability. On FPSD, PAR achieves scores of 211.8 against AFDB and 231.5 against\nPDB. By reducing the noise scaling parameter γ (Equation 6) from 0.45 to 0.3 in SDE sampling, we can\nreduce sampling stochasticity and further improve sample quality, improving the designability from 88.0% to\n96.00%. After fine-tuning, PAR achieved 96.6% designability and 161.0 FPSD against the PDB, highlighting\nits superior distributional fidelity compared to pure diffusion-based baselines. We provide additional analysis\non longer proteins in §C.3.\n4.2 Zero-Shot Task Generalization\nGuiding backbone generation with human-specified prompt. Proteins possess hierarchical and complex\nstructures, which makes it challenging to directly specify a target shape and design proteins accordingly. By\nleveraging PAR’s coarse-to-fine generation, a simple prompt (e.g., 16 points) can specify a protein’s coarse\nlayout, from which the model generates the complete structure as shown in Fig. 3. In particular, we first\nobtain a 16-point input prompt either by downsampling a real protein structure from the test set, or by\nspecifying the points manually (the top row in Fig. 3). Using a 5-scale PAR (S = {16, 32, 64, 128, 256}), we\ninitialize the first-scale prediction with the 16-point prompt and autoregressively upsample until the full\n7\n\n-- 7 of 23 --\n\nPrompt\n(16 points)\nGenerated\nstructure\nmainly alpha mainly beta mainly alpha \tbeta barrel \tmixed \tshape A\tshape P \tshape R\nFigure 3 Backbone generation with human prompt. Given a small number of points (e.g., 16) as prompt, PAR can\ngenerate protein backbones that adhere to the global arrangements specified by these points, without any finetuning.\nFor visualization, input points are interpolated to match the length of the generated structure.\nFigure 4 Zero-shot motif scaffolding. Given a motif structure, PAR can generate diverse, plausible scaffold\nstructures that accurately preserve the motif via teacher-forcing the motif coordinates at each scale, without additional\nconditioning or fine-tuning.\nprotein structure is generated, as illustrated in the bottom row of Fig. 3. Following this process, PAR can\ngenerate a new structure that preserves the coarse structural layout (first five examples), and explore entirely\nnovel structures (last three examples). If desired, longer prompts (e.g., 32 points) could be specified to achieve\nmore finer-grained control over backbone generation. As later shown in Tab. 5, we quantitatively evaluate the\nstructural consistency (TM-score) between the prompted layout and the final generation.\nMotif scaffolding. Besides the point-based layout, PAR can preserve finer-grained prompts like atomic\ncoordinates. Fig. 4 highlights the zero-shot motif scaffolding capabilities of PAR. Using a 5-scale PAR, we\ndownsample a raw protein structure into five scales and teacher-force the ground-truth motif coordinates at\neach scale before propagating into the next scale. To avoid clashes or discontinuities, we superimpose the\nground-truth motif residues and the generated motif segments before replacement. With no fine-tuning and\nno conditioning, PAR generates plausible scaffolds that preserve motif structures with high fidelity. This\nstands in contrast to diffusion or flow-based frameworks, which typically require fine-tuning on additional\nconditions such as masks or motif coordinates, or rely on decomposition strategies [14, 44, 45]. Moreover, the\ngenerated scaffolds differ substantially from the input structure, showing that PAR generates structurally\ndiverse scaffolds rather than merely copying. For example, the leftmost example in Fig. 4 preserves the yellow\nmotif helix while introducing new secondary structure elements like β-sheet and loops, in contrast to the\noriginal helices. We further benchmark zero-shot motif scaffolding in Tab. 10 in the appendix, following\nevaluation protocols in [14, 32].\n8\n\n-- 8 of 23 --\n\nFigure 5 Scaling effects of PAR. Performance of four metrics over varying training steps and model sizes, (a) FPSD\nvs. PDB, (b) FPSD vs. AFDB, (c) fS(T), (d) sc-RMSD.\nTable 2 Sampling efficiency. Combining SDE and ODE sampling across scales yields a 2.5× inference speedup\ncompared to the single-scale 400-step baseline, shown in the first and the last row. We generate 100 samples at each\nlength.\nSampling Steps Length 150 Length 200\nTime (s) Design. (%) Time (s) Design. (%)\nProteina (SDE) 0/0/400 131 97% 170 92%\n0/0/200 67 89% 86 80%\nAll SDE 400/400/400 312 97% 351 94%\n400/400/2 184 0% - -\nAll ODE 400/400/400 312 28% - -\nS/S/O 400/400/400 312 98% - -\n400/400/2 184 99% 186 91%\nS/O/O 400/400/400 312 96% - -\n400/2/2 67 97% 68 94%\n4.3 Empirical Analysis of Multiscale PAR\nScaling effects of PAR. We examine the model’s behaviors by varying the backbone decoder’s size and\nnumber of training steps in Fig. 5. We train PAR with 3 scales over three different model sizes with 60,200,400\nmillion parameters and three training durations over 200, 400, 600K steps. PAR demonstrates favorable\nbehavior when scaling both model size and training duration, effectively improving its ability to capture the\nprotein data distribution with FPSD scores of 187 against PDB and 170 against AFDB (first two columns in\nFig. 5). Further, the fS scores, which reflect quality and diversity, increase with larger model sizes and greater\ncomputational budgets. While extending training duration alone offers negligible gains, increasing model size\nsubstantially enhances designability, leading to lower sc-RMSD values. Meanwhile, we empirically observe\nthat scaling the autoregressive transformer has minimal impacts on the evaluation results. This allows us to\nreduce computational costs and prioritize increasing the backbone decoder’s model capacity that effectively\nimproves generation quality. We provide more discussion on varying model sizes in §C.7.\nEfficient sampling with multi-scale orchestration of SDE/ODE. While Tab. 1 reports results using a uniform\nnumber of sampling steps across scales, the multi-scale formulation of PAR actually offers advantages in\nsampling efficiency, as shown in Tab. 2. More specifically, (1) sampling at the coarser scale (e.g., first scale) is\nmore efficient than sampling at finer scales (e.g., 2nd scale) due to shorter sequence length; (2) we can use less\nnumber of sampling steps at finer scales than coarser scales. As shown in Tab. 2, by using SDE sampling only\nat the first scale, and switching to ODE sampling for the remaining scales, PAR could dramatically reduce\nthe diffusion steps from 400 to 2 steps at the last two scales without harming designability (97%), yielding\na 4.7x inference speedup. This is possible because a high-quality coarse topology places the model near\nhigh-density regions, enabling efficient refinement with ODE sampling. Naively reducing the SDE sampling\n9\n\n-- 9 of 23 --\n\nsteps significantly harms designability, dropping to 22% when reducing steps to 50, as shown in Fig. 7. This is\nconsistent with the observation of single-scale models like Proteina, where designability degrades to 89% when\nreducing SDE sampling steps to 200 in Tab. 2. Crucially, SDE sampling at the first scale is necessary for\nestablishing a reliable global topology, given that ODE-only sampling exhibits poor designability. Compared\nto the single-scale 400-step baseline, PAR achieves 1.96x and 2.5x sampling speedup at length 150 and 200,\nrespectively. This improvement is driven by speeding up the final scales, where the longer sequence lengths\ncause computational costs to grow quadratically in transformer architectures. Moreover, the computational\ncosts remain constant at the first scale because it has a fixed size 64, even when generating longer sequences.\nTable 3 Mitigating exposure bias for PAR. We adopted various training strategies to mitigate the exposure bias for\nmulti-scale autoregressive modeling. These techniques are consistently effective effective in improving structure quality.\nNCL: Noisy Context Learning. SS: Schedule Sampling. Results are obtained with 60M PAR trained for 100K steps.\nMethod sc-RMSD ↓ FPSD vs. (PDB/AFDB) ↓ fS-(C/A/T) ↑\nTeacher Forcing 2.20 99.66 / 37.64 2.53 / 5.56 / 29.67\n+ NCL 1.58 89.70 / 23.69 2.54 / 5.85 / 28.37\n+ NCL & SS 1.48 90.66 / 24.59 2.54 / 5.84 / 28.77\nMitigating exposure bias. To mitigate exposure bias, we adopted noisy context learning (NCL) and scheduled\nsampling (SS) as defined in §3.3. Noisy context learning encourages the model to infer structural guidance\nfrom corrupted context and boosts the structure generation quality. Tab. 3 shows that noisy context learning\neffectively improves the sc-RMSD of the generated structure from 2.20 to 1.58, and reduces FPSD against\nAFDB to 23.69 when using ODE sampling. The designability further improved to 1.48 along with scheduled\nsampling, which makes the training process more aligned with the inference scenario. Results are obtained\nwith 60M PAR trained for 100K steps.\n1 \t2 \t3 \t4 \t5\n1\n2\n3\n4\n5\nLayer 0\n1 \t2 \t3 \t4 \t5\n1\n2\n3\n4\n5\nLayer 5\n1 \t2 \t3 \t4 \t5\n1\n2\n3\n4\n5\nLayer 11\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 6 Visualization of the average attention scores in PAR autoregressive transformer over 5 scales.\nObtained from samples with lengths in (128, 256]. We provide attention map visualization for shorter proteins in §C.9\nInterpreting multi-scale PAR. We visualize the attention maps of the autoregressive transformer at each scale\n(Fig. 6). We average the attention scores within each scale, normalize them such that the scores across\nscales sum to 1, and average them over 50 test samples to obtain the scale-level attention distribution during\ninference. We summarize three key observations: (i) Most scales barely attend to the first scale, since the\ninput to this scale, a bos token, carries little structural signal. (ii) Each scale primarily attends to the previous\nscale, which typically contains richer contextual and structural information. (iii) Despite focusing most heavily\non the current scale, the model still retains non-negligible attention to earlier scales. This indicates that PAR\neffectively integrates information across multiple scales and maintains structural consistency during generation.\nThis aligns with results in Tab. 5 where the autoregressive Transformer effectively improves consistency with\nthe given prompt. We also observe similar patterns on shorter proteins, as shown by the attention maps in\nFig. 10 in the appendix.\nMulti-scale formulation. We ablate the effect of defining scale by length versus ratio, as shown in §3.1. Tab. 4\nshows that under comparable levels of upsampling ratio ({64, 128, 256} and {L/4, L/2, L}), the by-length\nstrategy outperforms by-ratio. Meanwhile, PAR obtains better designability and FPSD when increasing from\n10\n\n-- 10 of 23 --\n\nTable 4 Multi-scale formulation. We ablate different strategies for scale configuration in downsampling. Results are\nobtained with 60M PAR.\nDesignability FPSD vs. fS\nDefine scale (%)↑ (sc-RMSD)↓ PDB↓ AFDB↓ (C / A / T)↑\n{64, 256} 83.0 1.38 282.85 274.32 2.14/6.58/20.66\n{64, 128, 256} 85.0 1.39 279.63 267.35 2.15/6.52/20.35\n{64, 128, 192, 256} 77.8 1.55 296.70 282.69 2.05/6.04/18.69\n{64, 96, 128, 192, 256} 81.0 1.51 276.00 263.58 2.17/6.31/20.65\n{L/4, L/2, L} 86.4 1.49 310.64 298.30 2.00/5.87/18.91\ntwo scales to three scales. Beyond this point, increasing the scale configurations to four and five scales results\nin degraded designability, potentially due to error accumulation and exposure bias. These results support our\nchoice of adopting the 3-scale PAR as the default. All results are obtained using the 60M model.\nTable 5 Structural consistency for prompted generation. Using a transformer to encode prior-scale structural\nconditions shows better prompt-following than direct input. Results are obtained with 60M PAR.\nRMSD vs. Reference ↓ TM-score vs. Prompt ↑\nLength (32.64] (64,128] (128,256] (32.64] (64,128] (128,256]\nReference - - - 0.60 0.61 0.59\nDirect Input 2.13 3.38 6.51 0.58 0.61 0.59\nTrans. Encode 1.45 2.72 5.75 0.60 0.64 0.61\nAR Transformer improves structural consistency. We conduct an ablation study to evaluate the effectiveness\nof the autoregressive transformer in Tab. 5. We compare two different strategies for encoding prior-scale\nstructural context, including (i) direct input, where the multi-scale structures are directly fed into the\nbackbone decoder without any intermediate encoding; and (ii) transformer encoder, where all scales are\nprocessed autoregressively by a Transformer encoder, and the resulting encoded representation is then passed\nto the backbone decoder. We train two 60M models and evaluate both models by downsampling 588 testing\nstructures as prompts and re-upsamples them with PAR. As shown in Tab. 5, the transformer encoder\ndemonstrates better structural consistency, indicating that autoregressive encoding across scales produces\ncoherent structural guidance over scales, consistent with attention maps in Fig. 6.\n5 Discussion\nPAR is the first multi-scale autoregressive model for protein backbone generation, offering a general framework\nthat includes flow-based methods as a special case. PAR addressed limitations of standard autoregressive\nmodels, such as unidirectional dependency, discretization, and exposure bias. Our method robustly models\nstructures over multiple granularities and in turn enables strong zero-shot generalization. This capability\nincludes coarse-prompted conditional generation using points (e.g., 16 points) as structural layout and finer-\ngrained controls such as atomic-coordinate-based motif scaffolding. For unconditional backbone generation,\nPAR exhibits powerful distributional fidelity and generation quality. The analysis of scale-level attention map\nprovides additional insights into how the multi-scale formulation operates.\nWe hope that PAR unlocks the potential of autoregressive modeling for protein design. Some promising\nopen directions include: (1) Conformational dynamics modeling. PAR can, in principle, perform zero-shot\nmodeling of conformational distributions: we downsample a structure and upsample it with PAR to mimic\nlocal molecular dynamics. We leave this exciting application for future research. (2) All-atom modeling. This\nwork focuses on backbone Cα atoms to prioritize autoregressive design, but it’s natural to extend to full-atom\nrepresentations [37]. The multi-scale framework offers an advantage for flexible zero-shot prompt-based\nall-atom designs.\n11\n\n-- 11 of 23 --\n\nAcknowledgments\nWe thank Dr. Hang Li, Liang Hong, Xinyou Wang, Jiasheng Ye, Yi Zhou, Jing Yuan, Yilai Li, Zhenghua\nWang, Yuning Shen, Huizhuo Yuan, as well as other colleagues at ByteDance Seed for their valuable comments\nand support.\n12\n\n-- 12 of 23 --\n\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\n[2] Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework\nfor flows and diffusions. arXiv preprint arXiv:2303.08797, 2023.\n[3] Kushal Arora, Layla El Asri, Hareesh Bahuleyan, and Jackie Chi Kit Cheung. Why exposure bias matters: An\nimitation learning perspective of error accumulation in language generation. arXiv preprint arXiv:2204.01171,\n2022.\n[4] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with\nrecurrent neural networks. Advances in neural information processing systems, 28, 2015.\n[5] Avishek Joey Bose, Tara Akhound-Sadegh, Guillaume Huguet, Kilian Fatras, Jarrid Rector-Brooks, Cheng-Hao\nLiu, Andrei Cristian Nica, Maksym Korablyov, Michael Bronstein, and Alexander Tong. Se (3)-stochastic flow\nmatching for protein backbone generation. arXiv preprint arXiv:2310.02391, 2023.\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877–1901, 2020.\n[7] Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola. Generative flows on discrete\nstate-spaces: Enabling multimodal flows with applications to protein co-design. arXiv preprint arXiv:2402.04997,\n2024.\n[8] Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, and Ping Luo. Pixelflow: Pixel-space generative models\nwith flow. arXiv preprint arXiv:2504.07963, 2025.\n[9] Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusion models\nwith self-conditioning. arXiv preprint arXiv:2208.04202, 2022.\n[10] Alexander E Chu, Jinho Kim, Lucy Cheng, Gina El Nesr, Minkai Xu, Richard W Shuai, and Po-Ssu Huang. An\nall-atom protein generative model. Proceedings of the National Academy of Sciences, 121(27):e2311500121, 2024.\n[11] Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert J Ragotte, Lukas F Milles, Basile IM\nWicky, Alexis Courbet, Rob J de Haas, Neville Bethel, et al. Robust deep learning–based protein sequence design\nusing proteinmpnn. Science, 378(6615):49–56, 2022.\n[12] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 12873–12883, 2021.\n[13] Benoit Gaujac, Jérémie Donà, Liviu Copoiu, Timothy Atkinson, Thomas Pierrot, and Thomas D Barrett. Learning\nthe language of protein structure. arXiv preprint arXiv:2405.15840, 2024.\n[14] Tomas Geffner, Kieran Didi, Zuobai Zhang, Danny Reidenbach, Zhonglin Cao, Jason Yim, Mario Geiger, Christian\nDallago, Emine Kucukbenli, Arash Vahdat, et al. Proteina: Scaling flow-based protein structure generative models.\narXiv preprint arXiv:2503.00710, 2025.\n[15] Thomas Hayes, Roshan Rao, Halil Akin, Nicholas J Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Q\nTran, Jonathan Deaton, Marius Wiggert, et al. Simulating 500 million years of evolution with a language model.\nScience, 387(6736):850–858, 2025.\n[16] Tianxing He, Jingzhao Zhang, Zhiming Zhou, and James Glass. Exposure bias versus self-recovery: Are distortions\nreally incremental for autoregressive text generation? arXiv preprint arXiv:1905.10617, 2019.\n[17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained\nby a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing\nsystems, 30, 2017.\n[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural\ninformation processing systems, 33:6840–6851, 2020.\n13\n\n-- 13 of 23 --\n\n[19] Cheng-Yen Hsieh, Xinyou Wang, Daiheng Zhang, Dongyu Xue, Fei Ye, Shujian Huang, Zaixiang Zheng, and Quan-\nquan Gu. Elucidating the design space of multimodal protein language models. arXiv preprint arXiv:2504.11454,\n2025.\n[20] Chin-Wei Huang, Milad Aghajohari, Joey Bose, Prakash Panangaden, and Aaron C Courville. Riemannian\ndiffusion models. Advances in Neural Information Processing Systems, 35:2750–2761, 2022.\n[21] Po-Ssu Huang, Scott E Boyken, and David Baker. The coming of age of de novo protein design. Nature, 537\n(7620):320–327, 2016.\n[22] John B Ingraham, Max Baranov, Zak Costello, Karl W Barber, Wujie Wang, Ahmed Ismail, Vincent Frappier,\nDana M Lord, Christopher Ng-Thow-Hing, Erik R Van Vlack, et al. Illuminating protein space with a programmable\ngenerative model. Nature, 623(7989):1070–1078, 2023.\n[23] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn\nTunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure prediction\nwith alphafold. nature, 596(7873):583–589, 2021.\n[24] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361,\n2020.\n[25] Brian Kuhlman and Philip Bradley. Advances in protein structure prediction and design. Nature reviews molecular\ncell biology, 20(11):681–697, 2019.\n[26] Patrick Kunzmann and Kay Hamacher. Biotite: a unifying open source computational biology framework in\npython. BMC bioinformatics, 19(1):346, 2018.\n[27] Gilles Labesse, N Colloc’h, Joël Pothier, and J-P Mornon. P-sea: a new efficient assignment of secondary structure\nfrom cα trace of proteins. Bioinformatics, 13(3):291–295, 1997.\n[28] Han Li, Xinyu Peng, Yaoming Wang, Zelin Peng, Xin Chen, Rongxiang Weng, Jingang Wang, Xunliang Cai,\nWenrui Dai, and Hongkai Xiong. Onecat: Decoder-only auto-regressive model for unified understanding and\ngeneration. arXiv preprint arXiv:2509.03498, 2025.\n[29] Tianhong Li and Kaiming He. Back to basics: Let denoising generative models denoise. arXiv preprint\narXiv:2511.13720, 2025.\n[30] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without\nvector quantization. Advances in Neural Information Processing Systems, 37:56424–56445, 2024.\n[31] Yeqing Lin and Mohammed AlQuraishi. Generating novel, designable, and diverse protein structures by equivari-\nantly diffusing oriented residue clouds. arXiv preprint arXiv:2301.12485, 2023.\n[32] Yeqing Lin, Minji Lee, Zhao Zhang, and Mohammed AlQuraishi. Out of many, one: Designing and scaffolding\nproteins at the scale of the structural universe with genie 2. arXiv preprint arXiv:2405.15489, 2024.\n[33] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil,\nOri Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with a language\nmodel. Science, 379(6637):1123–1130, 2023.\n[34] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative\nmodeling. arXiv preprint arXiv:2210.02747, 2022.\n[35] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit:\nExploring flow and diffusion-based generative models with scalable interpolant transformers. In European\nConference on Computer Vision, pages 23–40. Springer, 2024.\n[36] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, pages 4195–4205, 2023.\n[37] Wei Qu, Jiawei Guan, Rui Ma, Ke Zhai, Weikun Wu, and Haobo Wang. P (all-atom) is unlocking new path for\nprotein design. bioRxiv, pages 2024–08, 2024.\n[38] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Beyond next-token: Next-x\nprediction for autoregressive visual generation. arXiv preprint arXiv:2502.20388, 2025.\n14\n\n-- 14 of 23 --\n\n[39] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans. Advances in neural information processing systems, 29, 2016.\n[40] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable\nimage generation via next-scale prediction. Advances in neural information processing systems, 37:84839–84865,\n2024.\n[41] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971, 2023.\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and\nIllia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n[43] Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, and Quanquan Gu. Dplm-2: A multimodal\ndiffusion protein language model. arXiv preprint arXiv:2410.13782, 2024.\n[44] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model.\narXiv preprint arXiv:2212.00490, 2022.\n[45] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody\nAhern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function\nwith rfdiffusion. Nature, 620(7976):1089–1100, 2023.\n[46] Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent neural networks.\nNeural computation, 1(2):270–280, 1989.\n[47] Jason Yim, Andrew Campbell, Andrew YK Foong, Michael Gastegger, José Jiménez-Luna, Sarah Lewis, Vic-\ntor Garcia Satorras, Bastiaan S Veeling, Regina Barzilay, Tommi Jaakkola, et al. Fast protein backbone generation\nwith se (3) flow matching. arXiv preprint arXiv:2310.05297, 2023.\n[48] Jason Yim, Brian L Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, and Tommi\nJaakkola. Se (3) diffusion model with application to protein backbone generation. In International Conference on\nMachine Learning, pages 40001–40039. PMLR, 2023.\n[49] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation\nautoencoders. arXiv preprint arXiv:2510.11690, 2025.\n15\n\n-- 15 of 23 --\n\nAppendix\nA Implementation and Evaluation Details\nWe follow the implementation of Proteina [ 14 ] for training PAR, using the same architecture and hyperparameter\nsetup. Training is conducted on 8 H100 GPUs, with a batch size of 15 per GPU, for a total of 200k steps.\nWe train the flow-based backbone decoder with 60 M, 200 M, and 400 M parameters, using the same\nnon-equivariant transformer architecture as Proteina. For the autoregressive module, we adopt Proteina’s\nsmallest configuration (60 M parameters), as we find that a small AR module is enough to yield competitive\ngeneration quality, discussed in §4.3. For a fair comparison, we trained Proteina from scratch under the same\nsetting and achieved similar or even better performance than results reported in the original paper. For other\nbaselines, we directly obtain the results from [ 14 ]. Model and training configurations can be found in Tab. 6.\nNote that we remove pair representations, triangle update as well as auxiliary loss for memory and training\nefficiency, and the additional trainable parameters come from the 60M autoregressive transformer encoder.\nTable 6 Hyperparameters for PAR models. Tθ : autoregressive transformer; vθ : flow-based atomic decoder.\nTθ \tvθ\nPAR Architecture \t60M 60M 200M 400M\ninitialization \trandom random random random\nsequence repr dim \t512 512 768 1024\nsequence cond dim \t128 128 512 512\nt sinusoidal enc dim \t196 196 256 256\ninterpolated position enc dim 196 196 128 128\n# attention heads \t12 \t12 \t12 \t16\n# transformer layers \t12 \t12 \t15 \t18\n# trainable parameters \t60M 60M 200M 400M\nA.1 Implementation Details\nIn §3.2 we briefly introduce two novel techniques for our autoregressive modeling: scale embedding and\ninterpolated position embedding.\nScale Embedding. Since we use a shared decoder to train across all scales, we introduce a scale embedding to\ndistinguish data distributions at different scales. Each scale is assigned a unique scale id, which is incorporated\ninto the model to help disambiguate the varying statistical characteristics associated with different scales.\nInterpolated Position Embedding. Interpolated position embedding is a natural extension to the standard\nposition embedding for sequence representation. In the raw structure, each residue is associated with a\n3D coordinate and a position ID ranging from 1 to L, where L is the protein length. Our downsampled\nstructure and interpolated position embeddings are derived from the raw structure and position IDs via\ninterpolation, following the sequential order of residues. Each interpolated residue is computed by interpolating\nthe coordinates of neighboring real residues, while each interpolated position ID is obtained by interpolating\nover the corresponding relative positions. This approach has the advantage that, across inputs of different\nlengths (i.e., different scales), the interpolated positions still reflect the relative location of each interpolated\nresidue within the original structure, providing a coarse-grained view of the real protein.\nA.2 Evaluation Metrics\nWe evaluate the model from multiple perspectives, including quality and diversity, following evaluation\nprotocols established in prior literature by [5, 48]. Specifically, we sample 100 structures for each of the five\nsequence lengths: 50, 100, 150, 200, and 250, resulting in a total of 500 structures for evaluation.\nDesignability. Following the procedure from Yim et al. [48], we generate 8 candidate sequences for each\nstructure using ProteinMPNN [ 11 ] with a temperature of 0.1. Each sequence is folded into a predicted\nstructure using ESMFold [ 33]. We compute the root-mean-square deviation (RMSD) between each predicted\n16\n\n-- 16 of 23 --\n\nstructure and the original generated structure, and record the minimum RMSD across the 8 predictions.\nA structure is considered designable if its minimum RMSD is less than 2 Å. We report the proportion of\ndesignable structures and the average minimum RMSD across all samples.\nDiversity. Following Bose et al. [5] , we compute the average pairwise TM-score among all designable structures\nfor each sequence length. The final diversity score is obtained by averaging these values across all five lengths.\nSecondary Structure. To analyze secondary structure characteristics, we annotate all designable structures\nusing the P-SEA algorithm [ 27] as implemented in Biotite [26]. For each structure, we compute the proportion\nof alpha helices and beta sheets, and report the average proportions across all samples.\nTo better assess the model’s overall structural fidelity at the distributional level, we adopt two metrics\nintroduced in Geffner et al. [14]. We randomly sample 125 structures at each sequence length from 60 to 255\n(with a step size of 5), resulting in 5,000 structures in total. Importantly, no designability filtering is applied\nduring this stage, and all samples are used for evaluation.\nFrchet Protein Structure Distance (FPSD). Analogous to the Fréchet Inception Distance (FID) [17 ], FPSD\nmeasures the Wasserstein distance between the distributions of generated and reference structures. Structures\nare embedded into a feature space defined by a fold class predictor, and the distance is computed based on\nthe resulting Gaussian approximations.\nProtein Fold Score (fS). Inspired by the Inception Score (IS) [ 39 ], the fS metric encourages both diversity\nand sample-level quality. High-quality generations lead to confident fold class predictions, while diversity is\ncaptured by the entropy across the predicted fold distribution.\nA.3 Unconditional Backbone Generation\nWe train 200M and 400M models for Proteina and PAR for 200k steps, using Adam optimizer with learning\nrate 1e-4, no warmup applied. For evaluation, we sample from Proteina and PAR with the same techniques\nbelow. We follow the optimal configuration and sample 400 steps for Proteina. For PAR, we find 1k steps\nshow better results.\nSelf conditioning. Self-conditioning has been widely employed in protein design. During sampling, the model’s\nown previous predictions\nˆx(xt) = xt + (1 − t)vθ\nt (xt) (7)\nare fed back as conditions to guide subsequent generation. During training, the model is conditioned on its own\npredictions with a probability of 50%. Sampling can be performed either with or without self-conditioning.\nLow temperature sampling. In Eqn. 6, the parameter γ is injected to control the scale of noise. When γ = 1,\nthis SDE yields the same marginals as the ODE defined by flow model. In practice, it is common to use a\nlower γ < 1 which empirically improves designability at the cost of diversity. In this paper, we use γ = 0.30\nby default.\nCategory of unconditional backbone generation baselines. We categorize each baseline based on their\nmodeling types and frameworks in the table below.\nTable 7 Category of unconditional backbone generation baselines.\nMethod Type Framework\nFrameDiff Frame Diffusion\nRFDiffusion Frame Diffusion\nESM3 Token PLM\nGenie2 Ca Diffusion\nProteina Ca FM\nPAR Ca PAR\n17\n\n-- 17 of 23 --\n\nB Datasets\nThe training data is derived from the curated AFDB representative dataset (denoted as DFS, containing 0.6M\nstructures), as processed by Proteina. This dataset ensures both high quality (pLDDT > 80) and structural\ndiversity, with sequence lengths ranging from 32 to 256 residues. We follow [14 ] and split it by 98:19:1 for\ntraining, validation and testing. For PDB finetuning, as the dataset used in [14] is not publicly available, we\nreproduce their filtering protocol and curate a designable subset of 21K samples from PDB.\nC More Empirical Analysis\nC.1 Efficient Sampling with SDE/ODE Orchestration\nFigure 7 Designability analysis of multi-scale SDE/ODE sampling methods. Naively reducing the SDE\nsampling steps substantially degrades the designability (red). Using ODE alone exhibits limited designability (purple).\nOrchestrating SDE and ODE sampling enables reduced sampling steps while retaining designability (blue and green).\nWe report the designability over varying sampling steps in Fig. 7. Leveraging SDE sampling at the first scale\nand ODE for the remaining scales, PAR could effectively reduce diffusion steps without harming designability,\nhighlighting the unique advantage of multi-scale design to orchestrate SDE and ODE sampling at different\nscales. In addition, aggressively reducing SDE steps or replacing SDE with ODE across all scales yields much\nworse designability, highlighting the necessity of combining both sampling methods. These results suggest\nthat PAR decomposes backbone generation into coarse topology formation and efficient structure refinement\nat later scales.\nC.2 Ablation with Self-Conditioning\nMulti-scale autoregressive modeling and self-conditioning similarly guide the generation with a coarse estimate\nof the structure. To evaluate the role of self-conditioning in our multiscale framework, we conducted an\nablation study (Fig. 8), where the results are from the same 60M model in the previous ablation study. Across\nall length ranges, the model with self-conditioning consistently generates higher-quality protein structures,\nin terms of sc-RMSD. Although self-conditioning also supplies an intermediate structural estimate during\ngeneration, it is complementary to the multi-scale formulation and yields further improvements in structural\nquality.\nC.3 Long Protein Generation\nFinetuning on longer protein chains. We follow Proteina to finetune our models on datasets with longer\nproteins. Since Proteina has not released its long-protein dataset, we cannot fully reproduce their experiment\nsetups. Instead, we follow the filtering procedure described in their appendix on PDB structures to curate\n18\n\n-- 18 of 23 --\n\n50 \t100 \t150 \t200 \t250\nProtein Length\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nsc-RMSD (Å)\n0.79\n1.07\n1.43\n1.81\n1.97\n0.88\n1.08\n1.57\n1.86\n2.02\nPAR\nw/o self-condition\nFigure 8 Ablation with self-conditioning. Self-conditioning consistently improves backbone generation performance\nof PAR across varying protein lengths, showing that both methods are compatible. Results are obtained with 60M\nPAR.\n300 \t400 \t500 \t600 \t700\nResidue Length\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\nDensity\nDistribution of Residue Lengths\nFigure 9 Protein length distribution for long protein finetuning.\na long-protein dataset. We filter PDB structures to lengths between 256 and 768 residues and keep only\ndesignable samples, resulting in 26k high-quality proteins. The length-distribution of this dataset (Fig. 9)\nexhibits a long-tail shape with peaks around 300-400 residues. We then finetune the 400M PAR and Proteina\nmodels in Tab. 1 on this dataset for 10k steps.\nLong-protein generation. We generate 100 proteins for each length in {300, 400, 500, 600, 700}. PAR exhibits\nhigher designability at lengths {300, 400}, consistent with the higher density of training samples in this range.\nAt lengths between 500 to 700, both Proteina and PAR show degraded designability, while PAR demonstrating\nslightly better results. We attribute this to the long-tail nature of the training set, which includes far fewer\nsamples in the length range between 500 and 700. The limited size of the training set (26K) also potentially\nhinders the model from reaching its full potential. We leave scaling up long-protein data as a promising\ndirection for future work.\nTable 8 Long protein generation. scR: sc-RMSD (Å) ↑. DesA: Designability (%) ↓.\n300 400 500 600 700\nscR DesA scR DesA scR DesA scR DesA scR DesA\nProteina 1.91 85 2.70 61 4.09 49 7.90 21 13.32 4\nPAR 1.28 93 1.65 72 3.19 52 6.80 29 11.29 10\n19\n\n-- 19 of 23 --\n\nC.4 Foldseek Cluster Diversity\nTable 9 Foldseek cluster diversity.\nγ Designable Clusters\n0.35 119\n0.40 126\n0.45 142\n0.50 140\n0.60 164\n0.70 160\n0.80 146\nWe investigated the foldseek cluster diversity of PAR-generated samples. A larger γ increases sampling\nstochasticity and improves the diversity, reaching its peak value at γ=0.6. We generate 500 structures, with\n100 samples for each length in {50, 100, 150, 200, 250}. We use the same foldseek command following Geffner\net al. [14] with a tmscore threshold of 0.5. The command is\nfoldseek easy-cluster <path_samples> <path_tmp>/res <path_tmp>\n--alignment-type 1 --cov-mode 0 --min-seq-id 0\n--tmscore-threshold 0.5\nC.5 Zero-shot Motif Scaffold Benchmark\nTable 10 Zero-shot motif scaffold benchmark. PAR* indicates our zero-shot model, producing 100 samples, while\nother baselines require finetuning. Baseline results are taken directly from Geffner et al. [14], which reports results\nusing 1000 samples. SR: success rate. Results are obtained with 60M PAR.\nUnique Solutions (%)\nPAR* Proteina Genie2 RFDiffusion FrameFlow\n1PRW \t0 \t0.3 \t0.2 \t0.1 \t0.3\n1BCF \t0 \t0.1 \t0.1 \t0.1 \t0.1\n5TPN \t0 \t0.4 \t0.8 \t0.5 \t0.6\n5IUS \t0 \t0.1 \t0.1 \t0.1 \t0\n3IXT \t6.0 \t0.8 \t1.4 \t0.3 \t0.8\n5YUI \t0 \t0.5 \t0.3 \t0.1 \t0.1\n1QJG \t1.0 \t0.3 \t0.5 \t0.1 \t1.8\n1YCR \t4.0 \t24.9 13.4 \t0.7 \t14.9\n2KL8 \t4.0 \t0.1 \t0.1 \t0.1 \t0.1\n7MRX.60 \t0 \t0.2 \t0.5 \t0.1 \t0.1\n7MRX.85 \t1.0 \t3.1 \t2.3 \t1.3 \t2.2\n7MRX.128 \t1.0 \t5.1 \t2.7 \t6.6 \t3.5\n4JHW \t0 \t0 \t0 \t0 \t0\n4ZYP \t0 \t1.1 \t0.3 \t0.6 \t0.4\n5WN9 \t0 \t0.2 \t0.1 \t0 \t0.3\n5TRV_short \t0 \t0.1 \t0.3 \t0.1 \t0.1\n5TRV_med \t1.0 \t2.2 \t2.3 \t1.0 \t2.1\n5TRV_long \t0 \t17.9 \t9.7 \t2.3 \t7.7\n6E6R_short \t6.0 \t5.6 \t2.6 \t2.3 \t2.5\n6E6R_med \t3.0 \t41.7 27.2 \t15.1 \t9.9\n6E6R_long \t3.0 \t71.3 41.5 \t38.1 \t11.0\n6EXZ_short \t3.0 \t0.3 \t0.2 \t0.1 \t0.3\n6EXZ_med \t8.0 \t4.3 \t5.4 \t2.5 \t11.0\n6EXZ_long \t10.0 29.0 32.6 \t16.7 \t40.3\n# tasks (SR ≥ 1%) 13 \t11 \t11 \t9 \t11\nWe quantify the zero-shot motif scaffolding performance of PAR in Tab. 10. For other training-based methods,\nwe directly quote the results reported in Proteina [14].\nWe use PAR to generate 100 backbone structures for each benchmark problem in Watson et al. [45] . Following\nProteina’s evaluation protocol, we produce 8 ProteinMPNN sequences with the motif residues fixed, and feed\n20\n\n-- 20 of 23 --\n\neach sequence to ESMFold. Using the predicted structure, we calculate ca-RMSD and MotifRMSD. A design\nis considered a success if any sequence achieves scRMSD ≤ 2Å, a motifRMSD ≤ 1Å, pLDDT ≥ 70, and pAE\n≤ 5. Note that our method is the only one evaluated in a zero-shot setting, whereas all other baselines rely\non training or finetuning with additional motif conditioning.\nC.6 Scale-Agnostic Inference\nTable 11 Inference with flexible scale configuration.\nDesignability FPSD ↓ fS ↑\n(%) ↑ (sc-RMSD) ↓ vs. PDB vs. AFDB (C/A/T)\nPAR (3 scale) 96.6 1.04 160.99 228.44 2.57/7.42/23.61\nw/o scale emb 92.8 1.16 175.09 246.34 2.54/7.66/26.68\n5 scale inference 72.6 1.74 177.01 246.76 2.56/7.53/26.78\nIn our original setup, we included a learnable scale embedding vector as part of the AR module’s conditioning.\nThis embedding allows the model to identify the current scale and adjust its behavior (e.g., generating coarse\nvs. fine structures). However, since the dimensionality of this learnable embedding is fixed to the number of\nscales, the model cannot be applied to a different scale configuration at inference.\nTo explore flexible scale configurations, we finetune an alternative model that simply discards the learnable\nembedding on the PDB designable subset for 5k steps. This formulation cancels the embedding from a fixed\nnumber of scales and enables inference across arbitrary scale settings. As shown in the Tab. 11, when inferring\nwith five scales using this 3-scale model, FPSD remains stable, suggesting that the model still captures the\nunderlying data distribution under altered scale configurations. However, the designability substantially drops,\nindicating that sampling with an unseen scale configuration fails to preserve structural detail, ultimately\nleading to lower-quality results.\nC.7 Ablating AR and Decoder Size\nTable 12 Effect of AR module and decoder size. Both AR and decoder utilize transformer-based architectures.\nAR Decoder sc-RMSD Designability (%)\n400M 60M 1.26 87.80\n60M 400M 1.01 96.00\n60M 60M 1.19 92.60\nWe introduced an ablation study examining the AR encoder size, and discussed crucial design choices for\nboth the AR encoder and flow-based decoder. We summarize key findings below.\nPer-token vs per-scale decoder. In our preliminary study, we implemented the model with a 200M-parameter\nAR module and, following MAR [ 30], used a 3-layer MLP ( 20M) as the diffusion head. However, this setup\nfailed to generate reasonable structures, yielding an average sc-RMSD of 16. This likely occurs because a\nper-token decoder is not expressive enough to capture the global correlations between atoms that is required\nto produce a reliable coarse structure at the first scale, which is crucial for the subsequent coarse-to-fine\nrefinement. These observations motivated our shift to a per-scale transformer-based decoder.\nLarge vs. small decoder. As shown in Tab. 12 and our scaling experiments in §4.3, using a large decoder\nbrings effective improvements to generation quality.\nLarge AR vs small AR. With the decoder size fixed, increasing the AR transformer size from 60M to 400M\ndoes not offer improvements. One one hand, this is consistent with the module’s role to generate scale-wise\nconditioning to guide the backbone generation, which does not require large model capacity—a similar trend\nobserved in image generation [ 8]. In addition, we believe this is due to exposure bias: the AR module overfits\n21\n\n-- 21 of 23 --\n\nto ground truth context to stabilize training, resulting in a mismatch with inference, where the model relies\non its predictions as context. This issue becomes more severe under several conditions:\n(1) Larger AR models tend to overfit the context more strongly, making exposure bias more severe.\n(2) Limited data increases overfitting risks: our 588K training structures (32–256 residues each) provide far\nless coverage than datasets like ImageNet (1.28M 256x256 images).\n(3) High precision tasks like protein modeling are sensitive to small errors, making exposure bias more serious\nthan in image generation, where the compressed VAE latents lie in a smoother Gaussian space that is\nrobust to small errors at the cost of some visual details [29, 49].\nOur noisy context learning and scheduled sampling mitigate this issue for the 60M PAR, but scaling the AR\ntransformer appears to intensify this issue. Exploring more training data is a potential solution and we leave\nthis for future work.\nC.8 Sequence-Based Downsampling Preserves Pairwise Spatial Relationships\nTable 13 RMSE and LDDT across different downsample sizes.\nSize(i) 16 32 64 128\nRMSE 0.362 0.275 0.217 0.170\nLDDT 1 1 1 1\nWe discuss whether 1D downsampling properly preserves pairwise spatial relationships. To study this,\nwe attempt to investigate the difference between pairwise distances computed after downsampling the 1D\ncoordinate sequence and those obtained by downsampling the full-resolution 2D distance map. We discuss\ndetails below.\nSpatial relationships in downsampled 1D sequence. We follow the process below to quantify the spatial\nrelationships:\n1. Downsample the coordinate sequence from RL×3 to Rsize(i)×3 for each scale i.\n2. We compute pairwise distance maps using the downsampled sequence, leading to a size(i) × size(i)\nmap.\nSpatial relationships in 3D space after downsampling. We quantify this using the pairwise distance map\ncalculated from the full-resolution structure:\n1. Calculate the pairwise distance map of the structure, producing a L × L map.\n2. We downsample pairwise map this using the F.interpolate(mode=’bicubic’) operation, resulting in\na size(i) × size(i) map.\nDoes sequence-based downsampling preserve spatial relationships?\nWe select all samples from the testing set, and calculate the RMSE and LDDT between the aforementioned\ntwo size(i) × size(i) pairwise maps for each sample. As expected, rmse slightly increases as size(i) decreases,\nreflecting the loss of fine-grained details at coarser scales. However, lddt remains consistently at 1 and the\nrmse values remain low across all scales. Together, these results indicate that, despiste small information loss\nat the coarse scales, 1D sequence downsampling preserves the essential pairwise spatial correlations captured\nby the downsampled 2D distance map.\nC.9 Visualization of Attention Scores\nWe provide attention score visualization for shorter proteins in Fig. 10. The pattern generally aligns with\nFig. 6, where each scale primarily attends to its previous scale.\n22\n\n-- 22 of 23 --\n\n1 \t2 \t3\n1\n2\n3\nLayer 0\n1 \t2 \t3\n1\n2\n3\nLayer 5\n1 \t2 \t3\n1\n2\n3\nLayer 11\n0.2\n0.4\n0.6\n0.8\n1.0\n1 \t2 \t3 \t4\n1\n2\n3\n4\nLayer 0\n1 \t2 \t3 \t4\n1\n2\n3\n4\nLayer 5\n1 \t2 \t3 \t4\n1\n2\n3\n4\nLayer 11\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 10 Visualization of the average attention scores in PAR autoregressive transformer over 3/4 scales.\nLeft Length ∈ (32, 64]. Right Length ∈ (64, 128].\nD Other Related Work\nFlow and diffusion-based structure generative models. Flow-based and diffusion methods have been\nwidely applied to protein backbone generation, with examples including RFDiffusion [ 45] and Chroma [22].\nSubsequently, various protein representations have been proposed for protein structure generation. FrameDiff,\nFoldFlow and FrameFlow [ 5, 47, 48] model protein structures through per-residue rotation and translation\npredictions, employing a frame-based Riemannian manifold representation [20 , 23]. Building upon FrameFlow,\nMultiflow [ 7] jointly models sequence and structures. In contrast, Genie and Genie2 [31, 32] generate protein\nbackbones by diffusing the Cα coordinates. Pallatom and Protpardelle [ 10, 37] further generate fully atomistic\nproteins that include side-chains. Meanwhile, Proteina [14] leverages a non-equivariant transformer architecture\nto model the Cα backbone coordinates, exhibiting scalability and simplicity. In addition to continuous diffusion\nand flow-matching based approaches, discrete diffusion methods like ESM3 [15 ] and DPLM-2 [ 43] have been\ntrained on structure tokens, which often reduce structural fidelity and thus limit structure generation quality\n[19].\n23\n\n-- 23 of 23 --\n\n",
    "analysis_json": "{\"title\":\"Protein Autoregressive Modeling via Multiscale Structure Generation\",\"authors\":[\"Yanru Qu\",\"Cheng-Yen Hsieh\",\"Zaixiang Zheng\",\"Ge Liu\",\"Quanquan Gu\"],\"publication_year\":\"2026\",\"summary\":\"PAR is a hierarchical protein structure generation framework that treats the protein design process like sculpting a statue. Unlike previous autoregressive models that struggle with protein geometry, PAR uses a multi-scale approach to predict coarse topology first and then refine structural details. It integrates an autoregressive transformer with a flow-based decoder to generate continuous 3D coordinates, overcoming the limitations of discretization and exposure bias through novel training strategies like noisy context learning.\",\"breakthrough_score\":85,\"breakthrough_reasoning\":\"This work successfully adapts the autoregressive paradigm—highly successful in LLMs—to the continuous 3D domain of protein structures. By moving from sequence-order prediction to 'next-scale' prediction, it solves the bidirectional dependency problem that previously made AR models inferior to Diffusion in structural biology. The zero-shot prompt-following capability is a significant advancement for functional protein design.\",\"key_claims\":[\"Next-scale prediction preserves spatial and bidirectional dependencies better than sequence-based prediction.\",\"A continuous flow-based decoder avoids the fidelity loss associated with tokenizing protein structures.\",\"Noisy context learning and scheduled sampling effectively mitigate the exposure bias typical of AR models.\",\"PAR enables zero-shot motif scaffolding and human-prompted generation without specific fine-tuning.\",\"A multi-scale approach allows for SDE/ODE orchestration, achieving up to 2.5x faster sampling than single-scale models.\"],\"testable_hypotheses\":[{\"hypothesis\":\"Coarse-scale SDE sampling combined with fine-scale ODE refinement maintains structural designability while significantly reducing compute time.\",\"how_to_test\":\"Compare a baseline model using 400 SDE steps at all scales against a PAR model using 400 SDE steps at the 1/4 scale and 2 ODE steps at the full scale.\",\"expected_outcome\":\"The hybrid SDE/ODE approach should maintain over 90 percent designability while reducing inference time by 2-3x.\"},{\"hypothesis\":\"Noisy Context Learning (NCL) is the primary factor in preventing error accumulation across scales.\",\"how_to_test\":\"Train identical PAR architectures with and without NCL, then measure the self-consistency RMSD (sc-RMSD) of generated proteins at the final scale.\",\"expected_outcome\":\"Models without NCL will show significantly higher sc-RMSD (poor quality) due to sensitivity to small errors in the initial coarse topology.\"}],\"key_equations\":[{\"name\":\"Multiscale Autoregressive Likelihood\",\"latex\":\"p_theta(X) = Product_{i=1}^{n} p_theta(x_i | X_{<i})\",\"description\":\"The probability of a full protein structure is decomposed into the product of probabilities of each scale given all previous (coarser) scales.\",\"variables\":[{\"name\":\"x_i\",\"description\":\"The protein representation at the i-th scale of granularity\",\"typical_range\":\"i=1 to 5\"},{\"name\":\"X_{<i}\",\"description\":\"All structural context generated in previous coarser steps\",\"typical_range\":\"Set of 3D coordinates\"}]},{\"name\":\"Flow Matching Objective with Multi-scale Conditioning\",\"latex\":\"L(theta) = E || v_theta(x_t, t, z_i) - (x_i - epsilon_i) ||^2\",\"description\":\"The decoder learns to predict the vector field that transforms noise into the target structural scale, conditioned on latent embeddings from the AR transformer.\",\"variables\":[{\"name\":\"v_theta\",\"description\":\"The flow-based backbone decoder network\",\"typical_range\":\"Velocity field\"},{\"name\":\"z_i\",\"description\":\"Conditional embedding produced by the AR transformer for scale i\",\"typical_range\":\"High-dimensional vector\"}]}],\"simulation_possibilities\":[{\"title\":\"Interactive 'Protein Sculptor'\",\"description\":\"A visualization tool showing the 5-scale generation process. Users can see a 'cloud' of 16 points expand into a rough 3D fold and finally refine into an alpha-helix or beta-sheet structure.\",\"complexity\":\"Medium\",\"variables\":[\"Sampling steps per scale\",\"Scale granularity\",\"Noise scaling parameter (gamma)\"],\"expected_insights\":\"Users see how the global topology is locked in early (Scale 1-2) while the secondary structure details only emerge at the final scales.\",\"visualization_type\":\"3d\"},{\"title\":\"Zero-Shot Motif Scaffolder\",\"description\":\"Users upload a small functional motif (e.g., a binding loop). The simulation 'freezes' these points and asks the AR transformer to generate the rest of the protein around it scale-by-scale.\",\"complexity\":\"High\",\"variables\":[\"Motif location\",\"Target protein length\",\"Teacher-forcing strength\"],\"expected_insights\":\"Demonstrates how the model naturally fills in gaps to create a stable structure that supports a specific function without specific task training.\",\"visualization_type\":\"interactive\"},{\"title\":\"SDE vs ODE Efficiency Explorer\",\"description\":\"A side-by-side comparison of generation speed and quality using different sampling strategies (All SDE vs. PAR's Hybrid S/O/O strategy).\",\"complexity\":\"Low\",\"variables\":[\"Inference time\",\"Trajectory smoothness\",\"Final structure designability\"],\"expected_insights\":\"Shows that 'stochasticity' is vital for finding the global shape but 'determinism' is sufficient for refining details.\",\"visualization_type\":\"chart\"}],\"field\":\"Computational Biology\",\"related_fields\":[\"Machine Learning\",\"Structural Bioinformatics\",\"Generative AI\",\"Biophysics\"],\"limitations\":[\"Currently limited to C-alpha backbone atoms only, requiring separate models for all-atom reconstruction.\",\"Exposure bias is mitigated but not entirely eliminated, potentially limiting performance on extremely long proteins.\",\"Quadratic scaling of transformer attention still poses challenges for very large protein complexes.\"],\"difficulty_to_understand\":\"Advanced\",\"prerequisites\":[\"Transformers and Attention Mechanisms\",\"Flow Matching and Diffusion Models\",\"Protein Hierarchical Structure (Primary/Secondary/Tertiary)\",\"Autoregressive Learning Principles\"]}",
    "created_at": 1770295429557
  }
}