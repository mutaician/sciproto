{
  "analyses": {
    "arxiv-2602.04883": {
      "hash": "arxiv-2602.04883",
      "filename": "2602.04883.pdf",
      "raw_text": "Protein Autoregressive Modeling via Multiscale Structure\nGeneration\nYanru Qu1,2,∗, Cheng-Yen Hsieh1,∗,†, Zaixiang Zheng1, Ge Liu2, Quanquan Gu1,‡\n1ByteDance Seed, 2University of Illinois Urbana-Champaign\n∗Equal Contributions, †Project Lead, ‡Corresponding Author\nAbstract\nWe present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for\nprotein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature\nof proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and\nrefining structural details over scales. To achieve this, PAR consists of three key components: (i)\nmulti-scale downsampling operations that represent protein structures across multiple scales during\ntraining; (ii) an autoregressive transformer that encodes multi-scale information and produces\nconditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that\ngenerates backbone atoms conditioned on these embeddings. Moreover, autoregressive models\nsuffer from exposure bias, caused by the training and the generation procedure mismatch, and\nsubstantially degrades structure generation quality. We effectively alleviate this issue by adopting\nnoisy context learning and scheduled sampling, enabling robust backbone generation. Notably,\nPAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional\ngeneration and motif scaffolding without requiring fine-tuning. On the unconditional generation\nbenchmark, PAR effectively learns protein distributions and produces backbones of high design\nquality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a\npromising framework for protein structure generation.\nDate: February 5, 2026\nCorrespondence: Quanquan Gu quanquan.gu@bytedance.com\nProject Page: https://par-protein.github.io\nNote: Work was done during Yanru Qu’s internship at ByteDance Seed\n1 Introduction\nDeep generative modeling of proteins has emerged as a way to design and model novel structures with desired\nfunctions and properties, with broad applications in biomedicine and nanotechnology [ 21, 25 ]. A widely\nadopted approach is to directly model the distribution of three-dimensional protein structures, which govern\nprotein function. Typically, structure generative models produce protein backbones without sequences or side\nchains. Prior work in this area could be broadly categorized into methods that predict the SE(3) backbone\nframe representations [ 45 , 47] and those that directly model atoms, e.g., Cα coordinates for simplicity and\nscalability [ 14, 31]. However, all these works are based on diffusion models and their variations (e.g., flow\nmatching).\n1\narXiv:2602.04883v1 [cs.LG] 4 Feb 2026\n\n-- 1 of 23 --\n\nFigure 1 Overview of PAR. PAR comprises the autoregressive (AR) transformer Tθ and the flow-based backbone\ndecoder vθ . During training, we downsample a backbone x ∈ RL×3 into multi-scale representations {x1, . . . , x}.\nAR transformer performs next-scale prediction, producing conditional embeddings (z1, . . . , zn) from (bos, . . . , xn−1).\nThe shared flow-based decoder learns to denoise backbones xi at each scale conditioned on zi. At inference, PAR\nautoregressively generates xi until the final structure x is constructed.\nOn the other hand, autoregressive (AR) modeling has emerged as a powerful paradigm for large language\nmodels [ 1, 41]. AR models employ next-token prediction to model the probability of each token based on prior\nones, showing striking empirical behaviors such as scalability [ 24] and zero-shot generalization to unseen tasks\n[6].\nDespite its success in other domains, AR modeling has received little attention in backbone modeling. We\nidentify two main reasons. (i) Extending AR models to continuous data, e.g. atomic positions in 3D, often\nrelies on data discretization [12], which can reduce structural fidelity and fine-grained details for proteins,\nlimiting generative performance [ 19]. (ii) Protein residues exhibit strong bidirectional dependencies: residues\ndistant in sequence may be spatially close and form hydrogen bonds or hydrophobic contacts. This mutual\ndependency conflicts with the unidirectional assumption of standard AR models, and thus limits the quality\nof previous attempts on autoregressive structure generation [ 13]. A natural question therefore arises: can we\napply AR modeling to protein backbone design?\nIn this paper, we answer the above question affirmatively, and propose PAR, a Protein AutoRegressive\nframework, to unlock the power of AR models for protein backbone generation. We take initiative from the\nhierarchical nature of proteins: their structures span multiple scales of granularity, from coarse 3D topology\nand tertiary fold arrangements, local secondary structures, to the finest atomic coordinates. PAR thus adopts\na multi-scale autoregressive framework via next-scale prediction, predicting each scale conditioned on prior\ncoarser scales. This strategy, inspired by advances in image generation, enabled AR models to surpass strong\ndiffusion models in image synthesis for the first time [ 40 ], and further allows multimodal LLMs to achieve\nunified text and image generation framework [28].\nBuilding on this multi-scale framework, PAR includes three key components (Fig. 1). The multi-scale\ndownsampling creates coarse-to-fine structural representations to serve as structural context and targets\nduring training. AR transformer, a stack of non-equivariant attention layers [42 ], encodes all preceding\nscales to produce a scale-wise conditional embedding following Li et al. [30] . The flow-based backbone decoder\nis conditioned on this embedding to model Cα backbone atoms directly. As a result, PAR avoids both\ndiscretization of protein structures and residue-wise unidirectional autoregressive ordering, thereby overcoming\nthe two aforementioned limitations that compromise structural fidelity and generative quality. Moreover,\ntraining on ground-truth structural context, AR models suffer from exposure bias [ 3], which is a key challenge\nsubstantially reducing structure generation quality in our preliminary study. We effectively mitigate such\nissue via noisy context learning and scheduled sampling, allowing the model to learn from corrupted context.\nThis multi-scale approach introduces several notable model behaviors. PAR generates backbones by establishing\na global topology and performing refinements, analogous to progressively sculpting a statue into a masterpiece\n(Fig. 2). For unconditional generation, PAR exhibits favorable scaling behavior, yielding competitive results on\n2\n\n-- 2 of 23 --\n\nFigure 2 Samples generated by PAR over scales. We illustrate PAR’s generation process across five scales. Much\nlike sculpting a statue, the model first formulates the global structural layout at coarse scales and progressively refines\nthe details at later scales.\ndistributional metrics like Fréchet Protein Structure Distance (FPSD). Unlike diffusion models, which operate\nat a single scale, PAR flexibly handles inputs at various granularities, and hence shows zero-shot generalization\nin tasks like prompt-based generation and motif scaffolding. The multi-scale formulation enables PAR to\norchestrate sampling strategies, achieving a 2.5x sampling speedup compared to single-scale baselines. Finally,\nPAR provides a more general framework, incorporating flow-based models as a special case when restricted to\na single scale, and thus remains compatible with techniques from flow-based models like self-conditioning [9].\nMain contributions: (i) We present PAR, the first multi-scale AR model for protein backbone generation\nthat addresses key limitations of existing AR methods. (ii) PAR comprises multi-scale downsampling, AR\ntransformer, and a flow-based decoder, to directly model Cα atom, avoiding discretization loss. (iii) We\nalleviate exposure bias through noisy context learning and scheduled sampling, effectively improving structure\ngeneration. (iv) Our model shows an interpretable generation process that forms coarse backbone topology\nand refines it progressively. (v) Benchmarking results show that PAR effectively captures protein data\ndistributions, achieving FPSD score of 161.0 against PDB dataset that further scale with training compute.\n(vi) PAR exhibits efficient sampling and zero-shot generalization potential, reflecting the versatility of AR\nlarge language models.\n2 Background and Related Work\nFlow and diffusion-based structure generative models. Flow-based and diffusion methods [18, 34] operate by\ntransforming samples from a prior distribution to the target data distribution, and have been widely applied\nto protein backbone generation. These methods either predict per-residue rotations and translations using\na frame-based Riemannian manifold representation [ 5, 22, 45, 47, 48 ] or directly model atom coordinates,\nsuch as Cα positions [14, 31 , 32], with some approaches generating fully atomistic proteins including side\nchains [ 10, 37]. Discrete diffusion methods [15, 43], trained on structure tokens, often reduce structural fidelity\nand limit generation quality [ 19]. Unlike most diffusion approaches, which are single-scale, PAR models\nprotein structures across multiple scales using a parameterized upsampling autoregressive process from short\nto long, allowing flexible handling of different structural granularities and zero-shot generalization to tasks\nlike prompt-based generation. In addition, PAR provides a more general framework, as it naturally reduces to\na flow-based model when restricted to a single scale.\n3\n\n-- 3 of 23 --\n\nAutoregressive modeling. Autoregressive (AR) modeling has been driving natural language processing and\ncomputer vision due to its strong scalability and zero-shot generalization [1, 40, 41]. The approach relies on\nnext-token prediction that predicts the distribution of the next token based on prior ones in a unidirectional\nsequence. However, adapting autoregressive models to continuous domains, like image generation, often\ninvolves tokenizers such as VQVAE [12 ], which discretizes the data for transformer training and may discard\nfine-grained details. Recently, Li et al. [30] used the AR model that produces conditioning for a diffusion\nnetwork (e.g., a small MLP) to model image latents, unlocking the operations of AR models in a continuous-\nvalued space. In addition, defining appropriate autoregressive orders that preserve data properties is crucial.\nSince next-token prediction inherently discards spatial locality by flattening the 2D image feature map into a\n1D sequence, VAR [40] introduced next-scale prediction. Leveraging a multi-scale VQVAE, the image feature\nmap is quantized into n multi-scale token maps that preserve the spatial and bidirectional correlations. To\nour knowledge, autoregressive modeling has not been widely applied to protein structure generation despite\ntheir success in other domains. The only exception is Gaujac et al. [13], which models structure tokens with\na causal transformer. In contrast, we design a multi-scale autoregressive framework that operates directly\nin continuous backbone space using a flow-based backbone decoder, thereby addressing the limitations of\ndiscrete token maps while respecting the bidirectional biophysical relations of protein structures.\n3 Protein Autoregressive Modeling\nIn this section, we introduce PAR, a multi-scale autoregressive (AR) framework for protein backbone generation.\nFormally, we want to model a protein backbone Cα structure with L residues x ∈ RL×3 in an autoregressive\nmanner as follows: pθ (x) = EX∼qdecompose(·|x)\n\u0002pθ (X = {x1, . . . , xn})\u0003\n= EX∼qdecompose(·|x)\nn\tY\ni=1\npθ (xi | X<i). (1)\nwhere qdecompose(·|x) defines a decomposition of autoregressive order for protein structure x into n scales\nX = {x1, . . . , xn} with xn = x, while pθ (xi | X<i) is the desired PAR model learning to generate x via a\nscale-wise autoregression.\nThe design space of qdecompose and pθ under this formulation (Eqn. 1) can be flexible. Recall that our goal is\nto enable AR modeling to preserve spatial dependencies and avoid discretization, as discussed in §1. To this\nend, in §3.1, we devise a non-parametric and deterministic qdecompose by multi-scale protein downsampling\n(Fig. 1) that represents protein backbones at multiple scales via hierarchical down-sampling (Eqn. 2), providing\nstructural context and training targets. In §3.2, we parameterize PAR pθ as a backbone autoregressive\nupsampling process via next-scale prediction and achieve direct Cα modeling in the continuous space (Eqn. 3).\nThis comprises two key components: (i) an autoregressive transformer (Fig. 1) that produces scale-wise\nconditional embeddings informed by preceding scales to guide generation (Eqn. 4); and (ii) a flow-based\nbackbone decoder (Fig. 1) which samples Cα backbone coordinates conditioned on the learned embeddings\n(Eqn. 5).\nFinally, in §3.3, we dedicated strategies to mitigate exposure bias [ 3], a mismatch between training on ground-\ntruth data and inference on model predictions that leads to error accumulations and degrading generation\nquality in AR models. Together, these components enable PAR to robustly generate protein backbones in a\ncoarse-to-fine manner.\n3.1 Multi-scale Protein Downsampling\nWe construct the multi-scale representations of protein structures via hierarchical downsampling to serve as\ntraining context and targets for PAR (Fig. 1). Given a protein structure x ∈ RL×3, it produces a hierarchy of\ncoarse-to-fine scales by progressively downsampling x into n scales:\nqdecompose : x 7 → X = {x1, x2, . . . , xn}\n= {Down(x, size(1)), Down(x, size(2)), . . . , x}. (2)\n4\n\n-- 4 of 23 --\n\nwhere Down(x, size(i)) ∈ Rsize(i)×3 denotes a downsampling operation that interpolates x along the sequence\ndimension, leading to size(i) 3D centroids that provide a coarse structural layout. Since qdecompose is designed\nas a deterministic mapping for every x, the likelihood of Eqn. 1 can be simplified without marginalization:\npθ (x) = Qn\ni=1 pθ (xi | X<i). We show that this downsampling strategy properly preserves pairwise spatial\nrelationships in §C.8.\nScale configurations. S = {size(1), . . . , size(n)} could be defined in two ways. When defined by length,\nscales are chosen as hyperparameters, e.g., S = {64, 128, 256}. In this case, if L lies in (size(i), size(i + 1)],\nthe protein could be generated with only i+1 autoregressive steps. When defined by ratio, scales are adaptively\ndetermined based on protein length, e.g., S = {L/4, L/2, L}. Empirically, defining scales by length yields\nslightly better results in modeling data distributions. We adopt this as the default configuration. This\ndesign enables training PAR with flexible scale configurations. In the following sections, we describe how this\nhierarchy of representations are modeled using the autoregressive transformer and backbone decoder.\n3.2 Coarse-to-Fine Backbone Autoregressive Modeling\nPreserving the inherent dependencies in data when defining the autoregressive order is crucial and affects\ngeneration performance [ 40]. Standard AR models assume unidirectional dependency, which conflicts with\nthe strong bidirectional interactions in protein sequences, e.g., spatially close residues can form hydrophobic\ncontacts or hydrogen bonds even if distant in sequence. PAR addresses this with a multi-scale AR framework\nvia next-scale prediction, capturing mutual structural dependency over each scale. Motivated by Li et al. [30] ,\nwe propose to use an AR Transformer with diffusion/flow-based regression loss to enable modeling of Cα\natoms directly in continuous space. That is, we could rewrite the likelihood as:\npθ (X = {x1, . . . , xn}) =\nn\tY\ni=1\npθ (xi|X<i)\n=\nn\tY\ni=1\npθ (xi | zi = Tθ (X<i)),\n(3)\nwhere Tθ is an AR Transformer that produces scale-wise conditioning zi while pθ (xi|zi) is optimized with a\nflow-based atomic decoder vθ with flow matching. This avoids discretizing protein structures into tokens,\npreserving structural details and generation fidelity. We describe each component below.\nAutoregressive transformer for scale-wise conditioning. To formulate the autoregressive order, we leverage\nthe hierarchical nature of proteins, where a protein structure could span various levels of representations from\ncoarse tertiary topology to the finest atomic coordinates. We adopt the next-scale prediction to model per-scale\ndistribution based on prior coarser scales, which further ensures that the bidirectional dependencies of residues\nare modeled over each scale. We train our autoregressive model (Fig. 1), a non-equivariant transformer Tθ , to\nproduce scale-wise conditioning embedding zi for scale i depending on prior scales X<i = {x1, . . . , xi−1}:\nzi = Tθ (X<i) = Tθ\n\u0010\u0002bos, Up(x1, size(2)), . . . , Up(xi−1, size(i))\u0003\u0011\n. (4)\nwhere bos ∈ Rsize(1)×3 is a learnable embedding, and Up(xi−1, size(i)) interpolates xi−1 to size(i) 3D\npoints. All inputs are concatenated along the sequence dimension before being fed into Tθ . The embedding zi\nis then used to condition the flow matching decoder to predict the backbone coordinates xi, detailed below.\nFlow-based atomic decoder. We enable PAR to directly model Cα positions x, wherein pθ (x|zi) is param-\neterized by an atomic decoder vθ with flow matching [FM, 34], which maps standard normal distribution\nto the target data distribution. We condition the vθ with scale-wise conditioning zi predicted by the AR\nTransformer Tθ at each scale i (Fig. 1). During training, we sample the noise ϵi ∼ N (0, I) and a time variable\nti ∈ [0, 1], and compute the interpolated sample as xi\nti = ti · xi + (1 − ti) · ϵi. As such, we can jointly train vθ\nand Tθ with an FM objective:\nL(θ) = Ex∼pD\n\u0014 1\nn\nn\tX\ni=1\n1\nsize(i) Eti∼p(ti),ϵi∼N (0,I) vθ (xi\nti , ti, zi) − (xi − ϵi) 2\n\u0015\n. (5)\n5\n\n-- 5 of 23 --\n\nwhere pD (x) denotes the training data distribution and p(t) denotes the t-sampling distribution in Geffner\net al. [14] . The conditioning embedding zi is injected into the atomic decoder network vθ through adaptive\nlayer norms [36]. We further concatenate a learnable scale embedding alongside zi to help the model identify\ndifferent scales and incorporate self-conditioning input as an additional condition [ 9], though we omit them in\nthe equation for simplicity. To formulate the indices for positional encoding pi at scale i, we uniformly sample\nsize(i) numbers from the interval [1, L], i.e., pi = linspace(1, L, size(i)). At coarse scales, the wide spacing\nbetween adjacent indices encourages the model to capture global structural layout, while at finer scales the\ndense indices allow the model to focus on local details. For more details, please refer to §A.1.\nLeveraging the learned flow network vθ , sampling could be performed at each scale through ordinary differential\nequation (ODE) dxt = vθ (xt, t) dt, with the scale superscript i and condition z omitted for simplicity. Moreover,\nwe could define the stochastic differential equation (SDE) for sampling:\ndxt = vθ (xt, t) dt + g(t) sθ (xt, t) dt + p2g(t)γ dWt, (6)\nwhere g(t) is a time-dependent scaling function for the score function sθ (xt, t) [2, 35] and the noise term, γ is\na noise scaling parameter, and Wt is a standard Wiener process. The score function, defined as the gradient\nof the log-probability of the noisy data distribution at time t, could be computed as sθ (xt, t) = t vθ (xt,t)−xt\n1−t .\nMulti-scale structure generation. At inference, the autoregressive transformer first produces z1 at the coarsest\nscale, which conditions the flow matching decoder to generate x1 either via ODE or SDE sampling in Eqn. 6.\nWe upsample x1 using Up(x1, size(2)) and send it back into the autoregressive transformer to predict the\nnext scale embedding z2. This coarse-to-fine process iterates n times until the flow-matching model generates\nthe full-resolution backbone x. KV cache is applied throughout the autoregressive process for efficiency.\n3.3 Mitigating Exposure Bias\nTraining AR models typically uses teacher forcing [46], where ground-truth data are fed as context to stabilize\nlearning. However, during inference the model is conditioned on its own predictions, creating a training-\ninference mismatch known as exposure bias [ 3, 16]. Errors can then accumulate across autoregressive steps,\ndegrading output quality. Our preliminary study shows that teacher forcing greatly reduces the designability\nof generated structures. To mitigate this, we adapt Noisy Context Learning (NCL) and Scheduled Sampling\n(SS), techniques from language and image AR modeling [4, 38], for PAR.\nNoisy context learning. We train PAR with noisy context, adding noise to the ground-truth prior-scale input\nduring training. This encourages the model to learn the per-scale distribution without relying on perfectly\naccurate context, improving robustness. We randomly sample n noise weights {w1\nncl, · · · , wn\nncl} ∈ [0, 1], and\ndraw n noise samples {ϵ1\nncl, · · · , ϵn\nncl} ∈ N (0, I). Each input context xi is corrupted as xi\nncl = wi\nncl · xi +\n(1 − wi\nncl) · ϵi\nncl. This perturbation is applied to the input context only during training, which updates the\nautoregressive step in Eqn. 4 as zi = Tθ\n\u0010\u0002bos, Up(x1\nncl, size(2)), . . . , Up(xi−1\nncl , size(i))\u0003\u0011\n.\nScheduled sampling. During training, we use scheduled sampling [ 4] by running the forward process\niteratively across scales. At the i-th scale, the flow-based backbone decoder predicts the clean data xi\npred =\nxi\nt + (1 − ti)vθ (xi\nt, ti, zi). With a probability of 0.5, we replace the ground truth context xi with this prediction\nxi\npred at later scales. This exposes the model to its own output and reduces the train-test gap. Notably, we\ncould combine noisy context learning with this technique by adding noises to the model predicted context\nxi\npred.\n4 Experiments\nWe begin by evaluating PAR on unconditional backbone generation and compare it with existing structure\ngenerative methods in §4.1. Next, we examine its zero-shot generalization ability in §4.2. We then study the\nscaling behavior, efficient sampling, and propose strategies to mitigate exposure bias, along with additional\nablations in §4.3. We include additional empirical analysis in §C.\n6\n\n-- 6 of 23 --\n\nTable 1 Unconditional backbone generation performance. We follow Geffner et al. [14] in adopting FPSD and fS\nto evaluate the model’s ability to capture the data distribution. PARpdb denotes the 400M model finetuned on the\nPDB subset.\nMethod \tDesignability FPSD vs. fS Diversity Sec. Struct. %\n(%)↑ sc-RMSD↓ PDB↓ AFDB↓ (C / A / T)↑ TM-Sc.↓ (α/β)\nFrameDiff (17M) 65.4 - 194.2 258.1 2.46/5.78/23.35 0.40 64.9/11.2\nRFDiffusion (60M) 94.4 - 253.7 252.4 2.25/5.06/19.83 0.42 64.3/17.2\nESM3 (1.4B) 22.0 - 933.9 855.4 3.19/6.71/17.73 0.42 64.5/8.5\nGenie2 (16M) 95.2 - 350.0 313.8 1.55/3.66/11.65 0.38 72.7/4.8\nProteina (200M) 92.8 1.14 282.3 285.6 2.17/6.22/21.48 0.37 66.3/9.2\nProteina (400M) 92.6 1.09 271.3 272.6 2.13/6.14/21.18 0.37 65.1/9.5\nPAR (200M) 87.0 1.33 252.0 237.9 2.11/6.41/19.22 0.37 64.3/8.8\nPAR (400M) 96.0 1.01 313.9 296.4 2.24/6.60/16.71 0.39 66.3/8.9\nγ=0.45 88.0 1.28 231.5 211.8 2.20/6.59/20.96 0.36 63.2/9.7\nPARpdb \t96.6 1.04 161.0 228.4 2.57/7.42/23.61 0.43 50.2/16.7\nγ=0.45 88.8 1.34 176.6 256.4 2.62/7.52/30.99 0.40 50.2/16.2\n4.1 Protein Backbone Generation\nGeneration over scales. We illustrate PAR’s backbone generation using a 5-scale model in Fig. 2 (i.e.,\nS = {L/16, L/8, L/4, L/2, L}), showing generated structures with target lengths of {50, 100, 200, 250} residues.\nGeneration proceeds in a coarse-to-fine manner, which resonates with statue sculpting: the coarser scales\nestablish a rough global layout, and finer scales progressively add local details. This multi-scale formulation\nyields a clear and interpretable generation process. We present the quantitative analysis on PAR’s backbone\ngeneration in the next paragraph.\nUnconditional generation benchmark. We compare 3-scale PAR’s (S = {64, 128, 256}) backbone generation\nperformance with other baselines in Tab. 1, following the evaluation protocol in Geffner et al. [14]. The\nbaselines span three categories: frame-based diffusion methods [ 45 , 48], multimodal protein language models\n[ 15], and diffusion/flow-based Cα generators [ 14, 32 ]. We disable the optional pair representations and triangle-\nbased modules [23] in both PAR and Proteina to align architectural capacity and improve computational\nefficiency. We train PAR using a two-stage procedure following Geffner et al. [14]: the model is first trained\nfor 200K steps on the AFDB representative dataset and subsequently fine-tuned for 5K steps on a PDB subset\nof 21K designable samples. Results for the remaining baselines are taken directly from Geffner et al. [14] .\nEvaluation metrics and baseline categories are detailed in §A.2§A.3. To better reflect the goal of unconditional\nprotein generation as modeling the full data distribution, we adopt FPSD, which jointly measures quality\nand diversity by comparing generated and reference distributions, analogous to FID in image generation\n[ 17]. As shown in Tab. 1, PAR generates samples that closely match the reference data distribution and\nmaintains competitive designability. On FPSD, PAR achieves scores of 211.8 against AFDB and 231.5 against\nPDB. By reducing the noise scaling parameter γ (Equation 6) from 0.45 to 0.3 in SDE sampling, we can\nreduce sampling stochasticity and further improve sample quality, improving the designability from 88.0% to\n96.00%. After fine-tuning, PAR achieved 96.6% designability and 161.0 FPSD against the PDB, highlighting\nits superior distributional fidelity compared to pure diffusion-based baselines. We provide additional analysis\non longer proteins in §C.3.\n4.2 Zero-Shot Task Generalization\nGuiding backbone generation with human-specified prompt. Proteins possess hierarchical and complex\nstructures, which makes it challenging to directly specify a target shape and design proteins accordingly. By\nleveraging PAR’s coarse-to-fine generation, a simple prompt (e.g., 16 points) can specify a protein’s coarse\nlayout, from which the model generates the complete structure as shown in Fig. 3. In particular, we first\nobtain a 16-point input prompt either by downsampling a real protein structure from the test set, or by\nspecifying the points manually (the top row in Fig. 3). Using a 5-scale PAR (S = {16, 32, 64, 128, 256}), we\ninitialize the first-scale prediction with the 16-point prompt and autoregressively upsample until the full\n7\n\n-- 7 of 23 --\n\nPrompt\n(16 points)\nGenerated\nstructure\nmainly alpha mainly beta mainly alpha \tbeta barrel \tmixed \tshape A\tshape P \tshape R\nFigure 3 Backbone generation with human prompt. Given a small number of points (e.g., 16) as prompt, PAR can\ngenerate protein backbones that adhere to the global arrangements specified by these points, without any finetuning.\nFor visualization, input points are interpolated to match the length of the generated structure.\nFigure 4 Zero-shot motif scaffolding. Given a motif structure, PAR can generate diverse, plausible scaffold\nstructures that accurately preserve the motif via teacher-forcing the motif coordinates at each scale, without additional\nconditioning or fine-tuning.\nprotein structure is generated, as illustrated in the bottom row of Fig. 3. Following this process, PAR can\ngenerate a new structure that preserves the coarse structural layout (first five examples), and explore entirely\nnovel structures (last three examples). If desired, longer prompts (e.g., 32 points) could be specified to achieve\nmore finer-grained control over backbone generation. As later shown in Tab. 5, we quantitatively evaluate the\nstructural consistency (TM-score) between the prompted layout and the final generation.\nMotif scaffolding. Besides the point-based layout, PAR can preserve finer-grained prompts like atomic\ncoordinates. Fig. 4 highlights the zero-shot motif scaffolding capabilities of PAR. Using a 5-scale PAR, we\ndownsample a raw protein structure into five scales and teacher-force the ground-truth motif coordinates at\neach scale before propagating into the next scale. To avoid clashes or discontinuities, we superimpose the\nground-truth motif residues and the generated motif segments before replacement. With no fine-tuning and\nno conditioning, PAR generates plausible scaffolds that preserve motif structures with high fidelity. This\nstands in contrast to diffusion or flow-based frameworks, which typically require fine-tuning on additional\nconditions such as masks or motif coordinates, or rely on decomposition strategies [14, 44, 45]. Moreover, the\ngenerated scaffolds differ substantially from the input structure, showing that PAR generates structurally\ndiverse scaffolds rather than merely copying. For example, the leftmost example in Fig. 4 preserves the yellow\nmotif helix while introducing new secondary structure elements like β-sheet and loops, in contrast to the\noriginal helices. We further benchmark zero-shot motif scaffolding in Tab. 10 in the appendix, following\nevaluation protocols in [14, 32].\n8\n\n-- 8 of 23 --\n\nFigure 5 Scaling effects of PAR. Performance of four metrics over varying training steps and model sizes, (a) FPSD\nvs. PDB, (b) FPSD vs. AFDB, (c) fS(T), (d) sc-RMSD.\nTable 2 Sampling efficiency. Combining SDE and ODE sampling across scales yields a 2.5× inference speedup\ncompared to the single-scale 400-step baseline, shown in the first and the last row. We generate 100 samples at each\nlength.\nSampling Steps Length 150 Length 200\nTime (s) Design. (%) Time (s) Design. (%)\nProteina (SDE) 0/0/400 131 97% 170 92%\n0/0/200 67 89% 86 80%\nAll SDE 400/400/400 312 97% 351 94%\n400/400/2 184 0% - -\nAll ODE 400/400/400 312 28% - -\nS/S/O 400/400/400 312 98% - -\n400/400/2 184 99% 186 91%\nS/O/O 400/400/400 312 96% - -\n400/2/2 67 97% 68 94%\n4.3 Empirical Analysis of Multiscale PAR\nScaling effects of PAR. We examine the model’s behaviors by varying the backbone decoder’s size and\nnumber of training steps in Fig. 5. We train PAR with 3 scales over three different model sizes with 60,200,400\nmillion parameters and three training durations over 200, 400, 600K steps. PAR demonstrates favorable\nbehavior when scaling both model size and training duration, effectively improving its ability to capture the\nprotein data distribution with FPSD scores of 187 against PDB and 170 against AFDB (first two columns in\nFig. 5). Further, the fS scores, which reflect quality and diversity, increase with larger model sizes and greater\ncomputational budgets. While extending training duration alone offers negligible gains, increasing model size\nsubstantially enhances designability, leading to lower sc-RMSD values. Meanwhile, we empirically observe\nthat scaling the autoregressive transformer has minimal impacts on the evaluation results. This allows us to\nreduce computational costs and prioritize increasing the backbone decoder’s model capacity that effectively\nimproves generation quality. We provide more discussion on varying model sizes in §C.7.\nEfficient sampling with multi-scale orchestration of SDE/ODE. While Tab. 1 reports results using a uniform\nnumber of sampling steps across scales, the multi-scale formulation of PAR actually offers advantages in\nsampling efficiency, as shown in Tab. 2. More specifically, (1) sampling at the coarser scale (e.g., first scale) is\nmore efficient than sampling at finer scales (e.g., 2nd scale) due to shorter sequence length; (2) we can use less\nnumber of sampling steps at finer scales than coarser scales. As shown in Tab. 2, by using SDE sampling only\nat the first scale, and switching to ODE sampling for the remaining scales, PAR could dramatically reduce\nthe diffusion steps from 400 to 2 steps at the last two scales without harming designability (97%), yielding\na 4.7x inference speedup. This is possible because a high-quality coarse topology places the model near\nhigh-density regions, enabling efficient refinement with ODE sampling. Naively reducing the SDE sampling\n9\n\n-- 9 of 23 --\n\nsteps significantly harms designability, dropping to 22% when reducing steps to 50, as shown in Fig. 7. This is\nconsistent with the observation of single-scale models like Proteina, where designability degrades to 89% when\nreducing SDE sampling steps to 200 in Tab. 2. Crucially, SDE sampling at the first scale is necessary for\nestablishing a reliable global topology, given that ODE-only sampling exhibits poor designability. Compared\nto the single-scale 400-step baseline, PAR achieves 1.96x and 2.5x sampling speedup at length 150 and 200,\nrespectively. This improvement is driven by speeding up the final scales, where the longer sequence lengths\ncause computational costs to grow quadratically in transformer architectures. Moreover, the computational\ncosts remain constant at the first scale because it has a fixed size 64, even when generating longer sequences.\nTable 3 Mitigating exposure bias for PAR. We adopted various training strategies to mitigate the exposure bias for\nmulti-scale autoregressive modeling. These techniques are consistently effective effective in improving structure quality.\nNCL: Noisy Context Learning. SS: Schedule Sampling. Results are obtained with 60M PAR trained for 100K steps.\nMethod sc-RMSD ↓ FPSD vs. (PDB/AFDB) ↓ fS-(C/A/T) ↑\nTeacher Forcing 2.20 99.66 / 37.64 2.53 / 5.56 / 29.67\n+ NCL 1.58 89.70 / 23.69 2.54 / 5.85 / 28.37\n+ NCL & SS 1.48 90.66 / 24.59 2.54 / 5.84 / 28.77\nMitigating exposure bias. To mitigate exposure bias, we adopted noisy context learning (NCL) and scheduled\nsampling (SS) as defined in §3.3. Noisy context learning encourages the model to infer structural guidance\nfrom corrupted context and boosts the structure generation quality. Tab. 3 shows that noisy context learning\neffectively improves the sc-RMSD of the generated structure from 2.20 to 1.58, and reduces FPSD against\nAFDB to 23.69 when using ODE sampling. The designability further improved to 1.48 along with scheduled\nsampling, which makes the training process more aligned with the inference scenario. Results are obtained\nwith 60M PAR trained for 100K steps.\n1 \t2 \t3 \t4 \t5\n1\n2\n3\n4\n5\nLayer 0\n1 \t2 \t3 \t4 \t5\n1\n2\n3\n4\n5\nLayer 5\n1 \t2 \t3 \t4 \t5\n1\n2\n3\n4\n5\nLayer 11\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 6 Visualization of the average attention scores in PAR autoregressive transformer over 5 scales.\nObtained from samples with lengths in (128, 256]. We provide attention map visualization for shorter proteins in §C.9\nInterpreting multi-scale PAR. We visualize the attention maps of the autoregressive transformer at each scale\n(Fig. 6). We average the attention scores within each scale, normalize them such that the scores across\nscales sum to 1, and average them over 50 test samples to obtain the scale-level attention distribution during\ninference. We summarize three key observations: (i) Most scales barely attend to the first scale, since the\ninput to this scale, a bos token, carries little structural signal. (ii) Each scale primarily attends to the previous\nscale, which typically contains richer contextual and structural information. (iii) Despite focusing most heavily\non the current scale, the model still retains non-negligible attention to earlier scales. This indicates that PAR\neffectively integrates information across multiple scales and maintains structural consistency during generation.\nThis aligns with results in Tab. 5 where the autoregressive Transformer effectively improves consistency with\nthe given prompt. We also observe similar patterns on shorter proteins, as shown by the attention maps in\nFig. 10 in the appendix.\nMulti-scale formulation. We ablate the effect of defining scale by length versus ratio, as shown in §3.1. Tab. 4\nshows that under comparable levels of upsampling ratio ({64, 128, 256} and {L/4, L/2, L}), the by-length\nstrategy outperforms by-ratio. Meanwhile, PAR obtains better designability and FPSD when increasing from\n10\n\n-- 10 of 23 --\n\nTable 4 Multi-scale formulation. We ablate different strategies for scale configuration in downsampling. Results are\nobtained with 60M PAR.\nDesignability FPSD vs. fS\nDefine scale (%)↑ (sc-RMSD)↓ PDB↓ AFDB↓ (C / A / T)↑\n{64, 256} 83.0 1.38 282.85 274.32 2.14/6.58/20.66\n{64, 128, 256} 85.0 1.39 279.63 267.35 2.15/6.52/20.35\n{64, 128, 192, 256} 77.8 1.55 296.70 282.69 2.05/6.04/18.69\n{64, 96, 128, 192, 256} 81.0 1.51 276.00 263.58 2.17/6.31/20.65\n{L/4, L/2, L} 86.4 1.49 310.64 298.30 2.00/5.87/18.91\ntwo scales to three scales. Beyond this point, increasing the scale configurations to four and five scales results\nin degraded designability, potentially due to error accumulation and exposure bias. These results support our\nchoice of adopting the 3-scale PAR as the default. All results are obtained using the 60M model.\nTable 5 Structural consistency for prompted generation. Using a transformer to encode prior-scale structural\nconditions shows better prompt-following than direct input. Results are obtained with 60M PAR.\nRMSD vs. Reference ↓ TM-score vs. Prompt ↑\nLength (32.64] (64,128] (128,256] (32.64] (64,128] (128,256]\nReference - - - 0.60 0.61 0.59\nDirect Input 2.13 3.38 6.51 0.58 0.61 0.59\nTrans. Encode 1.45 2.72 5.75 0.60 0.64 0.61\nAR Transformer improves structural consistency. We conduct an ablation study to evaluate the effectiveness\nof the autoregressive transformer in Tab. 5. We compare two different strategies for encoding prior-scale\nstructural context, including (i) direct input, where the multi-scale structures are directly fed into the\nbackbone decoder without any intermediate encoding; and (ii) transformer encoder, where all scales are\nprocessed autoregressively by a Transformer encoder, and the resulting encoded representation is then passed\nto the backbone decoder. We train two 60M models and evaluate both models by downsampling 588 testing\nstructures as prompts and re-upsamples them with PAR. As shown in Tab. 5, the transformer encoder\ndemonstrates better structural consistency, indicating that autoregressive encoding across scales produces\ncoherent structural guidance over scales, consistent with attention maps in Fig. 6.\n5 Discussion\nPAR is the first multi-scale autoregressive model for protein backbone generation, offering a general framework\nthat includes flow-based methods as a special case. PAR addressed limitations of standard autoregressive\nmodels, such as unidirectional dependency, discretization, and exposure bias. Our method robustly models\nstructures over multiple granularities and in turn enables strong zero-shot generalization. This capability\nincludes coarse-prompted conditional generation using points (e.g., 16 points) as structural layout and finer-\ngrained controls such as atomic-coordinate-based motif scaffolding. For unconditional backbone generation,\nPAR exhibits powerful distributional fidelity and generation quality. The analysis of scale-level attention map\nprovides additional insights into how the multi-scale formulation operates.\nWe hope that PAR unlocks the potential of autoregressive modeling for protein design. Some promising\nopen directions include: (1) Conformational dynamics modeling. PAR can, in principle, perform zero-shot\nmodeling of conformational distributions: we downsample a structure and upsample it with PAR to mimic\nlocal molecular dynamics. We leave this exciting application for future research. (2) All-atom modeling. This\nwork focuses on backbone Cα atoms to prioritize autoregressive design, but it’s natural to extend to full-atom\nrepresentations [37]. The multi-scale framework offers an advantage for flexible zero-shot prompt-based\nall-atom designs.\n11\n\n-- 11 of 23 --\n\nAcknowledgments\nWe thank Dr. Hang Li, Liang Hong, Xinyou Wang, Jiasheng Ye, Yi Zhou, Jing Yuan, Yilai Li, Zhenghua\nWang, Yuning Shen, Huizhuo Yuan, as well as other colleagues at ByteDance Seed for their valuable comments\nand support.\n12\n\n-- 12 of 23 --\n\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774, 2023.\n[2] Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework\nfor flows and diffusions. arXiv preprint arXiv:2303.08797, 2023.\n[3] Kushal Arora, Layla El Asri, Hareesh Bahuleyan, and Jackie Chi Kit Cheung. Why exposure bias matters: An\nimitation learning perspective of error accumulation in language generation. arXiv preprint arXiv:2204.01171,\n2022.\n[4] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with\nrecurrent neural networks. Advances in neural information processing systems, 28, 2015.\n[5] Avishek Joey Bose, Tara Akhound-Sadegh, Guillaume Huguet, Kilian Fatras, Jarrid Rector-Brooks, Cheng-Hao\nLiu, Andrei Cristian Nica, Maksym Korablyov, Michael Bronstein, and Alexander Tong. Se (3)-stochastic flow\nmatching for protein backbone generation. arXiv preprint arXiv:2310.02391, 2023.\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877–1901, 2020.\n[7] Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola. Generative flows on discrete\nstate-spaces: Enabling multimodal flows with applications to protein co-design. arXiv preprint arXiv:2402.04997,\n2024.\n[8] Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, and Ping Luo. Pixelflow: Pixel-space generative models\nwith flow. arXiv preprint arXiv:2504.07963, 2025.\n[9] Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusion models\nwith self-conditioning. arXiv preprint arXiv:2208.04202, 2022.\n[10] Alexander E Chu, Jinho Kim, Lucy Cheng, Gina El Nesr, Minkai Xu, Richard W Shuai, and Po-Ssu Huang. An\nall-atom protein generative model. Proceedings of the National Academy of Sciences, 121(27):e2311500121, 2024.\n[11] Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert J Ragotte, Lukas F Milles, Basile IM\nWicky, Alexis Courbet, Rob J de Haas, Neville Bethel, et al. Robust deep learning–based protein sequence design\nusing proteinmpnn. Science, 378(6615):49–56, 2022.\n[12] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 12873–12883, 2021.\n[13] Benoit Gaujac, Jérémie Donà, Liviu Copoiu, Timothy Atkinson, Thomas Pierrot, and Thomas D Barrett. Learning\nthe language of protein structure. arXiv preprint arXiv:2405.15840, 2024.\n[14] Tomas Geffner, Kieran Didi, Zuobai Zhang, Danny Reidenbach, Zhonglin Cao, Jason Yim, Mario Geiger, Christian\nDallago, Emine Kucukbenli, Arash Vahdat, et al. Proteina: Scaling flow-based protein structure generative models.\narXiv preprint arXiv:2503.00710, 2025.\n[15] Thomas Hayes, Roshan Rao, Halil Akin, Nicholas J Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Q\nTran, Jonathan Deaton, Marius Wiggert, et al. Simulating 500 million years of evolution with a language model.\nScience, 387(6736):850–858, 2025.\n[16] Tianxing He, Jingzhao Zhang, Zhiming Zhou, and James Glass. Exposure bias versus self-recovery: Are distortions\nreally incremental for autoregressive text generation? arXiv preprint arXiv:1905.10617, 2019.\n[17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained\nby a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing\nsystems, 30, 2017.\n[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural\ninformation processing systems, 33:6840–6851, 2020.\n13\n\n-- 13 of 23 --\n\n[19] Cheng-Yen Hsieh, Xinyou Wang, Daiheng Zhang, Dongyu Xue, Fei Ye, Shujian Huang, Zaixiang Zheng, and Quan-\nquan Gu. Elucidating the design space of multimodal protein language models. arXiv preprint arXiv:2504.11454,\n2025.\n[20] Chin-Wei Huang, Milad Aghajohari, Joey Bose, Prakash Panangaden, and Aaron C Courville. Riemannian\ndiffusion models. Advances in Neural Information Processing Systems, 35:2750–2761, 2022.\n[21] Po-Ssu Huang, Scott E Boyken, and David Baker. The coming of age of de novo protein design. Nature, 537\n(7620):320–327, 2016.\n[22] John B Ingraham, Max Baranov, Zak Costello, Karl W Barber, Wujie Wang, Ahmed Ismail, Vincent Frappier,\nDana M Lord, Christopher Ng-Thow-Hing, Erik R Van Vlack, et al. Illuminating protein space with a programmable\ngenerative model. Nature, 623(7989):1070–1078, 2023.\n[23] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn\nTunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure prediction\nwith alphafold. nature, 596(7873):583–589, 2021.\n[24] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361,\n2020.\n[25] Brian Kuhlman and Philip Bradley. Advances in protein structure prediction and design. Nature reviews molecular\ncell biology, 20(11):681–697, 2019.\n[26] Patrick Kunzmann and Kay Hamacher. Biotite: a unifying open source computational biology framework in\npython. BMC bioinformatics, 19(1):346, 2018.\n[27] Gilles Labesse, N Colloc’h, Joël Pothier, and J-P Mornon. P-sea: a new efficient assignment of secondary structure\nfrom cα trace of proteins. Bioinformatics, 13(3):291–295, 1997.\n[28] Han Li, Xinyu Peng, Yaoming Wang, Zelin Peng, Xin Chen, Rongxiang Weng, Jingang Wang, Xunliang Cai,\nWenrui Dai, and Hongkai Xiong. Onecat: Decoder-only auto-regressive model for unified understanding and\ngeneration. arXiv preprint arXiv:2509.03498, 2025.\n[29] Tianhong Li and Kaiming He. Back to basics: Let denoising generative models denoise. arXiv preprint\narXiv:2511.13720, 2025.\n[30] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without\nvector quantization. Advances in Neural Information Processing Systems, 37:56424–56445, 2024.\n[31] Yeqing Lin and Mohammed AlQuraishi. Generating novel, designable, and diverse protein structures by equivari-\nantly diffusing oriented residue clouds. arXiv preprint arXiv:2301.12485, 2023.\n[32] Yeqing Lin, Minji Lee, Zhao Zhang, and Mohammed AlQuraishi. Out of many, one: Designing and scaffolding\nproteins at the scale of the structural universe with genie 2. arXiv preprint arXiv:2405.15489, 2024.\n[33] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil,\nOri Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with a language\nmodel. Science, 379(6637):1123–1130, 2023.\n[34] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative\nmodeling. arXiv preprint arXiv:2210.02747, 2022.\n[35] Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit:\nExploring flow and diffusion-based generative models with scalable interpolant transformers. In European\nConference on Computer Vision, pages 23–40. Springer, 2024.\n[36] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, pages 4195–4205, 2023.\n[37] Wei Qu, Jiawei Guan, Rui Ma, Ke Zhai, Weikun Wu, and Haobo Wang. P (all-atom) is unlocking new path for\nprotein design. bioRxiv, pages 2024–08, 2024.\n[38] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Beyond next-token: Next-x\nprediction for autoregressive visual generation. arXiv preprint arXiv:2502.20388, 2025.\n14\n\n-- 14 of 23 --\n\n[39] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans. Advances in neural information processing systems, 29, 2016.\n[40] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable\nimage generation via next-scale prediction. Advances in neural information processing systems, 37:84839–84865,\n2024.\n[41] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971, 2023.\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and\nIllia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n[43] Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, and Quanquan Gu. Dplm-2: A multimodal\ndiffusion protein language model. arXiv preprint arXiv:2410.13782, 2024.\n[44] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model.\narXiv preprint arXiv:2212.00490, 2022.\n[45] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody\nAhern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function\nwith rfdiffusion. Nature, 620(7976):1089–1100, 2023.\n[46] Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent neural networks.\nNeural computation, 1(2):270–280, 1989.\n[47] Jason Yim, Andrew Campbell, Andrew YK Foong, Michael Gastegger, José Jiménez-Luna, Sarah Lewis, Vic-\ntor Garcia Satorras, Bastiaan S Veeling, Regina Barzilay, Tommi Jaakkola, et al. Fast protein backbone generation\nwith se (3) flow matching. arXiv preprint arXiv:2310.05297, 2023.\n[48] Jason Yim, Brian L Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, and Tommi\nJaakkola. Se (3) diffusion model with application to protein backbone generation. In International Conference on\nMachine Learning, pages 40001–40039. PMLR, 2023.\n[49] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation\nautoencoders. arXiv preprint arXiv:2510.11690, 2025.\n15\n\n-- 15 of 23 --\n\nAppendix\nA Implementation and Evaluation Details\nWe follow the implementation of Proteina [ 14 ] for training PAR, using the same architecture and hyperparameter\nsetup. Training is conducted on 8 H100 GPUs, with a batch size of 15 per GPU, for a total of 200k steps.\nWe train the flow-based backbone decoder with 60 M, 200 M, and 400 M parameters, using the same\nnon-equivariant transformer architecture as Proteina. For the autoregressive module, we adopt Proteina’s\nsmallest configuration (60 M parameters), as we find that a small AR module is enough to yield competitive\ngeneration quality, discussed in §4.3. For a fair comparison, we trained Proteina from scratch under the same\nsetting and achieved similar or even better performance than results reported in the original paper. For other\nbaselines, we directly obtain the results from [ 14 ]. Model and training configurations can be found in Tab. 6.\nNote that we remove pair representations, triangle update as well as auxiliary loss for memory and training\nefficiency, and the additional trainable parameters come from the 60M autoregressive transformer encoder.\nTable 6 Hyperparameters for PAR models. Tθ : autoregressive transformer; vθ : flow-based atomic decoder.\nTθ \tvθ\nPAR Architecture \t60M 60M 200M 400M\ninitialization \trandom random random random\nsequence repr dim \t512 512 768 1024\nsequence cond dim \t128 128 512 512\nt sinusoidal enc dim \t196 196 256 256\ninterpolated position enc dim 196 196 128 128\n# attention heads \t12 \t12 \t12 \t16\n# transformer layers \t12 \t12 \t15 \t18\n# trainable parameters \t60M 60M 200M 400M\nA.1 Implementation Details\nIn §3.2 we briefly introduce two novel techniques for our autoregressive modeling: scale embedding and\ninterpolated position embedding.\nScale Embedding. Since we use a shared decoder to train across all scales, we introduce a scale embedding to\ndistinguish data distributions at different scales. Each scale is assigned a unique scale id, which is incorporated\ninto the model to help disambiguate the varying statistical characteristics associated with different scales.\nInterpolated Position Embedding. Interpolated position embedding is a natural extension to the standard\nposition embedding for sequence representation. In the raw structure, each residue is associated with a\n3D coordinate and a position ID ranging from 1 to L, where L is the protein length. Our downsampled\nstructure and interpolated position embeddings are derived from the raw structure and position IDs via\ninterpolation, following the sequential order of residues. Each interpolated residue is computed by interpolating\nthe coordinates of neighboring real residues, while each interpolated position ID is obtained by interpolating\nover the corresponding relative positions. This approach has the advantage that, across inputs of different\nlengths (i.e., different scales), the interpolated positions still reflect the relative location of each interpolated\nresidue within the original structure, providing a coarse-grained view of the real protein.\nA.2 Evaluation Metrics\nWe evaluate the model from multiple perspectives, including quality and diversity, following evaluation\nprotocols established in prior literature by [5, 48]. Specifically, we sample 100 structures for each of the five\nsequence lengths: 50, 100, 150, 200, and 250, resulting in a total of 500 structures for evaluation.\nDesignability. Following the procedure from Yim et al. [48], we generate 8 candidate sequences for each\nstructure using ProteinMPNN [ 11 ] with a temperature of 0.1. Each sequence is folded into a predicted\nstructure using ESMFold [ 33]. We compute the root-mean-square deviation (RMSD) between each predicted\n16\n\n-- 16 of 23 --\n\nstructure and the original generated structure, and record the minimum RMSD across the 8 predictions.\nA structure is considered designable if its minimum RMSD is less than 2 Å. We report the proportion of\ndesignable structures and the average minimum RMSD across all samples.\nDiversity. Following Bose et al. [5] , we compute the average pairwise TM-score among all designable structures\nfor each sequence length. The final diversity score is obtained by averaging these values across all five lengths.\nSecondary Structure. To analyze secondary structure characteristics, we annotate all designable structures\nusing the P-SEA algorithm [ 27] as implemented in Biotite [26]. For each structure, we compute the proportion\nof alpha helices and beta sheets, and report the average proportions across all samples.\nTo better assess the model’s overall structural fidelity at the distributional level, we adopt two metrics\nintroduced in Geffner et al. [14]. We randomly sample 125 structures at each sequence length from 60 to 255\n(with a step size of 5), resulting in 5,000 structures in total. Importantly, no designability filtering is applied\nduring this stage, and all samples are used for evaluation.\nFrchet Protein Structure Distance (FPSD). Analogous to the Fréchet Inception Distance (FID) [17 ], FPSD\nmeasures the Wasserstein distance between the distributions of generated and reference structures. Structures\nare embedded into a feature space defined by a fold class predictor, and the distance is computed based on\nthe resulting Gaussian approximations.\nProtein Fold Score (fS). Inspired by the Inception Score (IS) [ 39 ], the fS metric encourages both diversity\nand sample-level quality. High-quality generations lead to confident fold class predictions, while diversity is\ncaptured by the entropy across the predicted fold distribution.\nA.3 Unconditional Backbone Generation\nWe train 200M and 400M models for Proteina and PAR for 200k steps, using Adam optimizer with learning\nrate 1e-4, no warmup applied. For evaluation, we sample from Proteina and PAR with the same techniques\nbelow. We follow the optimal configuration and sample 400 steps for Proteina. For PAR, we find 1k steps\nshow better results.\nSelf conditioning. Self-conditioning has been widely employed in protein design. During sampling, the model’s\nown previous predictions\nˆx(xt) = xt + (1 − t)vθ\nt (xt) (7)\nare fed back as conditions to guide subsequent generation. During training, the model is conditioned on its own\npredictions with a probability of 50%. Sampling can be performed either with or without self-conditioning.\nLow temperature sampling. In Eqn. 6, the parameter γ is injected to control the scale of noise. When γ = 1,\nthis SDE yields the same marginals as the ODE defined by flow model. In practice, it is common to use a\nlower γ < 1 which empirically improves designability at the cost of diversity. In this paper, we use γ = 0.30\nby default.\nCategory of unconditional backbone generation baselines. We categorize each baseline based on their\nmodeling types and frameworks in the table below.\nTable 7 Category of unconditional backbone generation baselines.\nMethod Type Framework\nFrameDiff Frame Diffusion\nRFDiffusion Frame Diffusion\nESM3 Token PLM\nGenie2 Ca Diffusion\nProteina Ca FM\nPAR Ca PAR\n17\n\n-- 17 of 23 --\n\nB Datasets\nThe training data is derived from the curated AFDB representative dataset (denoted as DFS, containing 0.6M\nstructures), as processed by Proteina. This dataset ensures both high quality (pLDDT > 80) and structural\ndiversity, with sequence lengths ranging from 32 to 256 residues. We follow [14 ] and split it by 98:19:1 for\ntraining, validation and testing. For PDB finetuning, as the dataset used in [14] is not publicly available, we\nreproduce their filtering protocol and curate a designable subset of 21K samples from PDB.\nC More Empirical Analysis\nC.1 Efficient Sampling with SDE/ODE Orchestration\nFigure 7 Designability analysis of multi-scale SDE/ODE sampling methods. Naively reducing the SDE\nsampling steps substantially degrades the designability (red). Using ODE alone exhibits limited designability (purple).\nOrchestrating SDE and ODE sampling enables reduced sampling steps while retaining designability (blue and green).\nWe report the designability over varying sampling steps in Fig. 7. Leveraging SDE sampling at the first scale\nand ODE for the remaining scales, PAR could effectively reduce diffusion steps without harming designability,\nhighlighting the unique advantage of multi-scale design to orchestrate SDE and ODE sampling at different\nscales. In addition, aggressively reducing SDE steps or replacing SDE with ODE across all scales yields much\nworse designability, highlighting the necessity of combining both sampling methods. These results suggest\nthat PAR decomposes backbone generation into coarse topology formation and efficient structure refinement\nat later scales.\nC.2 Ablation with Self-Conditioning\nMulti-scale autoregressive modeling and self-conditioning similarly guide the generation with a coarse estimate\nof the structure. To evaluate the role of self-conditioning in our multiscale framework, we conducted an\nablation study (Fig. 8), where the results are from the same 60M model in the previous ablation study. Across\nall length ranges, the model with self-conditioning consistently generates higher-quality protein structures,\nin terms of sc-RMSD. Although self-conditioning also supplies an intermediate structural estimate during\ngeneration, it is complementary to the multi-scale formulation and yields further improvements in structural\nquality.\nC.3 Long Protein Generation\nFinetuning on longer protein chains. We follow Proteina to finetune our models on datasets with longer\nproteins. Since Proteina has not released its long-protein dataset, we cannot fully reproduce their experiment\nsetups. Instead, we follow the filtering procedure described in their appendix on PDB structures to curate\n18\n\n-- 18 of 23 --\n\n50 \t100 \t150 \t200 \t250\nProtein Length\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nsc-RMSD (Å)\n0.79\n1.07\n1.43\n1.81\n1.97\n0.88\n1.08\n1.57\n1.86\n2.02\nPAR\nw/o self-condition\nFigure 8 Ablation with self-conditioning. Self-conditioning consistently improves backbone generation performance\nof PAR across varying protein lengths, showing that both methods are compatible. Results are obtained with 60M\nPAR.\n300 \t400 \t500 \t600 \t700\nResidue Length\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\nDensity\nDistribution of Residue Lengths\nFigure 9 Protein length distribution for long protein finetuning.\na long-protein dataset. We filter PDB structures to lengths between 256 and 768 residues and keep only\ndesignable samples, resulting in 26k high-quality proteins. The length-distribution of this dataset (Fig. 9)\nexhibits a long-tail shape with peaks around 300-400 residues. We then finetune the 400M PAR and Proteina\nmodels in Tab. 1 on this dataset for 10k steps.\nLong-protein generation. We generate 100 proteins for each length in {300, 400, 500, 600, 700}. PAR exhibits\nhigher designability at lengths {300, 400}, consistent with the higher density of training samples in this range.\nAt lengths between 500 to 700, both Proteina and PAR show degraded designability, while PAR demonstrating\nslightly better results. We attribute this to the long-tail nature of the training set, which includes far fewer\nsamples in the length range between 500 and 700. The limited size of the training set (26K) also potentially\nhinders the model from reaching its full potential. We leave scaling up long-protein data as a promising\ndirection for future work.\nTable 8 Long protein generation. scR: sc-RMSD (Å) ↑. DesA: Designability (%) ↓.\n300 400 500 600 700\nscR DesA scR DesA scR DesA scR DesA scR DesA\nProteina 1.91 85 2.70 61 4.09 49 7.90 21 13.32 4\nPAR 1.28 93 1.65 72 3.19 52 6.80 29 11.29 10\n19\n\n-- 19 of 23 --\n\nC.4 Foldseek Cluster Diversity\nTable 9 Foldseek cluster diversity.\nγ Designable Clusters\n0.35 119\n0.40 126\n0.45 142\n0.50 140\n0.60 164\n0.70 160\n0.80 146\nWe investigated the foldseek cluster diversity of PAR-generated samples. A larger γ increases sampling\nstochasticity and improves the diversity, reaching its peak value at γ=0.6. We generate 500 structures, with\n100 samples for each length in {50, 100, 150, 200, 250}. We use the same foldseek command following Geffner\net al. [14] with a tmscore threshold of 0.5. The command is\nfoldseek easy-cluster <path_samples> <path_tmp>/res <path_tmp>\n--alignment-type 1 --cov-mode 0 --min-seq-id 0\n--tmscore-threshold 0.5\nC.5 Zero-shot Motif Scaffold Benchmark\nTable 10 Zero-shot motif scaffold benchmark. PAR* indicates our zero-shot model, producing 100 samples, while\nother baselines require finetuning. Baseline results are taken directly from Geffner et al. [14], which reports results\nusing 1000 samples. SR: success rate. Results are obtained with 60M PAR.\nUnique Solutions (%)\nPAR* Proteina Genie2 RFDiffusion FrameFlow\n1PRW \t0 \t0.3 \t0.2 \t0.1 \t0.3\n1BCF \t0 \t0.1 \t0.1 \t0.1 \t0.1\n5TPN \t0 \t0.4 \t0.8 \t0.5 \t0.6\n5IUS \t0 \t0.1 \t0.1 \t0.1 \t0\n3IXT \t6.0 \t0.8 \t1.4 \t0.3 \t0.8\n5YUI \t0 \t0.5 \t0.3 \t0.1 \t0.1\n1QJG \t1.0 \t0.3 \t0.5 \t0.1 \t1.8\n1YCR \t4.0 \t24.9 13.4 \t0.7 \t14.9\n2KL8 \t4.0 \t0.1 \t0.1 \t0.1 \t0.1\n7MRX.60 \t0 \t0.2 \t0.5 \t0.1 \t0.1\n7MRX.85 \t1.0 \t3.1 \t2.3 \t1.3 \t2.2\n7MRX.128 \t1.0 \t5.1 \t2.7 \t6.6 \t3.5\n4JHW \t0 \t0 \t0 \t0 \t0\n4ZYP \t0 \t1.1 \t0.3 \t0.6 \t0.4\n5WN9 \t0 \t0.2 \t0.1 \t0 \t0.3\n5TRV_short \t0 \t0.1 \t0.3 \t0.1 \t0.1\n5TRV_med \t1.0 \t2.2 \t2.3 \t1.0 \t2.1\n5TRV_long \t0 \t17.9 \t9.7 \t2.3 \t7.7\n6E6R_short \t6.0 \t5.6 \t2.6 \t2.3 \t2.5\n6E6R_med \t3.0 \t41.7 27.2 \t15.1 \t9.9\n6E6R_long \t3.0 \t71.3 41.5 \t38.1 \t11.0\n6EXZ_short \t3.0 \t0.3 \t0.2 \t0.1 \t0.3\n6EXZ_med \t8.0 \t4.3 \t5.4 \t2.5 \t11.0\n6EXZ_long \t10.0 29.0 32.6 \t16.7 \t40.3\n# tasks (SR ≥ 1%) 13 \t11 \t11 \t9 \t11\nWe quantify the zero-shot motif scaffolding performance of PAR in Tab. 10. For other training-based methods,\nwe directly quote the results reported in Proteina [14].\nWe use PAR to generate 100 backbone structures for each benchmark problem in Watson et al. [45] . Following\nProteina’s evaluation protocol, we produce 8 ProteinMPNN sequences with the motif residues fixed, and feed\n20\n\n-- 20 of 23 --\n\neach sequence to ESMFold. Using the predicted structure, we calculate ca-RMSD and MotifRMSD. A design\nis considered a success if any sequence achieves scRMSD ≤ 2Å, a motifRMSD ≤ 1Å, pLDDT ≥ 70, and pAE\n≤ 5. Note that our method is the only one evaluated in a zero-shot setting, whereas all other baselines rely\non training or finetuning with additional motif conditioning.\nC.6 Scale-Agnostic Inference\nTable 11 Inference with flexible scale configuration.\nDesignability FPSD ↓ fS ↑\n(%) ↑ (sc-RMSD) ↓ vs. PDB vs. AFDB (C/A/T)\nPAR (3 scale) 96.6 1.04 160.99 228.44 2.57/7.42/23.61\nw/o scale emb 92.8 1.16 175.09 246.34 2.54/7.66/26.68\n5 scale inference 72.6 1.74 177.01 246.76 2.56/7.53/26.78\nIn our original setup, we included a learnable scale embedding vector as part of the AR module’s conditioning.\nThis embedding allows the model to identify the current scale and adjust its behavior (e.g., generating coarse\nvs. fine structures). However, since the dimensionality of this learnable embedding is fixed to the number of\nscales, the model cannot be applied to a different scale configuration at inference.\nTo explore flexible scale configurations, we finetune an alternative model that simply discards the learnable\nembedding on the PDB designable subset for 5k steps. This formulation cancels the embedding from a fixed\nnumber of scales and enables inference across arbitrary scale settings. As shown in the Tab. 11, when inferring\nwith five scales using this 3-scale model, FPSD remains stable, suggesting that the model still captures the\nunderlying data distribution under altered scale configurations. However, the designability substantially drops,\nindicating that sampling with an unseen scale configuration fails to preserve structural detail, ultimately\nleading to lower-quality results.\nC.7 Ablating AR and Decoder Size\nTable 12 Effect of AR module and decoder size. Both AR and decoder utilize transformer-based architectures.\nAR Decoder sc-RMSD Designability (%)\n400M 60M 1.26 87.80\n60M 400M 1.01 96.00\n60M 60M 1.19 92.60\nWe introduced an ablation study examining the AR encoder size, and discussed crucial design choices for\nboth the AR encoder and flow-based decoder. We summarize key findings below.\nPer-token vs per-scale decoder. In our preliminary study, we implemented the model with a 200M-parameter\nAR module and, following MAR [ 30], used a 3-layer MLP ( 20M) as the diffusion head. However, this setup\nfailed to generate reasonable structures, yielding an average sc-RMSD of 16. This likely occurs because a\nper-token decoder is not expressive enough to capture the global correlations between atoms that is required\nto produce a reliable coarse structure at the first scale, which is crucial for the subsequent coarse-to-fine\nrefinement. These observations motivated our shift to a per-scale transformer-based decoder.\nLarge vs. small decoder. As shown in Tab. 12 and our scaling experiments in §4.3, using a large decoder\nbrings effective improvements to generation quality.\nLarge AR vs small AR. With the decoder size fixed, increasing the AR transformer size from 60M to 400M\ndoes not offer improvements. One one hand, this is consistent with the module’s role to generate scale-wise\nconditioning to guide the backbone generation, which does not require large model capacity—a similar trend\nobserved in image generation [ 8]. In addition, we believe this is due to exposure bias: the AR module overfits\n21\n\n-- 21 of 23 --\n\nto ground truth context to stabilize training, resulting in a mismatch with inference, where the model relies\non its predictions as context. This issue becomes more severe under several conditions:\n(1) Larger AR models tend to overfit the context more strongly, making exposure bias more severe.\n(2) Limited data increases overfitting risks: our 588K training structures (32–256 residues each) provide far\nless coverage than datasets like ImageNet (1.28M 256x256 images).\n(3) High precision tasks like protein modeling are sensitive to small errors, making exposure bias more serious\nthan in image generation, where the compressed VAE latents lie in a smoother Gaussian space that is\nrobust to small errors at the cost of some visual details [29, 49].\nOur noisy context learning and scheduled sampling mitigate this issue for the 60M PAR, but scaling the AR\ntransformer appears to intensify this issue. Exploring more training data is a potential solution and we leave\nthis for future work.\nC.8 Sequence-Based Downsampling Preserves Pairwise Spatial Relationships\nTable 13 RMSE and LDDT across different downsample sizes.\nSize(i) 16 32 64 128\nRMSE 0.362 0.275 0.217 0.170\nLDDT 1 1 1 1\nWe discuss whether 1D downsampling properly preserves pairwise spatial relationships. To study this,\nwe attempt to investigate the difference between pairwise distances computed after downsampling the 1D\ncoordinate sequence and those obtained by downsampling the full-resolution 2D distance map. We discuss\ndetails below.\nSpatial relationships in downsampled 1D sequence. We follow the process below to quantify the spatial\nrelationships:\n1. Downsample the coordinate sequence from RL×3 to Rsize(i)×3 for each scale i.\n2. We compute pairwise distance maps using the downsampled sequence, leading to a size(i) × size(i)\nmap.\nSpatial relationships in 3D space after downsampling. We quantify this using the pairwise distance map\ncalculated from the full-resolution structure:\n1. Calculate the pairwise distance map of the structure, producing a L × L map.\n2. We downsample pairwise map this using the F.interpolate(mode=’bicubic’) operation, resulting in\na size(i) × size(i) map.\nDoes sequence-based downsampling preserve spatial relationships?\nWe select all samples from the testing set, and calculate the RMSE and LDDT between the aforementioned\ntwo size(i) × size(i) pairwise maps for each sample. As expected, rmse slightly increases as size(i) decreases,\nreflecting the loss of fine-grained details at coarser scales. However, lddt remains consistently at 1 and the\nrmse values remain low across all scales. Together, these results indicate that, despiste small information loss\nat the coarse scales, 1D sequence downsampling preserves the essential pairwise spatial correlations captured\nby the downsampled 2D distance map.\nC.9 Visualization of Attention Scores\nWe provide attention score visualization for shorter proteins in Fig. 10. The pattern generally aligns with\nFig. 6, where each scale primarily attends to its previous scale.\n22\n\n-- 22 of 23 --\n\n1 \t2 \t3\n1\n2\n3\nLayer 0\n1 \t2 \t3\n1\n2\n3\nLayer 5\n1 \t2 \t3\n1\n2\n3\nLayer 11\n0.2\n0.4\n0.6\n0.8\n1.0\n1 \t2 \t3 \t4\n1\n2\n3\n4\nLayer 0\n1 \t2 \t3 \t4\n1\n2\n3\n4\nLayer 5\n1 \t2 \t3 \t4\n1\n2\n3\n4\nLayer 11\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 10 Visualization of the average attention scores in PAR autoregressive transformer over 3/4 scales.\nLeft Length ∈ (32, 64]. Right Length ∈ (64, 128].\nD Other Related Work\nFlow and diffusion-based structure generative models. Flow-based and diffusion methods have been\nwidely applied to protein backbone generation, with examples including RFDiffusion [ 45] and Chroma [22].\nSubsequently, various protein representations have been proposed for protein structure generation. FrameDiff,\nFoldFlow and FrameFlow [ 5, 47, 48] model protein structures through per-residue rotation and translation\npredictions, employing a frame-based Riemannian manifold representation [20 , 23]. Building upon FrameFlow,\nMultiflow [ 7] jointly models sequence and structures. In contrast, Genie and Genie2 [31, 32] generate protein\nbackbones by diffusing the Cα coordinates. Pallatom and Protpardelle [ 10, 37] further generate fully atomistic\nproteins that include side-chains. Meanwhile, Proteina [14] leverages a non-equivariant transformer architecture\nto model the Cα backbone coordinates, exhibiting scalability and simplicity. In addition to continuous diffusion\nand flow-matching based approaches, discrete diffusion methods like ESM3 [15 ] and DPLM-2 [ 43] have been\ntrained on structure tokens, which often reduce structural fidelity and thus limit structure generation quality\n[19].\n23\n\n-- 23 of 23 --\n\n",
      "analysis_json": "{\"title\":\"Protein Autoregressive Modeling via Multiscale Structure Generation\",\"authors\":[\"Yanru Qu\",\"Cheng-Yen Hsieh\",\"Zaixiang Zheng\",\"Ge Liu\",\"Quanquan Gu\"],\"publication_year\":\"2026\",\"summary\":\"PAR is a hierarchical protein structure generation framework that treats the protein design process like sculpting a statue. Unlike previous autoregressive models that struggle with protein geometry, PAR uses a multi-scale approach to predict coarse topology first and then refine structural details. It integrates an autoregressive transformer with a flow-based decoder to generate continuous 3D coordinates, overcoming the limitations of discretization and exposure bias through novel training strategies like noisy context learning.\",\"breakthrough_score\":85,\"breakthrough_reasoning\":\"This work successfully adapts the autoregressive paradigm—highly successful in LLMs—to the continuous 3D domain of protein structures. By moving from sequence-order prediction to 'next-scale' prediction, it solves the bidirectional dependency problem that previously made AR models inferior to Diffusion in structural biology. The zero-shot prompt-following capability is a significant advancement for functional protein design.\",\"key_claims\":[\"Next-scale prediction preserves spatial and bidirectional dependencies better than sequence-based prediction.\",\"A continuous flow-based decoder avoids the fidelity loss associated with tokenizing protein structures.\",\"Noisy context learning and scheduled sampling effectively mitigate the exposure bias typical of AR models.\",\"PAR enables zero-shot motif scaffolding and human-prompted generation without specific fine-tuning.\",\"A multi-scale approach allows for SDE/ODE orchestration, achieving up to 2.5x faster sampling than single-scale models.\"],\"testable_hypotheses\":[{\"hypothesis\":\"Coarse-scale SDE sampling combined with fine-scale ODE refinement maintains structural designability while significantly reducing compute time.\",\"how_to_test\":\"Compare a baseline model using 400 SDE steps at all scales against a PAR model using 400 SDE steps at the 1/4 scale and 2 ODE steps at the full scale.\",\"expected_outcome\":\"The hybrid SDE/ODE approach should maintain over 90 percent designability while reducing inference time by 2-3x.\"},{\"hypothesis\":\"Noisy Context Learning (NCL) is the primary factor in preventing error accumulation across scales.\",\"how_to_test\":\"Train identical PAR architectures with and without NCL, then measure the self-consistency RMSD (sc-RMSD) of generated proteins at the final scale.\",\"expected_outcome\":\"Models without NCL will show significantly higher sc-RMSD (poor quality) due to sensitivity to small errors in the initial coarse topology.\"}],\"key_equations\":[{\"name\":\"Multiscale Autoregressive Likelihood\",\"latex\":\"p_theta(X) = Product_{i=1}^{n} p_theta(x_i | X_{<i})\",\"description\":\"The probability of a full protein structure is decomposed into the product of probabilities of each scale given all previous (coarser) scales.\",\"variables\":[{\"name\":\"x_i\",\"description\":\"The protein representation at the i-th scale of granularity\",\"typical_range\":\"i=1 to 5\"},{\"name\":\"X_{<i}\",\"description\":\"All structural context generated in previous coarser steps\",\"typical_range\":\"Set of 3D coordinates\"}]},{\"name\":\"Flow Matching Objective with Multi-scale Conditioning\",\"latex\":\"L(theta) = E || v_theta(x_t, t, z_i) - (x_i - epsilon_i) ||^2\",\"description\":\"The decoder learns to predict the vector field that transforms noise into the target structural scale, conditioned on latent embeddings from the AR transformer.\",\"variables\":[{\"name\":\"v_theta\",\"description\":\"The flow-based backbone decoder network\",\"typical_range\":\"Velocity field\"},{\"name\":\"z_i\",\"description\":\"Conditional embedding produced by the AR transformer for scale i\",\"typical_range\":\"High-dimensional vector\"}]}],\"simulation_possibilities\":[{\"title\":\"Interactive 'Protein Sculptor'\",\"description\":\"A visualization tool showing the 5-scale generation process. Users can see a 'cloud' of 16 points expand into a rough 3D fold and finally refine into an alpha-helix or beta-sheet structure.\",\"complexity\":\"Medium\",\"variables\":[\"Sampling steps per scale\",\"Scale granularity\",\"Noise scaling parameter (gamma)\"],\"expected_insights\":\"Users see how the global topology is locked in early (Scale 1-2) while the secondary structure details only emerge at the final scales.\",\"visualization_type\":\"3d\"},{\"title\":\"Zero-Shot Motif Scaffolder\",\"description\":\"Users upload a small functional motif (e.g., a binding loop). The simulation 'freezes' these points and asks the AR transformer to generate the rest of the protein around it scale-by-scale.\",\"complexity\":\"High\",\"variables\":[\"Motif location\",\"Target protein length\",\"Teacher-forcing strength\"],\"expected_insights\":\"Demonstrates how the model naturally fills in gaps to create a stable structure that supports a specific function without specific task training.\",\"visualization_type\":\"interactive\"},{\"title\":\"SDE vs ODE Efficiency Explorer\",\"description\":\"A side-by-side comparison of generation speed and quality using different sampling strategies (All SDE vs. PAR's Hybrid S/O/O strategy).\",\"complexity\":\"Low\",\"variables\":[\"Inference time\",\"Trajectory smoothness\",\"Final structure designability\"],\"expected_insights\":\"Shows that 'stochasticity' is vital for finding the global shape but 'determinism' is sufficient for refining details.\",\"visualization_type\":\"chart\"}],\"field\":\"Computational Biology\",\"related_fields\":[\"Machine Learning\",\"Structural Bioinformatics\",\"Generative AI\",\"Biophysics\"],\"limitations\":[\"Currently limited to C-alpha backbone atoms only, requiring separate models for all-atom reconstruction.\",\"Exposure bias is mitigated but not entirely eliminated, potentially limiting performance on extremely long proteins.\",\"Quadratic scaling of transformer attention still poses challenges for very large protein complexes.\"],\"difficulty_to_understand\":\"Advanced\",\"prerequisites\":[\"Transformers and Attention Mechanisms\",\"Flow Matching and Diffusion Models\",\"Protein Hierarchical Structure (Primary/Secondary/Tertiary)\",\"Autoregressive Learning Principles\"]}",
      "created_at": 1770295429557
    },
    "arxiv-2602.04836": {
      "hash": "arxiv-2602.04836",
      "filename": "2602.04836.pdf",
      "raw_text": "Are AI Capabilities Increasing Exponentially? A Competing Hypothesis\nHaosen Ge 1 Hamsa Bastani 2 Osbert Bastani 3\nAbstract\nRapidly increasing AI capabilities have sub-\nstantial real-world consequences, ranging from\nAI safety concerns to labor market conse-\nquences. The Model Evaluation & Threat Re-\nsearch (METR) report argues that AI capabilities\nhave exhibited exponential growth since 2019. In\nthis note, we argue that the data does not support\nexponential growth, even in shorter-term horizons.\nWhereas the METR study claims that fitting sig-\nmoid/logistic curves results in inflection points\nfar in the future, we fit a sigmoid curve to their\ncurrent data and find that the inflection point has\nalready passed. In addition, we propose a more\ncomplex model that decomposes AI capabilities\ninto base and reasoning capabilities, exhibiting in-\ndividual rates of improvement. We prove that this\nmodel supports our hypothesis that AI capabilities\nwill exhibit an inflection point in the near future.\nOur goal is not to establish a rigorous forecast of\nour own, but to highlight the fragility of existing\nforecasts of exponential growth.1\n1. Introduction\nA recent report by Model Evaluation & Threat Research\n(METR) (Kwa et al., 2025) conducted a series of analyses\nto measure AI capabilities in realistic tasks that require\nsignificant effort from human experts. They propose a novel\nmetric: 50% model horizon, which measures the difficulty\nof tasks that a model can solve successfully 50% of the time.\nThen, they show that according to this metric, AI capabilities\nare increasing exponentially over time—specifically, model\n1Wharton AI & Analytics Initiative, The Wharton School,\nUniversity of Pennsylvania, USA 2Department of Operations,\nInformation and Decisions, The Wharton School, University\nof Pennsylvania, USA 3Department of Computer and Infor-\nmation Science, University of Pennsylvania, USA. Correspon-\ndence to: Haosen Ge <hge@wharton.upenn.edu>, Hamsa Bas-\ntani <hamsab@wharton.upenn.edu>, Osbert Bastani <obas-\ntani@seas.upenn.edu>.\nPreprint. February 5, 2026.\n1Our code is available at https://github.com/\nobastani/AI_Forecasting.\nhorizons have been doubling every seven months since 2019.\nBased on these results, they predict that “within 5 years, AI\nsystems will be capable of automating many software tasks\nthat currently take humans a month.”\nSince dissemination, this report has drawn significant atten-\ntion and started heated discussion on the potential impact\nof such rapid improvement of AI capabilities. Much of the\nacademic conversation has focused on safety (Barnett &\nScher, 2025). However, there are substantial consequences\nfor rapidly increasing AI capabilities beyond safety. Most\nnotably, these results have raised substantial concerns about\nlabor market consequences, raising the potential for large-\nscale displacement of skilled workers (Brynjolfsson et al.,\n2025). Importantly, many consequences of these forecasts\nare immediate, shaping both policy outcomes as well as\nindividual decisions such as choices about education and\ncareer paths. Given the substantial consequences of the po-\ntential for exponential increase in AI capabilities, there is\nan urgent need for rigorous methodologies for performing\nand validating these kinds forecasts.\nWe argue to the contrary—plateauing growth is similarly\nwell supported by the data. While the METR study com-\npares to a small number of alternative hypotheses, aside\nfrom linear and super-exponential, none of these alterna-\ntives are visualized or discussed in detail. Most notably,\nthey discuss the sigmoid curve as a potential alternative, but\nclaim that their estimate of this model yields an inflection\npoint far in the future (see Appendix D.1 in their paper (Kwa\net al., 2025)); they rule out plateauing growth in the near\nfuture based on this finding. However, we find that fitting\na sigmoid curve results in an inflection point that is actu-\nally in the past (specifically, 2025-06-06), as visualized in\nFigure 1.2 This finding suggests that at the very least, it is\nplausible that AI capabilities may plateau soon.\nIn general, it is impossible to rule out either alternative from\ndata alone. Thus, to support our hypothesis, we posit a\ntheoretical model under which the exponential appearance\nof recent gains in AI capabilities can be interpreted as a\nconsequence of the introduction of reasoning capabilities\ninto base LLMs. Specifically, we model reasoning as a\n2While this fit is in-sample (due to the small dataset size), our\nmain goal is to estimate the inflection point from the data rather\nthan to provide accurate forecasts into the future.\n1\narXiv:2602.04836v1 [cs.AI] 4 Feb 2026\n\n-- 1 of 9 --\n\nAI Capabilities Are Not Increasing Exponentially\n(a) Sigmoid Curve Fit (b) Sigmoid Curve Fit vs METR\nFigure 1. Sigmoid Curve. This sigmoid curve is fit by minimizing the mean-squared error (MSE) of the curve hmodel = γ ·σ(δ1 ·dmodel +δ2)\nto the METR dataset, where hmodel is METR’s “50% model horizon time” for the given model, dmodel is the model release date, and\nγ, δ1, δ2 are parameters. We use gradient descent in PyTorch for parameter estimation. While it is not clear that progress will plateau,\nrecent progress clearly fits in the linear part of the sigmoid and the inflection point (2025-06-06) is in the past.\nseparate technology that contributes multiplicatively to the\noverall capability of LLMs—i.e., LLM capabilities can be\ndecomposed into two sigmoids, one for the base LLM and\none for reasoning. Indeed, as can be seen from Figure 1, the\nperiod of progress from o1-preview (released 2024-09-12)\nto the present forms the linear part of the sigmoid curve.\nThus, our hypothesis is that following initial exponential\ngrowth due to scaling data and model size, base capabilities\nplateaued, but overall capabilities continued to grow for a\nperiod due to rapid improvements in reasoning.\nWhile we present a specific alternative analysis, our goal is\nnot to discount the METR study; in fact, we believe con-\ntinuing exponential improvement is a plausible viewpoint,\nand it is important to take this potential outcome into con-\nsideration. However, we believe that our approach provides\na plausible alternative that similarly merits consideration.\n2. Background on the METR Study\nWe focus on the recent METR study (Kwa et al., 2025),\nwhich forecasts that AI capabilities are exponentially in-\ncreasing. This study was itself critiquing prior work, point-\ning out that existing metrics such as accuracy are bounded\nand cannot assess whether growth is exponential. To rem-\nedy this issue, they introduce a novel metric, the 50% model\nhorizon time, which quantifies the difficulty of tasks that a\nmodel can solve reliably. Unlike prior metrics, this one can\nincrease unboundedly, making it suitable for assessing the\npossibility of exponential growth. Their analysis concludes\nthat AI capabilities are improving exponentially.\nTheir experiments include three task families: HCAST, RE-\nBench, and SWAA. Specifically, HCAST contains a diverse\nset of challenges in cybersecurity, machine learning, soft-\nware engineering, and general reasoning. RE-Bench con-\nsists of challenging open-ended machine learning research\nengineering environments, each of which are intended to\ntake a human expert approximately 8 hours to complete.\nSWAA comprises 66 small tasks commonly performed in\nsoftware engineering work. In total, their study includes\n170 unique tasks from the three task families. Then, they\nevaluate 28 popular models on the 170 tasks. Among the\nmodels, they label a subset of 15 models as state-of-the-art\n(SOTA), representing the frontier of AI capabilities. We\nreuse their experimental results and focus exclusively on the\n15 state-of-the-art models to better characterize the scaling\nbehavior of frontier AI capabilities. We include the list of\nSOTA models and their release dates in Table 1.\nTheir data analysis first estimates the horizon time of each\nmodel on each dataset using the following regression:\npmodel = σ((log hmodel − log ttask) · βmodel), (1)\nwhere pmodel denotes the probability that the model solves a\ntask correctly, ttask denotes the difficulty of the task (mea-\nsured by the amount of time human expert takes to com-\nplete the task), hmodel is the 50% horizon time, βmodel is\nthe parameter they estimate, and σ(x) = ex/(1 + ex) is\n2\n\n-- 2 of 9 --\n\nAI Capabilities Are Not Increasing Exponentially\nTable 1. Release Dates of the Selected SOTA Models.\nModel Release Date\nDavinci-002 2020-05-28\nGPT-4 2023-03-14\nGrok-4 2025-07-09\nClaude 3.5 Sonnet (Oct 2024) 2024-10-22\nGPT-5 2025-08-07\nClaude 3.7 Sonnet 2025-02-24\nGPT-2 2019-02-14\nGPT-3.5 Turbo Instruct 2022-03-15\nGPT-o1-preview 2024-09-12\nGPT-4 (1106) 2023-11-06\nGPT-5.1 Codex Max 2025-11-19\nGPT-4o 2024-05-13\nClaude 3.5 Sonnet 2024-06-20\nGPT-o3 2025-04-16\nGPT-o1-elicited 2024-12-05\nthe sigmoid function. By construction, pmodel = 1/2 when\nlog hmodel = log ttask; thus, a model with capability hmodel\nattains a success probability of 1/2 on tasks with difficulty\nttask = hmodel. Thus, hmodel characterizes the task difficulty\nthreshold at which the model achieves a success rate of 1/2.\nAfter estimating the 50% horizon time, they examine the\ntemporal trend in model capabilities by fitting the following\nlinear regression model:\nlog hmodel = β0 + β1 · dmodel,\nwhere dmodel denotes the model’s release date; they fit this\ndata by treating the estimate hmodel from the previous step\nas ground truth h∗\nmodel and applying linear regression. Note\nthat this model is equivalent to\nhmodel = exp (β0 + β1 · dmodel) , (2)\nimplying an exponential relationship between the model’s\n50% horizon time and its release date. METR reports an\nR2 value of 0.98 for this regression, which they interpret as\nstrong evidence that the capabilities of frontier models grow\nexponentially over time. In their main paper, they compare\nthis regression to two others—linear and hyperbolic—and\nfind that an exponential curve fits the data substantially\nbetter. They discuss additional comparisons informally in\nAppendix D.1 of their paper, but do not provide quantitative\nevidence to support ruling out these alternatives. However,\nthis limited comparison to alternatives makes it difficult to\nassess the confidence we should have in their findings.\n3. Multiplicative Model of AI Progress\nWe present a model of growing AI capabilities that decom-\nposes it into two component technologies: the base model\nand reasoning. We prove that this model produces a growth\ncurve that is qualitatively consistent with the METR data.\n3.1. Motivation\nThe main hypothesis behind our model of AI progress is that\nmuch of the recent growth in AI capabilities has been driven\nby the introduction of reasoning into base LLMs. Specif-\nically, while chain-of-thought reasoning has been popular\nfor some time now (Wei et al., 2022), explicitly training\nmodels to perform reasoning is a more recent phenomenon,\nstarting with OpenAI’s o1 model (Jaech et al., 2024; Shao\net al., 2024). Since the release of o1, there has been startling\nprogress on a number of benchmarks, with current LLMs\nappearing to approach the performance of human experts.\nWe propose a model that explicitly separates progress on\nbase model capabilities from progress on reasoning capabil-\nities. Intuitively, the reason this model might show support\nfor slowing improvements is that technologies tend to ex-\nhibit very rapid growth during their introduction. Since\nLLMs have only been finetuned for chain-of-thought rea-\nsoning for a little over a year (starting with the introduction\nof OpenAI’s o1 model (Jaech et al., 2024)), it is natural that\nreasoning capabilities have dramatically improved capabili-\nties over the past year. However, we might expect reasoning\ncapabilities to start plateauing in the near future.\nWithout separating out improvements in reasoning, it\nis unsurprising that the rate of improvement appears\nexponential—prior to 2023, base capabilities improved ex-\nponentially due to scaling of data and model size, but these\nimprovements plateaued due to the prohibitive cost of scal-\ning. Reasoning capabilities have maintained this growth\nsince 2023. Under this view, one way to interpret the METR\nstudy is that it predicts new breakthroughs will continue\nto prop up exponential progress. This makes sense when\nextrapolating, since breakthroughs have been common in\nthe past decade; however, there is no guarantee that it will\ncontinue to be the case. Under our model, if breakthroughs\nstop happening, then exponential progress will end.\n3.2. Regression Model\nFor our data analysis, we consider an alternative regres-\nsion model to Eq. 2 that explicitly separates a model’s base\nperformance and its reasoning capabilities:\nhmodel = γ1 · hbase · (1 + γ2 · hreasoning)\nhbase = b(dmodel)\nhreasoning = r(dmodel) · 1{kthinking = 1},\nwhere hmodel is the 50% model horizon, kthinking indicates\nwhether the model has been post-trained with reasoning\ncapabilities and those capabilities are activated, γ1, γ2 are\nparameters to be estimated, and b, r are link functions de-\n3\n\n-- 3 of 9 --\n\nAI Capabilities Are Not Increasing Exponentially\nFigure 2. Sigmoid link inflection points. The curves are as in\nFigure 3; we show inflection points of the orange and green curves\nas dashed vertical lines of the corresponding color.\nscribed below. In other words, we treat an LLM’s over-\nall capability as the product of it’s base capability hbase(d)\n(i.e., without reasoning) and the quality hreasoning of its rea-\nsoning features. Intuitively, the base capability of a LLM\ncaptures advancement of the model’s pre-training phase\nand non-reasoning post-training phases, including model\nsizes and data curation, and the reasoning ability captures\npost-training techniques to improve chain-of-thought think-\ning. Both base and reasoning capabilities are functions of\nthe model’s release date dmodel. The parameters γ1 and γ2\nquantify the contributions of base capability and reasoning\ncapability to the overall capability.\nA key feature of our model is that technologies are multi-\nplicative—i.e., overall capability is the product (rather than,\ne.g., the sum) of the component technologies (in our case,\nthe base model and reasoning). This assumption is the key\ndriver behind the apparent exponential growth of staggered\nimprovements across different technologies; we provide\ntheoretical evidence that our model exhibits this kind of\nbehavior in Section 3.4. Intuitively, just as progress in one\ntechnology plateaus, another technology exhibits rapid im-\nprovement that props up exponential improvements. We\nbelieve this multiplicative model is realistic—for LLMs,\nreasoning cannot exist without strong base models, and\nwhile strong base models have useful capabilities, these are\nsubstantially boosted by reasoning.\n3.3. Choice of Link Functions\nThe link functions b, r encode how the base and reasoning\ncapabilities depend on the model’s release date dmodel. A\nwide range of functional forms could be considered; in this\nnote, we adopt the following plausible candidates.\nSigmoid. First, we consider the sigmoid function:\nb(dmodel) = σ(δ1 · dmodel + δ2) (3)\nr(dmodel) = σ(θ1 · dmodel + θ2), (4)\nwhere δ1, δ2, θ1, θ2 are parameters. Intuitively, these link\nfunctions say that each of base and reasoning capabilities\ngrow exponentially until reaching an “inflection point”, af-\nter which they plateau (formally, an inflection point of an\narbitrary function f (x) is the point at which f ′′(x) changes\nsign; the inflection of σ is at x = 0). In Section 3.4, we\nprovide an analysis of the implications of this model.\nExponential. Next, we consider an exponential function:\nb(dmodel) = exp(δ1 · dmodel + δ2)\nr(dmodel) = exp(θ1 · dmodel + θ2),\nwhere δ1, δ2, θ1, θ2 are parameters. Similar to METR’s orig-\ninal model, these link functions say that each of base and\nreasoning capabilities are increasing exponentially in time;\nthus, the overall capability is also increasing over time.\nSpline. Lastly, we consider a B-spline link function:\nb(dmodel) =\nNb\tX\ni=1\nδi · Bi(dmodel)\nr(dmodel) =\nNr\tX\ni=1\nθi · Bi(dmodel),\nwhere Nb and Nr denote the numbers of spline basis func-\ntions for b and r, respectively, {δi}Nb\ni=1, {θi}Nr\ni=1 are parame-\nters, and Bi(dmodel) is the ith B-spline basis function (which\nwe take to be a degree m polynomial), which includes its\nown parameters. This choice of link function induces a\nflexible, piecewise polynomial relationship between LLM\ncapabilities and the release date.\n3.4. Theoretical Analysis\nWe prove that a multiplicative model of technological\nprogress with sigmoid link exhibits exponential growth fol-\nlowed by plateauing. To simplify our analysis, we drop\nmany of the parameters and assume that the core model is a\nproduct of sigmoid functions with different inflection points;\nwe further simplify by assuming these inflection points are\nevenly spaced. Then, we prove that the resulting function\nexhibits (i) exponential growth ex before the first inflection\npoint, (ii) squared exponential growth ex2\nbetween the first\nand last inflection points, and (iii) plateauing thereafter.\n4\n\n-- 4 of 9 --\n\nAI Capabilities Are Not Increasing Exponentially\n(a) Sigmoid Link (b) Exponential Link (c) B-Spline Link\nFigure 3. Projections under Different Link Functions. The orange curves project base model capabilities, the green curve projects\nreasoning capabilities assuming the best base model (i.e., gpt-5.1-codex-max), and the blue curve shows the overall capabilities. The\nblack points denote the 50% model horizon estimated by METR.\nTheorem 3.1. Let x denote time, and consider the model\nf (x) =\nk\tY\ni=1\nfi(x) where fi(x) = σ(x − iα),\nwhere σ is the sigmoid function and α ≥ 2. Then:\n• If x ≤ 0, then\n1\n5 ekx exp\n\u0010\n− α\n2 · k(k + 1)\n\u0011\n≤ f (x) ≤ ekx exp\n\u0010\n− α\n2 · k(k + 1)\n\u0011\n.\n• If x ∈ [jα, (j + 1)α] for j ∈ {0, 1, ..., k − 1}, then\n1\n20 exp\n\u0010\n− α\n2 · (k − j + 1)(k − j)\n\u0011\n≤ f (x) ≤ exp\n\u0010\n− α\n2 · (k − j − 1)(k − j)\n\u0011\n.\n• If x ≥ kα, then\n1\n4 ≤ f (x) ≤ 1.\nIn the first case x ≤ 0, the model exhibits exponential\ngrowth ekx. In the second case, x ≈ jα ≈ (j + 1)α, so\nf (x) ≈ exp\n\u0012\n− α\n2 ·\n\u0010\nk − x\nα\n\u00112\u0013\n.\nIn other words, progress continues exponentially until the\nfinal inflection point kα. Intuitively, the exponent base de-\ncays throughout this phase; as progress for individual com-\nponents plateau, the base of the exponent becomes smaller.\nTable 2. Assessment of goodness of fit via MSE on hmodel.\nSpecification Mean Squared Error (MSE)\nSigmoid Link 203.69\nB-Spline Link 511.80\nExponential Link 2874.67\nMETR Exponential Curve 339.93\nSigmoid Curve 27.37\nFinally, in the third case, progress plateaus once the last\ninflection point has been crossed.\nThese trends are consistent with the data shown in Figure 1.\nAt a high level, the initial period of growth was exponential\ndue to scaling AI capabilities. Many researchers believed\nthat capabilities were plateauing; however, the introduction\nof post-training for reasoning sparked a second wave of\nimprovements in capabilities, resulting in a period of steep\nlinear increases between 2024-09-12 to the present. If our\nmodel accurately reflects reality, then new breakthroughs\nare necessary to sustain exponential growth.\n4. Our Analysis of the METR Data\nWe fit our model from Section 3 to data; our results support\nthe plausibility of plateauing AI capabilities.\n4.1. Methodology\nWe estimate our models the original experiment data shared\nby the METR study (Kwa et al., 2025) (including data from\nthe HCAST, RE-Bench, and SWAA benchmarks). We ac-\ncess the data via METR’s public Github repository: https:\n5\n\n-- 5 of 9 --\n\nAI Capabilities Are Not Increasing Exponentially\n(a) Long-term Projection (b) Log Model Capability\nFigure 4. Comparison of Sigmoid Link and METR Projection.\n//github.com/METR/eval-analysis-public.\nWe perform estimation by maximizing the log-likelihood\nof the probabilistic model pmodel in Eq. 1 using Stan (Car-\npenter et al., 2017). For the sigmoid and exponential link\nfunctions, we adopt the weakly informative prior N (0, 102)\nfor all parameters, and impose positivity constraints on\nγ1, γ2, δ1, θ1, βmodel. For the B-spline link function, we\nuse two breakpoints and polynomial splines of degree five\nfor both b and r (i.e., degree m = 5 and Nb = Nr =\n2 + 5 − 1 = 6). To ensure that hmodel remains strictly posi-\ntive and to avoid taking the logarithm of a negative quantity,\nwe constrain all spline coefficients δi and θi to be positive.\nFurthermore, we regularize the spline coefficients using\nrandom-walk priors, following standard practice to mitigate\noverfitting: δ1 ∼ N (0, 1), δi ∼ N (δi−1, τ ) for all i ≥ 2,\nand τ ∼ N (0, 1). The same priors are applied to θi.\n4.2. Results\nModel Fit. Figure 3 shows the fitted models; that com-\nbine base and reasoning components. Given the limited\nnumber of available models, we assess goodness-of-fit us-\ning the in-sample mean squared error (MSE) between the\nmodel-predicted AI capability and the observed AI capabil-\nity: (hmodel − h∗\nmodel)2; Table 2 reports the resulting MSEs.\nWe also include METR’s exponential curve and our sigmoid\ncurve in Figure 1; note that these models are not directly\ncomparable since they optimize different loss functions.\nAmong the three link functions considered, the sigmoid-\nlink model achieves by far the lowest MSE, suggesting that\nwithin our model, sigmoid growth appears more plausible\nthan exponential growth. Our approach also outperforms\nMETR’s exponential curve, though our model has more\nparameters so this comparison is not rigorous.\nInflection points. The key question is not the magnitude\nof recent performance gains (which are undeniable), but\nwhether these gains will continue. One argument in the\nMETR study is that there is no sign of an “inflection point”\nwhere exponential increase starts to slow down; we have\nalready shown in Figure 1 that the current data does not\nsupport this claim. To further understand inflection points\nin AI capabilities, we plot the inflection points of Eqs. (3)\n& (4) in Figure 2. The inflection points db of b(d) and dr of\nr(d) based on the estimated parameters are\nˆdb = 2024-11-21 and ˆdr = 2026-06-06,\nrespectively. These results support our finding—the inflec-\ntion point ˆdb for base model capabilities happened just after\nthe release of the first reasoning model (o1-preview) in 2024-\n09-12, and the inflection point ˆdr for reasoning capabilities\nis projected to happen in the near future. Thus, increasing\nreasoning capabilities drove the substantial overall improve-\nments we have seen in the past two years.\nBaked into our model is the idea that unless significant\nbreakthroughs happen, progress will plateau. Thus, the\nquestion about forecasting AI capabilities becomes a ques-\ntion of whether we should expect another breakthrough that\nproduces improvements on the same scale as reasoning.\nThis question can only be answered by domain knowledge;\nwe leave it for the broader community to answer.\nThese results highlight an additional benefit of decomposing\n6\n\n-- 6 of 9 --\n\nAI Capabilities Are Not Increasing Exponentially\noverall capability into component technologies—it enables\nus to understand and interpret forecasts of progress for indi-\nvidual components separately, providing an understanding\nof why progress looks a certain way.\nLong-term forecasts. Finally, we compare METR’s expo-\nnential forecast with our sigmoid-link model over a ten-year\nhorizon from 2019-01-01 to 2029-01-01. Results are shown\nin Figure 4a. The two models yield similar projections up to\napproximately 2026-07-03. Beyond this point, the METR\nmodel predicts an increasingly rapid rise in model capability,\nwhereas our model suggests that capabilities will plateau in\nthe near future. External forecasts cannot in general be vali-\ndated, so these results highlight the importance of leveraging\ndomain insights to assess the validity of forecasts.\n5. Limitations\nIn-sample evaluation. A key limitation is that our estimates\nare all evaluated in-sample; while the same is true for exist-\ning methodologies such as the METR study, our models also\ninclude more parameters. This limitation is inevitable due\nto the limited amount of data available. However, our goal\nis not to provide irrefutable evidence that AI capabilities\nare plateauing, but that it is a highly plausible alternative\nto continuing exponential growth. As more data becomes\navailable, it is critical to assess which models have more\naccurately forecast progress to build confidence in future\nforecasts. More broadly, more rigorous methodologies must\nbe developed for assessing the accuracy of these forecasts.\nEvaluation metric. A related issue is that comparing across\ndifferent kinds of models (ours vs. METR) is complicated\nby the fact that they are estimated in very different ways.\nThe METR paper performs regression to minimize the MSE\nin the space of log-outcomes log hmodel, whereas we have\nused probabilistic modeling directly on the final outcomes\npmodel; furthermore, our sigmoid curve in Figure 1 mini-\nmizes MSE on outcomes hmodel. While we have used MSE\nat predicting hmodel to compare models, this comparison is\nnot fair due to the diverse loss functions, especially given\nthe in-sample comparison. Again, this issue necessitates the\ndevelopment of more rigorous evaluation methodologies.\nMultiplicative assumption. The multiplicative assumption\nis critical for driving our theoretical analysis. While we be-\nlieve this assumption presents at least a plausible alternative,\nsubstantial work needs to be done to validate it in practice.\nLimited decomposition. We have only modeled base and\nreasoning capabilities; we believe these are the key driving\nfactors behind the substantial recent gains in AI capabilities,\nbut there are many other important factors. Prior to the\nrelease of GPT-4, many of the gains in performance were\ndriven simply by scaling the amount of training data and the\nmodel size, as well as basic instruction tuning. Recent im-\nprovements in reasoning capabilities have been driven by im-\nproved post-training procedures and the creation of datasets\ntailored to reasoning tasks; other non-public improvements\nmay also have contributed. In principle, these two com-\nponents can be further decomposed into aspects such as\ndata engineering, pre- and post-training algorithms, network\narchitecture, etc.; however, we currently lack enough in-\nformation about state-of-the-art LLMs to perform a more\ngranular analysis. Finding ways to address these issues\ncould help lead to more accurate forecasts.\n6. Conclusion\nTo contrast with the recent view on exponential increase of\nLLM capabilities, we have presented an alternative view-\npoint arguing that this increase is not exponential; instead,\nit is plateauing or possibly linear. Our forecasting method-\nology combines domain-specific modeling that decomposes\nprogress into separate base and reasoning capabilities, with\nan empirical analysis based on the METR dataset, which we\nbelieve could form the basis of future forecasting models.\nImportantly, our methodology is primarily intended to be a\ncompelling alternative rather than a definitive rebuttal. We\nbelieve that substantially more work needs to be done both\nin terms of improved forecasting methodologies, as well as\nimproved evaluation of these methodologies.\nReferences\nBarnett, P. and Scher, A. Ai governance to avoid extinction:\nThe strategic landscape and actionable research questions.\narXiv preprint arXiv:2505.04592, 2025.\nBrynjolfsson, E., Chandar, B., and Chen, R. Canaries in the\ncoal mine? six facts about the recent employment effects\nof artificial intelligence. Digital Economy, 2025.\nCarpenter, B., Gelman, A., Hoffman, M. D., Lee, D.,\nGoodrich, B., Betancourt, M., Brubaker, M., Guo, J.,\nLi, P., and Riddell, A. Stan: A probabilistic programming\nlanguage. Journal of statistical software, 76:1–32, 2017.\nHo, A., Denain, J.-S., Atanasov, D., Albanie, S., and Shah,\nR. A rosetta stone for ai benchmarks. arXiv preprint\narXiv:2512.00193, 2025.\nJaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky,\nA., Low, A., Helyar, A., Madry, A., Beutel, A., Car-\nney, A., et al. Openai o1 system card. arXiv preprint\narXiv:2412.16720, 2024.\nJimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press,\nO., and Narasimhan, K. Swe-bench: Can language mod-\nels resolve real-world github issues? arXiv preprint\narXiv:2310.06770, 2023.\n7\n\n-- 7 of 9 --\n\nAI Capabilities Are Not Increasing Exponentially\nKwa, T., West, B., Becker, J., Deng, A., Garcia, K., Hasin,\nM., Jawhar, S., Kinniment, M., Rush, N., Von Arx, S.,\net al. Measuring ai ability to complete long tasks. arXiv\npreprint arXiv:2503.14499, 2025.\nMaslej, N., Fattorini, L., Perrault, R., Gil, Y., Parli, V.,\nKariuki, N., Capstick, E., Reuel, A., Brynjolfsson, E.,\nEtchemendy, J., et al. Artificial intelligence index report\n2025. arXiv preprint arXiv:2504.07139, 2025.\nOwen, D. What will ai look like in 2030?, 2025. URL\nhttps://epoch.ai/files/AI_2030.pdf.\nPatwardhan, T., Dias, R., Proehl, E., Kim, G., Wang, M.,\nWatkins, O., Fishman, S. P., Aljubeh, M., Thacker, P.,\nFauconnet, L., et al. Gdpval: Evaluating ai model perfor-\nmance on real-world economically valuable tasks. arXiv\npreprint arXiv:2510.04374, 2025.\nShao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang,\nH., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Push-\ning the limits of mathematical reasoning in open language\nmodels. arXiv preprint arXiv:2402.03300, 2024.\nSinha, A., Arun, A., Goel, S., Staab, S., and Geiping, J.\nThe illusion of diminishing returns: Measuring long hori-\nzon execution in llms. arXiv preprint arXiv:2509.09677,\n2025.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,\nE., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting\nelicits reasoning in large language models. Advances in\nneural information processing systems, 35:24824–24837,\n2022.\nWijk, H., Lin, T., Becker, J., Jawhar, S., Parikh, N., Broadley,\nT., Chan, L., Chen, M., Clymer, J., Dhyani, J., et al. Re-\nbench: Evaluating frontier ai r&d capabilities of language\nmodel agents against human experts. arXiv preprint\narXiv:2411.15114, 2024.\nA. Related Work\nBenchmarking LLMs. A number of complex evaluation\nframeworks have been proposed to assess AI performance\nunder realistic conditions. SWE-bench evaluates LLMs on\nreal-world GitHub pull requests paired with gold-standard\nfixes and unit tests (Jimenez et al., 2023). GDPval compiles\na comprehensive set of tasks designed to be representative\nof the U.S. economy (Patwardhan et al., 2025); domain\nexperts from each representative industry are recruited to de-\nsign tasks for AI systems, while additional experts evaluate\nmodel outputs and annotate whether AI-generated solutions\nare preferred over human completions. Ho et al. (2025) pro-\npose a method for stitching together existing benchmarks\nto assess long-term AI performance. Wijk et al. (2024)\nintroduce RE-bench, which collects challenging research-\nand-development tasks hand-crafted by human experts.\nForecasting AI capabilities. Maslej et al. (2025) systemati-\ncally documents advances and trends in AI across multiple\ndomains over time, providing an empirical basis for fore-\ncasting. Epoch AI has published papers and reports offering\nin-depth analyses of AI capabilities and future development\ntrajectories (e.g., Owen (2025)). Sinha et al. (2025) distin-\nguish between “execution” and “planning,” focusing on a\nmodel’s ability to correctly execute a complex but prede-\nfined plan. They show that diminishing improvements in\nsingle-step accuracy can compound, resulting in exponential\ngrowth in the length of tasks a model can complete.\nB. Proof of Theorem 3.1\nFirst, suppose that for some j ∈ {0, 1, ..., k − 1}, we have\nx ∈ [jα, j′α], where j′ = j + 1. For all i ≤ j,\nfi(x) = 1 − e−x+iα\n1 + e−x+iα ≥ 1 − e−(j−i)α\n2 ,\nso we have\nj\tY\ni=1\nfi(x) ≥ 1 − 1\n2\nj\tX\ni=1\ne−(j−i)α ≥ 1 − 1\n2\n∞\tX\nh=0\ne−hα\n≥ 1 − 1\n2\n∞\tX\nh=0\n1\n3h ≥ 1\n4 ,\nsince e−2 ≤ 1/3. In addition, we have fi(x) ≤ 1, so\nQi\nj=1 fi(x) ≤ 1. Next, for all i ≥ j′, we have\nfi(x) = σ(x − iα) = ex−iα\n1 + ex−iα ≥ e−(i−j)α\n1 + e−(i−j′)α ,\nso\nk\tY\ni=j′\nfi(x) ≥\nQk\ni=j′ e−(i−j)α\nQk\ni=j′ (1 + e−(i−j′)α) .\nFor the numerator, we have\nk\tY\ni=j′\ne−(i−j)α =\nk−j\tY\nh=1\ne−hα = exp −α\nk−j\tX\nh=1\nh\n!\n= exp\n\u0010\n− α\n2 · (k − j + 1)(k − j)\n\u0011\n.\nFor the denominator, we have\nk\tY\ni=j′\n(1 + e−(i−j′)α) ≤\n∞\tY\nh=0\n(1 + e−hα)\n= exp\n∞\tX\nh=0\nlog(1 + e−hα)\n!\n≤ exp\n∞\tX\nh=0\ne−hα\n!\n≤ exp\n\u0012 1\n1 − e−α\n\u0013\n≤ 5.\n8\n\n-- 8 of 9 --\n\nAI Capabilities Are Not Increasing Exponentially\nIn addition, fi(x) ≤ ex−iα ≤ e−(i−j′)α, so\nk\tY\ni=j′\nfi(x) ≤ exp\n\u0010\n− α\n2 · (k − j′)(k − j)\n\u0011\n.\nPutting everything together, we have\n1\n20 exp\n\u0010\n− α\n2 · (k − j + 1)(k − j)\n\u0011\n≤ f (x) ≤ exp\n\u0010\n− α\n2 · (k − j − 1)(k − j)\n\u0011\n.\nNext, if x ≥ kα, a similar argument shows that\n1\n4 ≤ f (x) ≤ 1.\nFinally, if x ≤ 0, then we have\nk\tY\ni=1\nfi(x) ≥\nQk\ni=1 ex−iα\nQk\ni=1(1 + ex−iα) .\nFor the numerator, we have\nk\tY\ni=1\nex−iα = ekx exp −α\nk\tX\ni=1\ni\n!\n= ekx exp\n\u0010\n− α\n2 · k(k + 1)\n\u0011\n,\nand the denominator is bounded by 5 as before. In addition,\nk\tY\ni=1\nfi(x) ≤\nk\tY\ni=1\nex−iα ≤ ekx exp −α\nk\tX\ni=1\ni\n!\n= ekx exp\n\u0010\n− α\n2 · k(k + 1)\n\u0011\n.\nThus, we have\n1\n5 ekx exp\n\u0010\n− α\n2 · k(k + 1)\n\u0011\n≤ f (x) ≤ ekx exp\n\u0010\n− α\n2 · k(k + 1)\n\u0011\n.\nThe claim follows.\n9\n\n-- 9 of 9 --\n\n",
      "analysis_json": "{\"title\":\"Are AI Capabilities Increasing Exponentially? A Competing Hypothesis\",\"authors\":[\"Haosen Ge\",\"Hamsa Bastani\",\"Osbert Bastani\"],\"publication_year\":\"2026\",\"summary\":\"This paper challenges the widely cited METR report which claims AI capabilities are growing exponentially. The authors argue that a sigmoid (plateauing) model fits existing data better and that recent progress is better explained by a 'Multiplicative Model' where distinct technology waves, specifically base capabilities and reasoning capabilities, stack on top of each other. Their analysis suggests that the inflection point for base models has already passed, and reasoning-driven gains may also plateau in the near future.\",\"breakthrough_score\":72,\"breakthrough_reasoning\":\"The paper provides a significant mathematical critique of mainstream AI forecasting. By introducing a multiplicative decomposition of technology components (Base + Reasoning), it shifts the conversation from simple curve-fitting to a more nuanced 'technology wave' theory. While it relies on the same limited dataset as the original METR report, its rigor in highlighting the fragility of exponential projections is a major contribution to AI governance and policy planning.\",\"key_claims\":[\"Current AI capability data does not uniquely support an exponential growth hypothesis.\",\"A sigmoid curve suggests the inflection point for base AI capabilities occurred around June 2025.\",\"AI progress is better modeled as the product of independent technological components (Base x Reasoning).\",\"The 'exponential' look of recent progress is an artifact of models being in the steep linear phase of a new reasoning technology sigmoid.\"],\"testable_hypotheses\":[{\"hypothesis\":\"AI capability gains on 50% model horizon tasks will show significant deceleration by mid-2026.\",\"how_to_test\":\"Benchmark frontier models released in 2026 (e.g., successors to GPT-5 or Claude 4) on RE-Bench and SWAA benchmarks to see if performance stays within sigmoid bounds.\",\"expected_outcome\":\"The observed 50% model horizon will deviate significantly below the exponential forecast and align with the plateauing sigmoid model.\"}],\"key_equations\":[{\"name\":\"Multiplicative Capability Model\",\"latex\":\"h_model = gamma_1 * h_base(d_model) * (1 + gamma_2 * h_reasoning(d_model))\",\"description\":\"Models the total AI capability as a product of base pre-training strength and reasoning-specific post-training enhancements.\",\"variables\":[{\"name\":\"h_base\",\"description\":\"The base capability level of the model without advanced reasoning.\",\"typical_range\":\"0 to 1 (normalized)\"},{\"name\":\"h_reasoning\",\"description\":\"The enhancement factor provided by reasoning-specific training (e.g., chain-of-thought).\",\"typical_range\":\"0 to 1\"}]},{\"name\":\"Sigmoid Link Function\",\"latex\":\"b(d_model) = 1 / (1 + exp(-(delta_1 * d_model + delta_2)))\",\"description\":\"Defines how a specific technology component (like base capability) evolves over time, following an S-curve.\",\"variables\":[{\"name\":\"delta_1\",\"description\":\"The growth rate or steepness of the capability increase.\",\"typical_range\":\"0.1 to 2.0\"},{\"name\":\"delta_2\",\"description\":\"The temporal offset determining when the inflection point occurs.\",\"typical_range\":\"Variable based on date encoding\"}]}],\"simulation_possibilities\":[{\"title\":\"The 'Stacked Sigmoids' Illusion Generator\",\"description\":\"An interactive dashboard where users can create 'technology waves' (e.g., Scaling, Reasoning, Agentic Behavior). By adjusting the start date and growth rate of each wave, users can see how a series of plateauing curves (sigmoids) can perfectly mimic a single exponential curve over a short timeframe.\",\"complexity\":\"Medium\",\"variables\":[\"Growth rate per wave\",\"Wave onset date\",\"Number of technology waves\",\"Saturation level\"],\"expected_insights\":\"Users will understand the 'multiplicative' argument by seeing how 'exponential' progress is often just the sum of several overlapping S-curves.\",\"visualization_type\":\"interactive\"},{\"title\":\"Model Horizon Predictor\",\"description\":\"A visualization tool that plots real model release data against two competing forecasts: Exponential vs. Sigmoid Link. Users can input a future model release date and see the disparity in task-solving capability predictions between the two theories.\",\"complexity\":\"Low\",\"variables\":[\"Release Date\",\"Model Type (Base vs Reasoning-Enabled)\"],\"expected_insights\":\"Demonstrates the 'fragility' of long-term forecasts and how quickly the two models diverge after 2026.\",\"visualization_type\":\"chart\"}],\"field\":\"AI Forecasting\",\"related_fields\":[\"Machine Learning\",\"Econometrics\",\"AI Safety\",\"Technological Forecasting\"],\"limitations\":[\"Small dataset (only 28 models evaluated).\",\"In-sample fitting makes the results descriptive rather than definitively predictive.\",\"Simplistic decomposition into only two components (Base and Reasoning).\",\"Lack of transparency from AI labs on exact release dates and post-training methods.\"],\"difficulty_to_understand\":\"Intermediate\",\"prerequisites\":[\"Basic knowledge of Sigmoid functions and growth curves\",\"Familiarity with the distinction between LLM pre-training and reasoning-based post-training\",\"Understanding of Mean Squared Error (MSE) in regression\"]}",
      "created_at": 1770298565647
    },
    "arxiv-2602.04813": {
      "hash": "arxiv-2602.04813",
      "filename": "reanalysis.pdf",
      "raw_text": "Received 10 December 2025, accepted 31 December 2025, date of publication 5 January 2026, date of current version 9 January 2026.\nDigital Object Identifier 10.1109/ACCESS.2026.3651218\nAgentic AI in Healthcare & Medicine: A\nSeven-Dimensional Taxonomy for\nEmpirical Evaluation of LLM-based\nAgents\nSHUBHAM VATSAL1, HARSH DUBEY2 AND ADITI SINGH3 (Senior Member, IEEE)\n1 Department of Computer Science, New York University, CIMS, New York, USA (e-mail: sv2128@nyu.edu)\n2 Department of Computer Science, New York University, CIMS, New York, USA (e-mail: hd2225@nyu.edu)\n3 Department of Computer Science, Cleveland State University, Cleveland, USA (e-mail: a.singh22@csuohio.edu)\nCorresponding author: Aditi Singh (e-mail: a.singh22@csuohio.edu).\nABSTRACT Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape\nhealthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR\nanalysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely\nconsists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory,\nplanning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49\nstudies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction\nPatterns, Adaptation & Learning, Safety & Ethics, Framework Typology and Core Tasks & Subtasks with\n29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully\nImplemented ✓, Partially Implemented ∆, Not Implemented ✗), we map each study to the taxonomy\nand report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical\nanalysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension\nunder Knowledge Management is commonly realized (∼76% ✓) whereas Event-Triggered Activation\nsub-dimenison under Interaction Patterns is largely absent (∼92% ✗) and Drift Detection & Mitigation\nsub-dimension under Adaptation & Learning is rare (∼98% ✗). Architecturally, Multi-Agent Design sub-\ndimension under Framework Typology is the dominant pattern (∼82% ✓) while orchestration layers remain\nmostly partial. Across Core Tasks & Subtasks, information centric capabilities lead e.g., Medical Question\nAnswering & Decision Support and Benchmarking & Simulation, while action and discovery oriented areas\nsuch as Treatment Planning & Prescription still show substantial gaps (∼59% ✗). Together, these findings\nprovide an empirical baseline indicating that current agents excel at retrieval-grounded advising but require\nstronger adaptation and compliance platforms to move from early-stage systems to dependable systems.\nINDEX TERMS Agentic AI, Healthcare, Medicine, Prompt Engineering, Large Language Model, Multi-\nAgent, Taxonomy, Survey, Empirical Analysis, Clinical Trial, Patient Interaction, Diagnostic Reasoning\nI. INTRODUCTION\nThe advent of LLMs has significantly accelerated advance-\nments across multiple domains, showcasing transformative\npotential far beyond traditional applications. These models,\nincluding GPT-4 [1] and PaLM [2] are trained on extensive\ndatasets comprising billions or even trillions of parameters\nenabling broad generalization and sophisticated understand-\ning. Empirical studies have showcased that increasing model\nscale directly correlates with enhanced performance in com-\nplex reasoning, multi-step decision making and interactive\nplanning tasks [3]. The enhanced cognitive capabilities of\nmodern LLMs have rapidly driven their integration into crit-\nical sectors such as healthcare [4], medicine [5], finance [6]\nfor risk assessment and investment strategies and education\n[7] for personalized learning and instructional assistance. Re-\ncent research has further evolved to emphasize prompt engi-\nneering which is a technique involving precise manipulation\nof natural language inputs to extract and enhance task specific\nVOLUME 14, 2026 1\narXiv:2602.04813v1 [cs.AI] 4 Feb 2026\n\n-- 1 of 25 --\n\nreasoning capabilities of LLMs [8]–[12]. This shift under-\nscores a critical progression in LLM research highlighting the\ntransition from predictive modeling to sophisticated, context\nsensitive cognitive interactions. Furthermore, the emergence\nof LLM-based agents has marked another significant mile-\nstone enabling autonomous decision making and interactive\ntask execution across diverse application areas [13].\nIn healthcare and medicine, LLMs have demonstrated\nconsiderable promise in streamlining clinical workflows, en-\nhancing diagnostic accuracy and supporting clinical research.\nFor instance, recent studies have utilized LLMs to auto-\nmate clinical documentation, significantly reducing clinician\nworkload and improving the quality of clinical records [14],\n[15]. Similarly, LLM-based decision support systems have\nbeen applied to medical question answering, achieving expert\nlevel performance in interpreting complex medical literature\nand clinical guidelines [16]. Complementing these advances,\nLLMs achieve state-of-the-art results on biomedical machine\nreading comprehension benchmarks [17] and show measur-\nable gains on guideline-based prior-authorization question\nanswering over noisy, real-world records [18]. The inte-\ngration of multimodal capabilities into LLMs enabling the\ninterpretation of radiology scans and clinical images has\nfurther advanced diagnostic efficiency as has been seen in\nframeworks like ChatCAD [19]. Apart from diagnostics,\nLLMs have been explored for enhancing treatment planning\nand supporting evidence-based medicine by synthesizing vast\ncorpora of medical research and clinical guidelines [20]. In\ndrug discovery, LLMs accelerate the identification of novel\ndrug candidates and streamline clinical trial design which\nhas significantly helped in reducing both time and cost\n[21]. Patient interaction and clinical communication have\nalso benefited from LLM-powered conversational agents\nthereby improving adherence to treatment plans, facilitating\ntelemedicine and enabling remote patient monitoring [22].\nAlthough these advancements demonstrate substantial im-\npact, they also highlight the need for continuous evaluation of\nLLMs in healthcare to maintain high standards of reliability\nand interpretability.\nRecently, the development of LLM-based agents which\nare often described under the broader umbrella of Agentic\nAI has represented an emerging paradigm that capitalizes on\nthe advanced cognitive capabilities of LLMs. These agents\nintegrate multiple cognitive modules including reasoning,\nplanning and memory management allowing them to do\nsophisticated problem solving and context aware interactions\nacross a variety of domains. Recent research such as Chain-\nof-Thought prompting [3] and Tree-of-Thoughts reasoning\n[23] has highlighted the importance of modular cognitive\narchitectures which help in facilitating the effective execution\nof complex multi-step reasoning tasks within Agentic AI\ndesign space. Additionally, memory-augmented architectures\nthat incorporate episodic and semantic memory systems have\nfurther enhanced agents’ capacities to maintain continuity\nacross interactions improving both task accuracy and reliabil-\nity. Adaptive learning mechanisms including reinforcement\nlearning with human feedback (RLHF) enable these agents to\niteratively refine their behavior by integrating user input and\nenvironmental signals resulting in dynamic adaptation and\nalignment with evolving user objectives. As the landscape of\nLLM-based agents evolves, their impact in decision making\nis evident across healthcare, finance, law and education.\nThis evolution is reshaping industry practices enabling more\nintelligent, adaptive, context aware systems that redefine how\ntasks are automated and decisions made.\nBuilding on these foundational advancements, LLM-based\nagents have emerged as a pivotal innovation within healthcare\nand medicine. These agents integrate advanced reasoning\nand planning which help in facilitating complex tasks such\nas differential diagnosis and real-time patient monitoring.\nResearch has shown their utility in automating critical work-\nflows including patient triage [24], laboratory result interpre-\ntation and EHR management [25]. As Agentic AI systems,\nLLM-based agents show promise in augmenting clinical\ndecision making through interactive dialogues that foster\ncollaboration between medical professionals and AI systems.\nThese agents also facilitate continuous medical education by\nproviding real-time access to updated clinical knowledge,\nbest practices and evidence-based guidelines. In specialized\nfields such as oncology, cardiology and surgery, LLM-based\nagents are being employed to support complex risk assess-\nments and treatment optimization [26], [27]. As their inte-\ngration deepens across healthcare, Agentic AI-driven LLM-\nbased agents can assist in not only streamlining decision\nsupport but also enable more holistic care pathways.\nA. FOCUS OF OUR SURVEY\nThe growing interest in LLM-based agents has prompted\nexploration of potential applications in healthcare and\nmedicine. Yet, the field lacks a structured framework for\nevaluating their multifaceted capabilities and system designs.\nExisting studies often examine specific applications or agent\narchitectures in isolation, limiting a thorough understanding\nof their operational scope and performance dimensions. In\nthis survey, we address this gap by conducting an exten-\nsive evaluation of 49 research papers on LLM-based agents\ndeployed within healthcare and medicine. We propose an\nintegrated taxonomy spanning 7 core dimensions and 29 sub-\ndimensions, systematically characterizing these agents across\nCognitive Capabilities, Knowledge Management, Interaction\nPatterns, Adaptation & Learning, Safety & Ethics, Frame-\nwork Typology and Core Tasks & Subtasks. Our survey not\nonly defines and contextualizes each dimension but also maps\nthe selected studies against this taxonomy enabling a large\nscale quantitative analysis of prevailing methodologies. To\naid synthesis, we include a taxonomy diagram that visualizes\nhow the 49 studies map across the 7 dimensions and 29 sub-\ndimensions. We also derive broader insights that illuminate\nhow different dimensions and sub-dimensions manifest in\npractice. These insights help us in emphasizing on diverse\nagent capabilities, domain specific methodologies and vary-\ning implementation approaches in healthcare and medicine.\n2 VOLUME 14, 2026\n\n-- 2 of 25 --\n\nFIGURE 1. Timeline Diagram of All the Papers Evaluated in Our Work. Refer to Table 1 for Mapping Between Paper Names and Corresponding Citations\nThis extensive analysis provides a structured basis for de-\nveloping robust, reliable and ethical Agentic AI-driven LLM\nagents in healthcare.\nWhile several existing surveys [28]–[33] offer broad\noverviews of LLM-based agents, they tend to emphasize gen-\neral purpose capabilities without delving deeply into health-\ncare and medicine specific applications. [34] introduces per-\ntinent discussions around LLM-based agents in medicine but\noffers a limited exploration, covering roughly 25 papers.\nIn contrast, our survey expands the scope significantly by\nanalyzing 49 studies to capture a wider range of trends\nand advancements within healthcare and medicine. Similarly,\nsurveys such as [35]–[39] examine select capabilities like\nmemory, planning and reasoning in isolation. However, these\nfragmented perspectives fall short of providing a unified,\nmultidimensional framework. Our work addresses this gap by\ndelivering a comprehensive evaluation across 7 core dimen-\nsions and 29 sub-dimensions. This evaluation offers an inte-\ngrated understanding that aligns with the unique challenges\nof healthcare and medicine.\nThroughout this survey, we focus on a specific slice of\nAgentic AI: systems built around an LLM that drives the\nagent’s planning and decision-making, and can use tools,\nmaintain memory or take actions. We do not cover agents\nthat rely purely on symbolic reasoning, hand-crafted rules or\narchitectures where the LLM is not the primary controller.\nHybrid or multimodal setups are included only when the\nLLM is clearly the component coordinating the agent’s be-\nhavior.\nB. LITERATURE SEARCH PROCESS\nWe adopted a structured three-step filtering process to curate\nthe research papers reviewed in this survey.\nStep 1: We used multiple resources such as Google\nScholar, PubMed, DBLP, Scopus and ArXiv to conduct\na comprehensive search. Some of the search queries we\nused included LLM-based Agents in Healthcare, LLM-based\nAgents in Medicine, Using LLM-based Agents for Health-\ncare Tasks, LLM-based Agents for Electronic Health Record\nManagement, LLM-based Agents for Drug Discovery, LLM-\nbased Agents for Clinical Documentation, Using LLM-based\nAgents for Clinical Summarization and LLM-based Agents in\nthe Medical Domain. This helped us gather an initial pool of\nresearch articles focused on LLM-based agents in healthcare\nand medicine. The initial search resulted in 137 articles after\npreliminary manual filtering to exclude irrelevant works. The\npreliminary manual screening removed papers for reasons\nsuch as: (i) duplicate postings of the same work across ArXiv\nand conference proceedings (ii) short workshop abstracts\nor extended abstracts lacking an implemented system (iii)\npapers whose contributions centered solely on dataset cre-\nation rather than agent behavior (iv) incomplete manuscripts\nincluding posters, project pages or early-stage preprints with-\nout technical detail. A group of 3 reviewers independently\nscreened the initial pool of papers. Each paper was reviewed\nby at least 2 reviewers and disagreements were resolved\nin joint meetings through consensus. This process helped\nmaintain consistency and minimize individual reviewer bias\nduring inclusion/exclusion decisions.\nStep 2: We applied two stringent selection criteria to\nfurther narrow the corpus. First, we included only studies in\nwhich an LLM is the core component of the agent’s design\naligning with our focus on LLM-based agents. Second, we\nretained papers that explicitly address tasks in healthcare\nor medicine, while excluding studies primarily rooted in\nother domains. Because healthcare and medicine sometimes\nintersect with closely related fields such as biomedicine, we\nevaluated such overlaps case by case. If the core contribution\nof the paper was predominantly aligned with healthcare or\nmedicine even in the presence of interdisciplinary elements,\nwe included it in our survey.\nStep 3: We then limited the corpus to articles published\nbetween October 2023 and June 2025. Figure 1 shows the\ntimeline of all included papers. The combination of Steps 2\nVOLUME 14, 2026 3\n\n-- 3 of 25 --\n\nand 3 output gave us a curated set of 49 research papers that\nunderpin our evaluation framework.\nTo summarize the full pipeline, we also include a\nPRISMA-style flow diagram in Figure 2 outlining identifi-\ncation, screening, eligibility assessment and final inclusion\nof the 49 studies in this survey.\nC. OUTLINE\nThe remainder of the paper is structured as follows. Section\n2 introduces the 7 core dimensions and 29 sub-dimensions\nthat form the basis of our evaluation framework, providing\ndetailed definitions and motivations for each. In Section 3, we\npresent key insights derived from our analysis, including pat-\nterns such as the most frequently employed sub-dimensions\nin healthcare and medicine, methodological trends and dom-\ninant architectural choices. Finally, Section 4 concludes the\npaper by summarizing our contributions.\nII. EVALUATION DIMENSIONS\nThis section introduces the 7 evaluation dimensions and 29\nsub-dimensions that structure our analysis of 49 studies.\nThese dimensions are derived through a mixed method-\nology combining inductive extraction, structured synthesis\nand cross-referencing with prior frameworks. We began by\nperforming open coding on all 49 papers, identifying recur-\nring mechanisms related to planning, memory, interaction,\nlearning, safety and system structure. These codes are then\niteratively grouped into higher-level constructs through con-\nstant comparison until stable themes emerged. In parallel, we\nconsulted existing surveys on LLM-based agents, agentic ar-\nchitectures and healthcare-AI evaluation to ensure conceptual\ncontinuity while avoiding direct replication. Dimensions are\nretained only when: (i) they appeared in several independent\npapers (ii) they captured operational behaviors that could\nbe scored consistently (iii) they are not already subsumed\nby a broader construct. Sub-dimensions are defined a priori\nonly when the literature already used a well-established term\n(e.g., planning, knowledge integration). All remaining sub-\ndimensions are inductively extracted from empirical imple-\nmentation patterns in the 49-paper corpus. This combined\nprocess ensures that the taxonomy is both grounded in lit-\nerature and empirically reproducible.\nWe emphasize that these definitions are practical and de-\nsigned for consistent scoring across papers. They are not\nmeant to be exhaustive. Healthcare LLM agents are evolving\nquickly and accordingly scholars may draw these boundaries\ndifferently. Our aim is operational clarity and cross-study\ncomparability and not completeness. Each dimension admits\nmultiple valid readings and some capabilities can appear\nunder more than one heading depending on the system design\nand reported evidence. For the same reason, the categorical\nlabels we apply Fully Implemented ✓, Partially Implemented\n∆ and Not Implemented ✗ should be read as decision aids\nrather than hard thresholds. They are not strictly mutually\nexclusive. Systems often straddle boundaries and overlap can\nreasonably occur between ✓ and ∆ or between ∆ and ✗ de-\nFIGURE 2. PRISMA Flow Diagram of the Study Selection Process\npending on deployment context and evidence granularity. To\nhandle these gray areas consistently, we favor conservative\nassignment grounded in verifiable descriptions. We default to\n∆ where claims are implicit which means they are based only\non ablations or demonstrated solely in simulation. We use ✗\nwhere a mechanism is asserted without concrete procedure or\nevaluation. We assign ✓ where the capability is implemented\nend-to-end with explicit procedures and demonstrations. De-\nspite the inherent interpretive space, our goal is compara-\nbility. We therefore standardize terminology across papers,\napply common decision rules within each sub-dimension\nand record concise rationales explaining how evidence maps\nto labels. This yields a uniform reading of heterogeneous\ndesigns spanning clinical tasks and data modalities while\nleaving room for nuance and evolution. The taxonomy and\nlabels in this study should be treated as a transparent baseline\nfor cross-study synthesis and not as prescriptive judgments.\nSubsequent sections detail the rubrics we used so that others\ncan reproduce, refine or contest specific assignments as the\nfield matures.\nThe taxonomy diagram in Figure 3 and 4 visualizes our\nsurvey’s structure. In this diagram, the 7 evaluation dimen-\nsions are grouped with their 29 sub-dimensions. For each\nsub-dimension, we list every study rated Fully Implemented\n✓ under our rubric. This view acts as a navigational index.\nIt highlights clusters of maturity (sub-dimensions with many\n✓ papers), brings our attention to sparse areas where imple-\nmentations are rare and surfaces studies with multi-capability\ndesigns. This diagram clarifies overlap among neighboring\nconstructs and complements the tabular summaries.\nA. COGNITIVE CAPABILITIES\nAgents must first translate clinical goals into workable\ncourses of action, breaking long horizon objectives into\nmilestones and decision points. Contemporary prompting\nstrategies like Tree-of-Thoughts [23] and Plan-and-Solve\n4 VOLUME 14, 2026\n\n-- 4 of 25 --\n\nFIGURE 3. Taxonomy Diagram of Dimensions Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation & Learning, Safety & Ethics,\nFramework Typology and their Corresponding Sub-Dimensions with Research Papers Rated Fully Implemented. Refer to Table 1 for Mapping Between Paper\nNames and Corresponding Citations.\n[40] formalize this decomposition and search. In healthcare,\nsuch structured lookahead supports care pathway drafting,\ntriage routing and prior-authorization checks under real-\nworld constraints. These plans depend on semantic intake\nfrom heterogeneous inputs including notes, labs, images,\ndevice streams and guidelines. Multimodal pretraining shows\nhow cross-modal alignment enables robust grounding [41]\nwhile domain tuned language models such as Med-PaLM 2\n[42] capture medical nomenclature and discourse. By con-\nverting raw EHR and imaging signals into clinically mean-\ningful representations, agents can anchor plans to patient\nspecific context rather than surface strings. Agents interleave\nVOLUME 14, 2026 5\n\n-- 5 of 25 --\n\nFIGURE 4. Taxonomy Diagram of Dimension Core Tasks & Subtasks and it’s Corresponding Sub-Dimensions with Research Papers Rated Fully Implemented.\nRefer to Table 1 for Mapping Between Paper Names and Corresponding Citations.\nreasoning with tool calls as exemplified in ReAct [43] and\nlearn when and how to invoke APIs as shown in Toolformer\n[44]. Because clinical stakes demand vigilance, agents bene-\nfit from self-monitoring that critiques intermediate products.\nReflective control loops maintain episodic traces to improve\nfuture iterations [45] while diversity aware decoding aggre-\ngates multiple rationales to stabilize conclusions [46]. These\nmechanisms surface deferral conditions, steer escalation to\nhumans and promote conservative behavior when evidence\nis weak. Finally, reliable clinical decisions require conver-\ngence across competing hypotheses and sources. Multi-agent\ndeliberation frameworks coordinate debate and mediation\ntoward consensus [47] whereas sampling-based cross-checks\nexpose factual divergence indicative of hallucination [48].\nSuch protocols reconcile guideline recommendations with\npatient level data, reduce contradictory orders and justify\nsuggestions with traceable agreement. Collectively, these ca-\npabilities constitute the Cognitive Capabilities of LLM-based\nagents. Table 1 reports evaluations across the 5 Cognitive\nCapabilities sub-dimensions:\n1) Planning\nPlanning is the agent’s capacity to synthesize, maintain and\nrevise executable task structures that link objectives to ac-\ntions. It entails principled task decomposition, temporally co-\nherent sequencing and hierarchical organization that distin-\nguishes strategic intent from tactical steps. Robust planning\nagents also enumerate alternative strategies, select among\nthem and run critique-refine loops to repair omissions or dead\nends. Within this survey, we judge Planning ✓ when there\nis clear evidence of dynamic plan construction with multi-\nstep decomposition, explicit comparison of alternatives or\nrefinement cycles and appropriate tool selection. We judge\nit ∆ when there is task breakdown or dynamic role as-\nsignment but no substantive strategy generation, comparison\nor iterative revision. It is ✗ when workflows are fixed or\nlimited to simple conditional branching. In clinical contexts,\nthe implemented form supports adaptive diagnostic workups,\nmulti-step prior-authorization reviews and iterative treatment\nplanning aligned to evolving patient data.\n2) Perception (Input Processing)\nPerception is the transformation of heterogeneous inputs into\nreliable, model usable representations through normaliza-\ntion, denoising and cross-modal fusion. Beyond tokenization,\nit encompasses various other things like ASR for speech,\ntemporal alignment for waveforms, structure induction for\nEHR fields etc. This is followed by construction of prove-\nnance sensitive embeddings suitable for downstream reason-\ning and action. For evaluation, ✓ denotes robust pipelines\nthat integrate multiple modalities via dedicated encoders and\ncross-modal fusion, exposing calibrated representations to\nthe agent’s planning and action components. ∆ describes\nsystems that perform competent text parsing or per-modality\nencoding with minimal fusion and limited preprocessing.\n✗ applies when inputs are passed as raw strings with no\nnormalization or multimodal handling.\n3) Action (Output & Execution)\nAction (Output & Execution) is the agent’s execution layer\nthat converts internal decisions into externally verifiable ef-\nfects. It encompasses controlled text production along with\n6 VOLUME 14, 2026\n\n-- 6 of 25 --\n\ntool and API invocation. Mature executors validate precon-\nditions and postconditions, manage side effects and route\nobservations back to the controller for closed loop operation.\nWe label this sub-dimension ✓ when the system demon-\nstrates actual tool use and external execution beyond text\ngeneration. It is ∆ when actions are limited to basic API\ncalls or database lookups without richer tool orchestration\nor environment manipulation. It is ✗ when outputs are text\nonly. In healthcare settings, the ✓ level enables safe medica-\ntion reconciliation, order validation and prior-authorization\nchecks with auditable traces.\n4) Meta-Capabilities\nMeta-Capabilities comprise introspective control over an\nagent’s own reasoning and actions. It comprises of generating\ncritiques of intermediate steps, calibrating confidence, iden-\ntifying knowledge gaps and deciding when to abstain or seek\nevidence. In clinical contexts, this layer enables agents to\nsurface uncertainty in differential diagnoses, trigger literature\nretrieval or second opinions and defer to clinicians when\nevidence is insufficient. For evaluation, ✓ indicates explicit\ncritique and revision loops, calibrated uncertainty estimates\ntied to deferral and logged escalation. ∆ reflects ad hoc self-\nmonitoring like memory feedback or heuristic self-checks.\n✗ indicates absence of self-critique, confidence signaling or\ndeferral behavior.\n5) Consistency & Conflict Resolution\nConsistency & Conflict Resolution in agents refers to the\nsystematic identification and reconciliation of contradictory\ninformation originating from parametric recall, external re-\ntrieval or multi-agent outputs. Effective implementations\ndeploy layered strategies including contradiction detection\nthrough natural language inference models and ensemble\nagreement protocols. In complex domains like healthcare,\nsuch mechanisms are vital for ensuring that recommen-\ndations remain coherent, evidence-based and aligned with\nestablished clinical standards. The extent of realization can\nvary: (a) Fully Implemented ✓ systems incorporate auto-\nmated conflict detection with structured resolution pipelines\nusing majority vote ensembling and adaptive re-weighting of\nspecialist outputs (b) Partially Implemented ∆ systems may\nrely on limited validation or ad hoc human oversight without\ncomprehensive reconciliation (c) Not Implemented ✗ omit\nexplicit mechanisms altogether, increasing the risk of unsafe\noutputs.\nB. KNOWLEDGE MANAGEMENT\nKnowledge Management in LLM-based agents centers on\nhow parametric knowledge is complemented and routed\nthrough external resources when the model’s internal recall\nis insufficient. Agents arbitrate between weights and the\nworld by invoking retrieval-augmented generation (RAG)\n[95], pretraining time retrieval such as REALM [96] and\nby interpolating with non-parametric memories as in kNN-\nLM [97]. In clinical settings, this blend helps surface up-to-\nTABLE 1. Evaluation of Cognitive Capabilities\nPapers Plan Perception Action Meta\nCapa-\nbilities\nConsistency\n& Conflict\nResolution\nColacare [25] ✗ ∆ ✗ ✓ ✓\nMedagent [49] ✗ ∆ ✗ ✓ ✓\nMedaide [50] ∆ ∆ ✗ ✗ ✗\nDoctoragent [51] ∆ ∆ ✗ ✗ ✗\nAihospital [52] ✗ ∆ ✗ ✗ ✓\nMmedagent [53] ∆ ✓ ✓ ✗ ✗\nMedchat [54] ∆ ✓ ✗ ✗ ✓\nMedco [55] ✗ ✓ ✓ ∆ ✗\nMedagentpro [56] ✓ ✓ ✓ ∆ ✗\nClinicalagent [57] ✓ ∆ ✓ ✗ ✗\nKtas [58] ✗ ∆ ✓ ✗ ✗\nAgentmd [59] ✗ ∆ ✓ ✗ ✗\nMdagent [60] ∆ ✓ ✗ ✓ ✓\nAgenthospital [61] ✗ ∆ ✗ ✓ ✗\nPharmagent [62] ✗ ✓ ✓ ✓ ✗\nTxagent [63] ✓ ∆ ✓ ✓ ✗\nAutoct [64] ✓ ∆ ✓ ✓ ✗\nKg4diagnosis [65] ∆ ∆ ✗ ✓ ✗\nMdteamgpt [66] ∆ ∆ ✗ ✓ ✓\nMitigating [67] ✗ ∆ ✗ ∆ ∆\nPolaris [68] ∆ ✓ ✗ ∆ ✓\nRxstrategist [69] ✗ ✓ ✗ ✗ ✗\nClinicallab [70] ∆ ✓ ✗ ✗ ✗\nAgenticllm [71] ✗ ∆ ✗ ✓ ✗\nSurgbox [72] ∆ ∆ ✗ ∆ ✗\nAipatient [73] ✗ ∆ ✗ ∆ ∆\nHealthagent [74] ✓ ✓ ✓ ✗ ✗\nIcdcoding [75] ✗ ∆ ✗ ✗ ✓\nAha [76] ✗ ∆ ✗ ✗ ✓\nDdo [77] ∆ ∆ ✗ ✗ ✗\nBeyond [78] ✗ ∆ ✗ ✗ ✓\nZodiac [26] ✗ ✓ ∆ ✗ ∆\nFhirviz [79] ✗ ∆ ∆ ✗ ✗\nFhir [80] ∆ ∆ ∆ ✗ ✗\nFuasagent [81] ✓ ✓ ✓ ✓ ✗\nAitherapist [82] ∆ ∆ ✗ ✗ ✗\nRadiology [83] ✗ ✓ ✗ ✓ ✓\nTriageagent [24] ✗ ∆ ✗ ✗ ✓\nPiors [84] ∆ ∆ ∆ ✓ ✗\nSelfevolve [85] ✗ ✓ ✗ ✓ ✓\nDrugagent [86] ✓ ∆ ✓ ✗ ✗\nCtagent [87] ✓ ✓ ✓ ✗ ✗\nAdagent [88] ∆ ✓ ✓ ✗ ∆\nDrugpilot [89] ∆ ✓ ✓ ✓ ✗\nLlmadmin [90] ✓ ✓ ✓ ✗ ✗\nSwarm [91] ✗ ✓ ✓ ✓ ✗\nDrugdiscover [92] ✗ ✓ ✓ ∆ ✗\nRagdrug [93] ✓ ✓ ✓ ✗ ∆\nDrugreason [94] ∆ ✓ ✓ ✗ ✓\ndate guidelines and trial evidence with citations for clinician\nreview. Equally important is the agent’s internal memory\narchitecture. Short-term buffers maintain discourse and tool\nuse state. On the other hand, long-term episodic and semantic\nstores compress prior interactions and domain ontologies\nunder learned controllers that page information into and out\nof context as in MemGPT [98]. These mechanisms reduce\ncontext length bottlenecks, stabilize multi-step plans and\nenable retrieval policies that are sensitive to patient spe-\ncific history. In healthcare, such memory supports longitu-\ndinal summarization, medication reconciliation and cross-\nVOLUME 14, 2026 7\n\n-- 7 of 25 --\n\nencounter continuity where sustained context improves care\nquality. Finally, knowledge must evolve and accordingly\nagents need to continuously index new documents, prioritize\nfresh sources and de-emphasize stale entries. Self-reflective\nretrieval frameworks that critique and re-query [99] can im-\nprove factuality and citation reliability while targeted para-\nmetric edits can correct entrenched facts at scale [100]. In\nclinical deployments, such update and forget mechanisms are\nvital for aligning agent outputs with evolving guidelines and\nnewly published trials. Table 2 summarizes outcomes across\nits 3 sub-dimensions:\n1) External Knowledge Integration\nExternal Knowledge Integration refers to an agent’s capabil-\nity to augment its intrinsic parametric knowledge encoded in\nmodel weights with non-parametric information. This non-\nparametric information is retrieved from external, often do-\nmain specific repositories at inference time. Such repositories\nmay include structured clinical databases, medical knowl-\nedge graphs, regulatory guideline repositories or unstruc-\ntured corpora embedded in vector indices. The integration\nprocess requires not only the retrieval of relevant content\nbut also mechanisms for arbitration between internally stored\nand externally sourced knowledge ensuring outputs are both\ntemporally current and contextually precise. In healthcare\nand clinical decision support, this enables an LLM-based\nagent to ground its reasoning in authoritative sources such as\ndrug interaction databases, treatment guidelines or up-to-date\nclinical trial results. The degree to which an agent embodies\nthis sub-dimension can vary: (a) fully realized implementa-\ntions ✓ incorporate structured RAG pipelines with calibrated\narbitration between internal recall and external evidence (b)\npartial implementations ∆ rely on limited, static or manually\ncurated external content (c) absent implementations ✗ oper-\nate purely from parametric memory.\n2) Memory Module\nMemory Module in LLM-based agents provide structured\nmechanisms for storing and retrieving information beyond\nthe transient context window enabling continuity and lon-\ngitudinal reasoning. Short-term memory mechanisms only\nmaintain immediate conversational history. However, long-\nterm stores capture episodic narratives like past consultations\nand semantic abstractions such as knowledge graphs. These\nmodules are often mediated by learned or rule-based con-\ntrollers that determine what information should be commit-\nted to persistent storage and when historical records should\nbe recalled for decision making. In healthcare applications,\nsuch architectures support richer patient interactions allow-\ning agents to reference prior lab results, follow-up notes\nor evolving care plans without reintroduction by the user.\nSystems may fully implement ✓ this sub-dimension through\nsophisticated, persistent multi-tier memory with dynamic\nread/write control. Systems may partially implement ∆ it\nthrough limited short-term session scoped recall or basic\nTABLE 2. Evaluation of Knowledge Management\nPapers External\nKnowledge\nIntegration\nMemory\nModule\nDynamic\nUpdates &\nForgetting\nColacare [25] ✓ ∆ ✗\nMedagent [49] ✗ ✗ ✗\nMedaide [50] ✓ ∆ ✗\nDoctoragent [51] ✗ ∆ ✗\nAihospital [52] ✗ ✗ ✗\nMmedagent [53] ✓ ∆ ✗\nMedchat [54] ✗ ✗ ✗\nMedco [55] ✗ ✓ ∆\nMedagentpro [56] ✓ ∆ ✗\nClinicalagent [57] ✓ ∆ ✗\nKtas [58] ✓ ∆ ✗\nAgentmd [59] ✓ ∆ ✗\nMdagent [60] ✓ ∆ ✗\nAgenthospital [61] ✓ ✓ ∆\nPharmagent [62] ✓ ✓ ∆\nTxagent [63] ✓ ∆ ✗\nAutoct [64] ✓ ∆ ✗\nKg4diagnosis [65] ✓ ✓ ∆\nMdteamgpt [66] ✓ ✓ ∆\nMitigating [67] ✗ ✗ ✗\nPolaris [68] ✓ ∆ ✗\nRxstrategist [69] ✓ ∆ ✗\nClinicallab [70] ✗ ✗ ✗\nAgenticllm [71] ✗ ✗ ✗\nSurgbox [72] ✓ ∆ ✗\nAipatient [73] ✓ ✓ ∆\nHealthagent [74] ✓ ✓ ∆\nIcdcoding [75] ✓ ∆ ✗\nAha [76] ✗ ✗ ✗\nDdo [77] ✓ ✓ ∆\nBeyond [78] ✓ ∆ ✗\nZodiac [26] ✓ ∆ ✗\nFhirviz [79] ✓ ✓ ∆\nFhir [80] ∆ ∆ ✗\nFuasagent [81] ✓ ✓ ∆\nAitherapist [82] ✗ ✗ ✗\nRadiology [83] ✓ ∆ ✗\nTriageagent [24] ✓ ∆ ✗\nPiors [84] ✓ ✓ ∆\nSelfevolve [85] ✓ ✓ ∆\nDrugagent [86] ✓ ∆ ✗\nCtagent [87] ✓ ✓ ∆\nAdagent [88] ✗ ✗ ✗\nDrugpilot [89] ✓ ✓ ✓\nLlmadmin [90] ✓ ∆ ✗\nSwarm [91] ✓ ✓ ∆\nDrugdiscover [92] ✓ ✓ ∆\nRagdrug [93] ✓ ∆ ✗\nDrugreason [94] ✓ ∆ ✗\nretrieval buffers. Systems may not implement ✗ it at all\nrelying solely on the model’s default context window.\n3) Dynamic Updates & Forgetting\nDynamic Updates & Forgetting captures an agent’s ability to\nkeep its knowledge current and hygienic rather than merely\ncumulative. Beyond simple retrieval, the system must: (i)\ningest new evidence via continuous indexing (ii) re-weight\nstored items using temporal decay (iii) remove stale or re-\ntracted facts through explicit pruning rules. In clinical con-\ntexts, these mechanisms align outputs with evolving guide-\n8 VOLUME 14, 2026\n\n-- 8 of 25 --\n\nlines, drug recalls and newly published or retracted trials\nthus reducing the risk of obsolete recommendations. We con-\nsider this ✓ when both sides of knowledge maintenance are\npresent: active addition (continuous updates/write-back) and\nexplicit forgetting (time-decay thresholds and deprecation\npolicies). It is ∆ when updates occur but no formal decay\nis defined resulting in static accumulation over time. It is ✗\nwhen the system is static with no continuous updates decay\nor pruning beyond the model’s default context.\nC. INTERACTION PATTERNS\nInteraction patterns specify how an agent is invoked, sustains\ncontext and negotiates turns across multimodal exchanges\nso that state remains coherent across intermittent sessions.\nConversation with LLM-based agents is not merely turn\ntaking but it is the disciplined management of context over\ntime. Robust dialogue practices such as greetings, confir-\nmations, clarifications and summarization stabilize meaning\nand keep state coherent [101]). Empirical studies of LLM\nvoice assistants likewise show richer, more adaptive turn\ntaking that mitigates breakdowns through proactive follow-\nups [102]. In clinical and patient facing settings, such dis-\nciplined conduct reduces miscommunication, improves data\ncapture and supports safer handoffs between encounters.\nBeyond direct chat, agent behavior must also be orchestrated\nby external signals like incoming lab results, device data\nstreams or workflow events. In healthcare operations, such\nevent-driven activation enables timely triage, order checks\nand monitoring escalations without relying on continuous\nuser supervision. Because clinical decisions have conse-\nquences, interaction must explicitly allocate roles for over-\nsight. Human-AI collaboration work in medicine shows that\nexpert steerable tools improve trust and utility without sac-\nrificing accuracy [103]. Finally, interactions must anticipate\nfailure and recover gracefully. Generic safeguards include\ngreedy decoding such as self-consistency to expose diver-\ngent reasoning paths [46], Chain-of-Verification routines that\nplan and answer fact checking questions before issuing a\nfinal response [104] and reflective replanning that rewrites\ngoals or tool sequences after unsuccessful trials [45]. Op-\nerationally, resilient agents employ transactional rollbacks,\nbounded retries, circuit breakers and degraded safe modes.\nIn clinical environments, such modality agnostic behaviors\nhelp avert propagation of errors such as halting downstream\norder placement after a failed formulary check, suppressing\nduplicate documentation after API timeouts etc. As compiled\nin Table 3, Interaction Patterns are assessed along:\n1) Conversational Mode\nConversational Mode characterizes an agent’s capacity for\nsynchronous, turn-based exchanges that preserve discourse\nstate across turns and sessions. Hallmarks include dialogue\nstate persistence, session scoped memory for user intents\nand support for clarifying follow-ups rather than single-shot\nreplies. Systems may expose a chat UI but are distinguished\nby continuity and resumability after interruptions. We rate\nthis ✓ when the agent provides a turn-based conversational\ninterface that maintains context across turns and supports\nresumptions. ∆ applies when multi-round interaction exists\nbut is primarily a structured workflow or prompt sequenced\nexchange with fragile context retention. ✗ describes single\nquery processing with no evidence of turn-based dialogue,\nsession continuity or conversational context maintenance. In\nhealthcare, virtual intake, symptom follow-ups and discharge\neducation implemented systems reduce re-entry of informa-\ntion and support more efficient clinician-patient exchanges.\n2) Event-Triggered Activation\nEvent-Triggered Activation denotes an agent’s capacity to\nautonomously monitor external signals via webhooks or mes-\nsage buses and initiate workflows when specified conditions\nare met without explicit user prompting. Mature designs\nsubscribe to streams, apply temporal windows, enrich events\nwith context and invoke idempotent handlers under rate lim-\nits. We rate this ✓ when real-time, automated triggers from\nexternal sources activate agents with continuous monitoring\nand proactive execution. ∆ is earned when basic triggering\nexists (API callbacks or scheduled polling) but activation is\nlargely initiated by the user or lacks durable stream integra-\ntion. ✗ is labeled when operation relies solely on prompted\ninteractions or internal thresholds with no autonomous mon-\nitoring of external streams. For healthcare settings, robust\nactivation delivers timely interventions like abnormal vitals,\nformulary aware order suppression etc.\n3) Human-in-the-Loop\nHuman-in-the-Loop captures an agent’s explicit insertion of\nhuman control during execution and not merely at input\nor after output. This is achieved via confirmation gates,\nuncertainty-triggered pauses and rollback controls. The gath-\nered feedback is then stored in an auditable manner to refine\nmemory or downstream policies. We rate this ✓ when users\ncan confirm, approve or redirect the agent mid-run. ∆ applies\nwhen only basic review or approval is simulated or confirma-\ntions are optional without durable feedback incorporation. ✗\ndescribes pipelines with no in-process oversight. In work-\nflows like prior-authorization and sensitive chart updates,\nrobust Human-in-the-Loop limits unchecked autonomy, im-\nproves accountability and supports clinical governance.\n4) Error Recovery\nError Recovery denotes an agent’s capacity to detect, contain\nand correct failures across parsing, planning, tool use and\nsystem integration. In this sub-dimension, the agent flags\nanomalies, conducts a structured diagnosis of likely causes\nand then applies corrective steps which can be issuing disam-\nbiguation prompts, switching to alternative tools or repairing\nparameters. All these corrective measures are taken ensuring\ntransactional safety. Mature designs incorporate explicit re-\ncovery nodes that capture exceptions and route control back\nto an executor for re-execution. We rate this ✓ when robust\ndetection, retries and fallback procedures are integrated as\nVOLUME 14, 2026 9\n\n-- 9 of 25 --\n\nTABLE 3. Evaluation of Interaction Patterns\nPapers Conver-\nsational\nMode\nEvent-\nTriggered\nActivation\nHuman-\nin-the\nLoop\nError\nRecovery\nColacare [25] ✗ ✗ ✗ ∆\nMedagent [49] ∆ ✗ ✗ ∆\nMedaide [50] ✓ ✗ ✗ ✗\nDoctoragent [51] ✓ ✗ ✗ ✗\nAihospital [52] ✓ ✗ ✗ ✗\nMmedagent [53] ✓ ✗ ✗ ✗\nMedchat [54] ✓ ✗ ✗ ✗\nMedco [55] ∆ ✗ ✗ ✗\nMedagentpro [56] ✗ ✗ ✗ ∆\nClinicalagent [57] ✓ ✗ ✗ ✗\nKtas [58] ✗ ✗ ✗ ✗\nAgentmd [59] ✗ ✗ ✗ ∆\nMdagent [60] ✓ ✗ ✗ ∆\nAgenthospital [61] ✗ ✓ ✗ ∆\nPharmagent [62] ✗ ✗ ✗ ∆\nTxagent [63] ✗ ✗ ✗ ∆\nAutoct [64] ✗ ✗ ✗ ∆\nKg4diagnosis [65] ✗ ✗ ✓ ✗\nMdteamgpt [66] ∆ ✗ ✗ ∆\nMitigating [67] ✓ ✗ ✗ ✗\nPolaris [68] ✓ ✗ ✓ ∆\nRxstrategist [69] ✗ ✗ ✗ ✗\nClinicallab [70] ✗ ✗ ✓ ✗\nAgenticllm [71] ✗ ✗ ✗ ✗\nSurgbox [72] ✓ ✓ ✗ ✗\nAipatient [73] ✓ ✗ ✗ ∆\nHealthagent [74] ✓ ✗ ✗ ✗\nIcdcoding [75] ✗ ✗ ∆ ✗\nAha [76] ✗ ✗ ✗ ✗\nDdo [77] ✓ ✗ ✗ ✗\nBeyond [78] ✗ ✗ ✗ ✗\nZodiac [26] ✗ ∆ ✗ ✗\nFhirviz [79] ✗ ✗ ✓ ✓\nFhir [80] ✗ ✗ ✗ ✗\nFuasagent [81] ✗ ✗ ✗ ✗\nAitherapist [82] ✓ ✗ ✗ ✗\nRadiology [83] ✗ ✗ ✗ ∆\nTriageagent [24] ✓ ✗ ✗ ✗\nPiors [84] ✓ ✗ ✗ ✗\nSelfevolve [85] ✓ ✗ ✓ ∆\nDrugagent [86] ✗ ✗ ✗ ✗\nCtagent [87] ✗ ✗ ✗ ✗\nAdagent [88] ✓ ✗ ✗ ✗\nDrugpilot [89] ✓ ✗ ✓ ✓\nLlmadmin [90] ✓ ✓ ✗ ∆\nSwarm [91] ✗ ✗ ✗ ∆\nDrugdiscover [92] ✗ ✗ ✗ ✗\nRagdrug [93] ✗ ✗ ✗ ✗\nDrugreason [94] ✓ ✗ ✗ ∆\nfirst class protocols with demonstrated re-execution paths. ∆\napplies when only basic detection or limited fallbacks and\nretries are present. ✗ describes pipelines lacking explicit run-\ntime error handling or recovery strategies. Robust recovery\nprevents error cascades in clinical care. It halts malformed\norder writes, suppresses duplicate documentation when APIs\ntime out and pauses chart updates until verification succeeds.\nD. ADAPTATION & LEARNING\nAdaptation & Learning is the discipline of keeping de-\nployed agents calibrated to a moving world. It begins with\ncontinuous sensing for distributional and behavioral shifts,\nthen applying targeted mitigations before clinical perfor-\nmance erodes. Drift can be detected by monitoring changes\nin text embedding distributions [105], by activation delta\nprobes [106] and by black box statistical tests that audit\nbehavior changes over time [107]. Mitigations span prompt\nhotfixes, domain adaptation and scheduled retraining. Their\nnecessity is evidenced by temporal dataset shifts degrading\nICU prediction models in EHRs [108]. In practice, health-\ncare deployments couple these monitors with escalation and\nrollback to preserve patient safety and regulatory traceabil-\nity. Beyond detection, agents must learn from evaluators.\nReinforcement-based adaptation formalizes feedback as re-\nwards. Research shows that constitutional self-supervision\nsubstitutes AI feedback for human labels to reduce harm-\nfulness [109] and verbal reward agents store reflective cri-\ntiques as episodic memories to steer subsequent decisions\nwithout weight updates [45]. In clinical settings, these path-\nways enable clinician-in-the-loop reward shaping and post\ndeployment alignment. For example, medical question an-\nswering systems refined with domain specific supervision\nand evaluation protocols achieve ground breaking reliability\non USMLE style questions [42]. Finally, rapid specialization\nunder data scarcity relies on meta-learning and few-shot\ncompetence. Gradient-based meta-learners initialize models\nfor fast adaptation from a handful of cases [110], while\nLLMs exhibit in-context learning that acquires new tasks\nfrom a few demonstrations at inference [111]. Combined\nwith retrieval and sparse labeling, these methods let agents\ngeneralize to a new hospital, imaging device or rare disease\nwith minimal annotation burden. This is especially crucial\nwhere prospective data collection is slow and expensive.\nTable 4 details Adaptation & Learning results across its 3\nsub-dimensions:\n1) Drift Detection & Mitigation\nThis sub-dimension concerns the capacity of LLM-based\nagents to remain reliable under evolving data distributions,\ntask definitions or usage contexts. Drift manifests when the\nstatistical profile of inputs diverges from the training regime\nor when task requirements subtly shift leading to perfor-\nmance degradation. In well implemented systems, statistical\ndivergence tests on incoming data streams, embedding space\ncomparisons or activation level drift classifiers provide early\nwarnings. This is followed by solid mitigation through re-\ntraining, prompt adaptation or deployment of updated models\nconstituting a ✓ state. In more limited cases, agents may\nexhibit only partial safeguards such as exposure to diverse\nsamples during iterative training that reduces but does not\ndirectly monitor drift, depicting a ∆ state. Conversely, sys-\ntems without explicit statistical monitoring or any corrective\nmechanisms remain vulnerable to silent degradation aligning\nwith an ✗ state. In healthcare, where distributional shifts\noccur due to population demographics or evolving clinical\nguidelines, drift detection and mitigation is indispensable for\npreserving patient safety and minimizing diagnostic error.\n10 VOLUME 14, 2026\n\n-- 10 of 25 --\n\nTABLE 4. Evaluation of Adaptation & Learning\nPapers Drift Detec-\ntion & Miti-\ngation\nReinforce-\nment Based\nAdaptation\nMeta-\nLearning &\nFew-Shot\nColacare [25] ✗ ✗ ✓\nMedagent [49] ✗ ✗ ✓\nMedaide [50] ✗ ✗ ✗\nDoctoragent [51] ✗ ✓ ✗\nAihospital [52] ✗ ✗ ✗\nMmedagent [53] ✗ ✗ ✗\nMedchat [54] ✗ ✗ ✗\nMedco [55] ✗ ✗ ✗\nMedagentpro [56] ✗ ✗ ✗\nClinicalagent [57] ✗ ✗ ✓\nKtas [58] ✗ ✗ ✗\nAgentmd [59] ✗ ✗ ✗\nMdagent [60] ✗ ✗ ✓\nAgenthospital [61] ✗ ✗ ✗\nPharmagent [62] ✗ ✗ ✗\nTxagent [63] ✗ ✗ ✗\nAutoct [64] ✗ ∆ ✗\nKg4diagnosis [65] ✗ ✓ ✗\nMdteamgpt [66] ✗ ✗ ✓\nMitigating [67] ✗ ✗ ✗\nPolaris [68] ∆ ✓ ✗\nRxstrategist [69] ✗ ✗ ✗\nClinicallab [70] ✗ ✗ ✗\nAgenticllm [71] ✗ ∆ ✗\nSurgbox [72] ✗ ✗ ✗\nAipatient [73] ✗ ✗ ✓\nHealthagent [74] ✗ ✗ ✗\nIcdcoding [75] ✗ ✗ ✗\nAha [76] ✗ ✗ ✓\nDdo [77] ✗ ✓ ✗\nBeyond [78] ✗ ✗ ✗\nZodiac [26] ✗ ✗ ✓\nFhirviz [79] ✗ ✗ ✗\nFhir [80] ✗ ✗ ✗\nFuasagent [81] ✗ ∆ ✗\nAitherapist [82] ✗ ✗ ✗\nRadiology [83] ✗ ✗ ✗\nTriageagent [24] ✗ ✗ ✗\nPiors [84] ✗ ✗ ✗\nSelfevolve [85] ✗ ✗ ✓\nDrugagent [86] ✗ ✗ ✗\nCtagent [87] ✗ ✗ ✓\nAdagent [88] ✗ ✗ ✗\nDrugpilot [89] ✗ ✗ ✗\nLlmadmin [90] ✗ ✗ ✗\nSwarm [91] ✗ ✗ ✗\nDrugdiscover [92] ✗ ✓ ✗\nRagdrug [93] ✗ ✗ ∆\nDrugreason [94] ✗ ✗ ✗\n2) Reinforcement-Based Adaptation\nThis sub-dimension captures how LLM-based agents refine\ntheir behavior by translating evaluative feedback into struc-\ntured reward signals that shape future decisions. In fully\nfledged implementations, explicit reinforcement learning\nmechanisms such as RLHF are used. In these mechanisms,\nexpert preferences are used to optimize policies that encode\nverbal feedback into memory for iterative improvement and\nmake up a ✓ label. These systems demonstrate continuous\nadaptation where reward-driven loops refine decision policies\nand integrate expert validation. In contrast, partial imple-\nmentations may rely on simplified forms of feedback such\nas binary success or failure signals, heuristic optimization,\nverbal reinforcement in prompts etc. These approaches pro-\nvide directional guidance but lack systematic reinforcement\nmechanisms thereby portraying a ∆ label. Finally, systems\nthat omit explicit reward modeling, reinforcement learning\nor preference-driven updates align with an ✗ category. In\nhealthcare contexts, safe adaptation requires agents to inte-\ngrate clinician feedback, regulatory requirements and patient\nspecific preferences.\n3) Meta-Learning & Few-Shot\nMeta-Learning & Few-Shot refers to the ability of LLM-\nbased agents to adapt quickly to new tasks or domains\nwith minimal supervision. Meta-learning methods such as\ngradient-based algorithms that train models to update effec-\ntively from only a few examples enable rapid reconfiguration\nacross tasks without requiring extensive retraining. Comple-\nmentary to this, in-context few-shot prompting leverages pre-\ntrained representations to generalize at inference time where\na handful of demonstrations within the prompt can guide\naccurate reasoning. Systems that combine these strategies\nusing either meta-learned initialization or sophisticated few-\nshot prompting protocols are regarded as ✓. More limited\napproaches such as relying solely on zero-shot generalization\nor retrieving exemplar cases without systematic adaptation\nare categorized as ∆. Meanwhile, frameworks that show\nno mention of few-shot adaptation, meta-learning protocols\nor rapid task specialization are classified as ✗. This sub-\ndimension’s capabilities are particularly consequential in\nmedicine where models must rapidly accommodate shifts\nin clinical practice, rare patient phenotypes and evolving\nevidence bases in dynamic care environments.\nE. SAFETY & ETHICS\nLLM agents in healthcare must proactively prevent unsafe\nbehaviors through layered alignment, systematic red team-\ning and attack aware orchestration. Empirical work shows\nthat jailbreaks can be auto-generated even when the model\nis a black box [112]. Recent studies document automatic\nprompt injection attacks [113] and organize defenses into\nclear taxonomies [114] while frameworks like Constitutional\nAI [109] and end-to-end red-teaming methods [115] establish\nprocedural guardrails. Equity requires detecting and mitigat-\ning systematic errors across patient groups, with evaluation\nand remediation guided by LLM bias surveys [116], [117].\nIn healthcare specifically, evidence such as the analysis by\n[118] of a widely used risk algorithm illustrates how proxy\ntargets can induce racial disparities, motivating stratified\nreporting and bias stress tests. Patient privacy hinges on\nminimizing identifiability and resisting model level leakage.\nEmpirical attacks retrieve verbatim training data from models\n[119] underscoring the need for privacy-preserving learning.\nIn healthcare, federated learning enables cross-institutional\ncollaboration without centralizing PHI [120] complemented\nby robust clinical text deidentification. These practices enable\nVOLUME 14, 2026 11\n\n-- 11 of 25 --\n\nmodel development, evaluation and monitoring while honor-\ning confidentiality constraints at the point of care. Finally,\ntrustworthy deployment demands verifiable compliance and\nauditability. Scholarly analyses detail how the EU AI Act\nreshapes risk classification and obligations for health AI\n[121]. In the U.S., reviews of device oversight highlight gaps\nin transparency and representation for AI tools [122]. These\nframeworks ground provenance logging, rationale tracing\nand post market surveillance necessary for clinical account-\nability. Table 5 presents Safety & Ethics evaluations across\nits 4 sub-dimensions:\n1) Safety Guardrails & Adversarial Robustness\nSafety Guardrails & Adversarial Robustness defines the de-\nfense controls that prevent, detect and contain harmful behav-\nior when agents are steered intentionally or accidentally away\nfrom intended clinical use. It spans hardening of inputs to\nprevent anything like prompt injection, resilient planning and\noutput governance with continuous stress testing. Under this\nsub-dimension, ✓ denotes mature, multi-stage safeguards\nwith automated detectors, adversarial evaluation and rapid\nremediation pathways. ∆ indicates isolated filters or ad hoc\nhuman review without systematic testing. ✗ reflects the ab-\nsence of explicit mechanisms or evidence of effectiveness.\nIn healthcare delivery, the distinction between these labels\nis practical. Robust systems block unsafe orders, resist ma-\nnipulation of EHR linked tools, catch dosing anomalies and\nprevent escalation of failures across multi-step workflows.\n2) Bias & Fairness\nBias & Fairness concerns whether an agent’s behavior re-\nmains equitable across clinically salient populations and\ncontexts. It requires: (i) representative data with verified\nlabels (ii) modeling choices that constrain disparate error\n(iii) deployment policies to minimize inequity. Within this\nsub-dimension, ✓ denotes balanced representation across\nclinically salient cohorts like age and gender. It also com-\nprises of subgroup bias analyses with reported disparities and\napplied mitigation. ∆ applies when only privacy steps such as\ndeidentification/anonymization are used or bias concerns are\nmerely acknowledged. But there is no demographic auditing,\nno fairness metrics and no implemented bias detection or\nmitigation mechanisms. ✗ describes the absence of bias as-\nsessment and fairness mechanisms completely. In healthcare,\nbias and fairness is essential to prevent underdiagnosis or\ninequitable access particularly across intersections of race,\nage, sex, language, disability and socioeconomic status.\n3) Privacy-Preserving Mechanism\nThis sub-dimension specifies the architectural and procedural\ncontrols that restrict who and what can access patient data\nacross the agent lifecycle. This sub-dimension broadly cov-\ners mechanisms including edge inference, trusted execution,\nencryption in transit, private retrieval with scoped context\nwindows, deidentification/pseudonymization and differential\nprivacy for training as well as analytics. Programs should\nTABLE 5. Evaluation of Safety & Ethics\nPapers Safety\nGuardrails &\nAdversarial\nRobustness\nBias &\nFairness\nPrivacy\nPreserving\nMechanism\nRegulatory\n& Com-\npliance\nConstraints\nColacare [25] ✗ ∆ ∆ ✗\nMedagent [49] ✗ ✗ ✗ ✗\nMedaide [50] ∆ ∆ ∆ ✗\nDoctoragent [51] ∆ ✗ ✗ ✓\nAihospital [52] ✗ ∆ ∆ ✗\nMmedagent [53] ✗ ✗ ✗ ✗\nMedchat [54] ✗ ✗ ✗ ✗\nMedco [55] ✗ ✗ ∆ ✗\nMedagentpro [56] ✗ ✗ ✗ ✗\nClinicalagent [57] ✗ ✗ ✗ ✗\nKtas [58] ✗ ✗ ✓ ✓\nAgentmd [59] ✗ ✗ ∆ ∆\nMdagent [60] ✗ ∆ ✗ ✗\nAgenthospital [61] ✗ ∆ ✗ ✗\nPharmagent [62] ✗ ✗ ✗ ✗\nTxagent [63] ∆ ∆ ✓ ✗\nAutoct [64] ✗ ✗ ✗ ✗\nKg4diagnosis [65] ∆ ✗ ✗ ✗\nMdteamgpt [66] ✓ ∆ ✗ ✗\nMitigating [67] ✗ ✓ ✗ ✗\nPolaris [68] ✓ ✓ ✓ ✗\nRxstrategist [69] ✗ ✗ ∆ ✗\nClinicallab [70] ∆ ✓ ∆ ∆\nAgenticllm [71] ∆ ✗ ✗ ✗\nSurgbox [72] ∆ ∆ ∆ ✗\nAipatient [73] ∆ ∆ ✓ ✓\nHealthagent [74] ✗ ∆ ∆ ✗\nIcdcoding [75] ✗ ✗ ∆ ✗\nAha [76] ∆ ✗ ∆ ✗\nDdo [77] ✗ ✗ ✗ ✗\nBeyond [78] ✗ ✗ ✓ ✗\nZodiac [26] ∆ ✓ ✓ ✓\nFhirviz [79] ∆ ✗ ✗ ✗\nFhir [80] ✗ ✗ ✓ ✗\nFuasagent [81] ∆ ∆ ✓ ✓\nAitherapist [82] ✗ ✗ ✗ ✗\nRadiology [83] ∆ ✗ ✗ ✗\nTriageagent [24] ✗ ✗ ∆ ✗\nPiors [84] ✗ ∆ ∆ ✗\nSelfevolve [85] ✗ ✓ ✗ ✗\nDrugagent [86] ✗ ✗ ✗ ✗\nCtagent [87] ✗ ✗ ✗ ✗\nAdagent [88] ✗ ✗ ✓ ✗\nDrugpilot [89] ∆ ✗ ✗ ✗\nLlmadmin [90] ✓ ✗ ∆ ✗\nSwarm [91] ✗ ✗ ✗ ✗\nDrugdiscover [92] ✗ ✗ ✗ ✗\nRagdrug [93] ✗ ✗ ✗ ✗\nDrugreason [94] ✗ ✗ ✗ ✗\nalso test privacy explicitly via threat modeling, leakage\naudits and redaction verification. ✓ indicates agent level\nprivacy mechanisms beyond dataset preprocessing entailing\non-device handling of sensitive inputs, built in deidentifi-\ncation or anonymization, strong encryption and IRB/ethics\napproval when human or clinical data are involved. Many\nimplementations also use a dual layer design that keeps\nsensitive processing local. ∆ applies when privacy is lim-\nited to dataset deidentification or general compliance claims\n(e.g., HIPAA compliant). This does not incorporate an agent\n12 VOLUME 14, 2026\n\n-- 12 of 25 --\n\nside deidentification/anonymization mechanism or technical\ncontrols such as encryption, differential privacy and secure\ncomputation. ✗ describes systems with no technical privacy\ncontrols as defined above. In clinical deployments, these\ncontrols prevent PHI exfiltration from EHR linked tools,\nreduce insider exposure and enable compliant collaboration\nwithout centralizing sensitive data.\n4) Regulatory & Compliance Constraints\nRegulatory & Compliance Constraints specifies how an\nagent’s design, data handling and operating procedures sat-\nisfy binding laws across collection, training, inference and\npost deployment monitoring. Core elements include: (i) doc-\numented consent and lawful bases (ii) role scoped access\nand retention schedules (iii) cross-border transfer controls\n(iv) formal risk assessments that tie policies to technical\nsafeguards. Compliance should be evidenced and not merely\nasserted through policies, test records and release gates.\nIn healthcare, these controls determine whether an agent’s\nrecommendation features would be treated as regulated soft-\nware, whether PHI handling meets HIPAA or GDPR duties\nand whether deployments withstand clinical legal review. ✓\nindicates clear alignment with applicable laws like HIPAA\nor GDPR, operational consent and retention controls. It also\nencompasses IRB or ethics approval when human subjects or\nclinical data are involved and incident response procedures\nthat include documented drills. ∆ applies when evidence is\nlimited to isolated attestations like HIPAA compliant cloud\nand third party certifications. This does not cover docu-\nmented consent flows or risk assessment with policy state-\nments that lack operational evidence and reliance on vendor\nassurances. ✗ describes systems with no explicit mapping to\nlaws or standards.\nF. FRAMEWORK TYPOLOGY\nFramework Typology delineates the structural and opera-\ntional blueprints underlying LLM-based agent systems. It\narticulates how multiple reasoning units are composed and\nhow control is exercised over their joint behavior. In many\nrecent systems, we see a design that partitions cognitive\nlabor across several specialized agent roles which collaborate\nvia rounds of discussion to reach consensus. For example,\n[49] uses a multi-round discussion among LLM agents that\nmimic domain experts, combining their individual analyses\ninto a summary before arriving at a decision. Similarly, the\nMulti-Agent Conversation (MAC) [123] framework emulates\nmultidisciplinary team discussions in rare disease diagnosis.\nIt involves multiple doctor agents plus a supervisor agent\ndiscussing clinical features and different diagnostic hypothe-\nses. Systems of this kind yield benefits in medical question\nanswering, clinical decision support and diagnosis because\nthey mirror multidisciplinary workflows. They contrast with\nmonolithic designs where a single LLM is responsible for all\nstages of reasoning which may be simpler but risk domain\nmisknowledge in healthcare settings. Complementing the\ncomposition of agent roles is the mechanism by which con-\ntrol, state, feedback and adaptation are governed over time.\nThis includes how tasks are orchestrated and whether there\nis a central controller that assigns subtasks, monitors agent\noutputs and enforces checks. For instance, [124] implements\na central planning agent that breaks down complex objectives\nand delegates to sub-agents, then aggregates and synthesizes\ntheir outputs. Such centralized orchestration is particularly\ncrucial in clinical environments to ensure compliance, to\nmanage latency and to integrate human interventions at criti-\ncal junctures. On the other hand, some frameworks explore\ndecentralized or adaptive orchestration. Along those lines,\n[125] lets agents self-specialize, evolve their connections in\na DAG structure and route tasks without a single central con-\ntroller. Table 6 provides Framework Typology assessments\nacross:\n1) Multi-Agent Design\nMulti-Agent Design captures how responsibilities within an\nLLM-based system are distributed across one or multiple\nagents and how these agents interact to accomplish tasks.\nAt one extreme, single-agent designs encapsulate reasoning,\nplanning and execution within a unified process providing us\nwith simplicity but limiting specialization. More advanced\nsystems decompose functionality into distinct agents such\nas planners, retrievers or verifiers. These distinct agents\nthen exchange messages or deliberate collectively, thereby\nenabling modularity, redundancy and clearer alignment with\ndomain specific expertise. In healthcare, such distributed\nstructures can emulate multidisciplinary collaboration. For\ninstance, having separate agents to parse imaging results, val-\nidate treatment guidelines and draft patient facing summaries\nresembles such multidisciplinary collaboration. Evaluating\nthis sub-dimension requires distinguishing whether a system\ndemonstrates a genuine multi-agent architecture, only partial\ntraces of such coordination or no such implementation at all.\nThus, ✓ applies when specialized roles are clearly instanti-\nated, ∆ captures ambiguous or superficial coordination and ✗\ndenotes monolithic or tool-augmented single-agent systems.\n2) Centralized Orchestration\nCentralized Orchestration refers to how control is exercised\nover the flow of tasks, data and decisions in LLM-based agent\nsystems. In its strongest form, a central controller manages\nthe sequencing of agents, monitors their outputs and inte-\ngrates results into a coherent whole. Although, centralization\ncan introduce single points of failure and latency bottlenecks,\nit ensures global consistency, enables oversight and provides\nclear points of intervention. Other frameworks rely on looser\ncoordination such as hierarchical delegation or peer-to-peer\ncommunication where agents collaborate without a single\ncoordinating authority. These offer flexibility but may reduce\nreliability in safety critical settings. Some systems operate\nwith only sequential task execution and minimal oversight,\nresembling simple pipelines rather than true orchestration.\nIn healthcare, these distinctions carry significant weight. A\nrobust orchestrator can ensure that multiple evidence sources\nVOLUME 14, 2026 13\n\n-- 13 of 25 --\n\nTABLE 6. Evaluation of Framework Typology\nPapers Multi-Agent\nDesign\nCentralized\nOrchestration\nColacare [25] ✓ ✓\nMedagent [49] ✓ ∆\nMedaide [50] ✓ ✓\nDoctoragent [51] ✓ ✓\nAihospital [52] ✓ ∆\nMmedagent [53] ✗ ✗\nMedchat [54] ✓ ∆\nMedco [55] ✓ ∆\nMedagentpro [56] ✗ ∆\nClinicalagent [57] ✓ ✓\nKtas [58] ✓ ∆\nAgentmd [59] ✗ ✗\nMdagent [60] ✓ ✓\nAgenthospital [61] ✓ ✓\nPharmagent [62] ✓ ∆\nTxagent [63] ✓ ∆\nAutoct [64] ✓ ∆\nKg4diagnosis [65] ✓ ✓\nMdteamgpt [66] ✓ ✓\nMitigating [67] ✓ ∆\nPolaris [68] ✓ ✓\nRxstrategist [69] ✓ ∆\nClinicallab [70] ✓ ∆\nAgenticllm [71] ∆ ✗\nSurgbox [72] ✓ ✓\nAipatient [73] ✓ ∆\nHealthagent [74] ✗ ✗\nIcdcoding [75] ✓ ∆\nAha [76] ✓ ∆\nDdo [77] ✓ ∆\nBeyond [78] ✓ ∆\nZodiac [26] ✓ ∆\nFhirviz [79] ✓ ∆\nFhir [80] ✓ ✓\nFuasagent [81] ✓ ∆\nAitherapist [82] ✓ ∆\nRadiology [83] ✓ ∆\nTriageagent [24] ✓ ∆\nPiors [84] ✓ ∆\nSelfevolve [85] ✓ ∆\nDrugagent [86] ✓ ∆\nCtagent [87] ✗ ✗\nAdagent [88] ✗ ✗\nDrugpilot [89] ✗ ✗\nLlmadmin [90] ✓ ✓\nSwarm [91] ✓ ∆\nDrugdiscover [92] ✗ ✗\nRagdrug [93] ✓ ∆\nDrugreason [94] ✓ ✓\nare reconciled before clinical decisions are proposed and\ntrigger escalation to human experts when uncertainty is high.\nWithout such orchestration, systems risk fragmented reason-\ning, missed contradictions and limited accountability. Opera-\ntionally, orchestration is often realized via workflow engines\nand policy-based schedulers that implement timeouts, retries\nand circuit breakers. The evaluation of this sub-dimension\nrests on the sophistication of coordination. ✓ applies when\na central orchestrator is explicitly defined and bi-directional\ncontrol is visible. ∆ captures looser structures such as peer-\nto-peer or hierarchical delegation. ✗ describes sequential or\nsingle-agent designs.\nG. CORE TASKS & SUBTASKS\nCore tasks and subtasks define what an LLM-based agent\nactually does in healthcare. They scope the agent’s opera-\ntional responsibilities and the verifiable outputs it must de-\nliver. Starting from raw clinical data, these tasks progress to-\nward clinical recommendations and system level evaluation.\nThis dimension therefore grounds agent design in concrete\nworkflows and measurable endpoints. To start with, agents\ncan transform unstructured records into computable artifacts.\nThey can summarize problems, extract entities and link to\ncodes. They can reason over multi-table EHRs to answer\nrecord level queries as shown by EHRAgent [126]. Multi-\nagent pipelines can mirror documentation roles as well. For\nexample, ColaCare [25] coordinates DoctorAgents with a\nsupervising MetaAgent to integrate structured EHR data and\ntext reasoning. [76] frames prior-authorization evidence ex-\ntraction as a multi-agent classification and retrieval problem.\nThese designs connect directly to everyday charting, coding\nand utilization management. Next, agents can answer clinical\nquestions and broker guideline-based support. MDAgents\n[60] adapts agent roles moderator, retriever, verifier to task\ncomplexity and shows gains across medical decision bench-\nmarks. For EHR-grounded questions, agents plan tool use\nand compute evidence inside the chart as in EHRAgent [126].\nThis pattern ties answers to provenance which clinicians need\nfor accountable decision support. Further, agents can conduct\ntriage and seed differentials under uncertainty. Role special-\nized teams emulate emergency clinicians, pharmacists and\ncoordinators. [58] shows how a multi-agent clinical decision\nsupport system aligned to the Korean Triage and Acuity Scale\noperationalizes triage, treatment planning and routing in the\nemergency department. Hospital simulations let agents prac-\ntice intake, inquiry and differential ranking end-to-end as in\n[61]. These approaches reflect the time pressure, missingness\nand bias checks typical of frontline care. Next, agents can\nperform diagnostic reasoning through plans, tool calls and\nexplicit updates to hypotheses. Hierarchical teams split gen-\neralist intake from subspecialist review while grounding steps\nin knowledge graphs as discussed in [65]. MedAgent-Pro\n[56] showcases a multimodal reasoning agent for diagnosis.\nModular designs target interactive differential diagnosis with\nan orchestrator and role agents as the authors of MEDDx-\nAgent show in [127]. This mirrors consult pathways where\nuncertainty tracking and escalation are mandatory. Contin-\nuing, agents can plan treatments and verify prescriptions.\nFrameworks focused on therapeutics treat tool use as a first\nclass operation. TxAgent [63] selects from a ToolUniverse\nto check contraindications and personalize options. Prescrip-\ntion verification can be staged across indication, dose and\ninteraction checks with an agentic pipeline as documented\nin Rx Strategist [69]. Conversational reconciliation agents\nbring patients into the loop and reconcile lists before orders\nare signed [128]. These steps align with order sets, pharmacy\nverification and stewardship policies in practice. Agents can\nalso accelerate discovery and clinical trials. In discovery,\n14 VOLUME 14, 2026\n\n-- 14 of 25 --\n\nchemistry agents plan syntheses and reason over lab tools as\nillustrated by ChemCrow [129]. In trials, multi-agent systems\nsupport outcome prediction and study analytics [57]. Patient-\ntrial matching benefits from agentic knowledge augmentation\n[130] and complementary retrieval + LLM approaches such\nas TrialGPT [131]. These functions shorten candidate screen-\ning and increase equitable access to studies. Finally, agents\ncan sustain patient interaction and monitoring between visits.\nWearable systems compute personalized insights via agen-\ntic code execution and retrieval as the experiments show\nin [132]. Clinical deployments study physician supervised\nagents for medication reconciliation and safe communication\n[128]. These designs map to nurse line triage, remote mon-\nitoring programs and digital chronic care pathways. Lastly,\nagents can be benchmarked and stress tested in interactive,\ntool rich environments. Simulation testbeds evaluate end-to-\nend decisions, dialogue and tool use with realistic artifacts.\nAgentClinic [133] measures agent behavior in multimodal\nclinical encounters across specialties and languages. MedA-\ngentBench [134] embeds agents in a FHIR compliant vir-\ntual EHR with hundreds of physician written tasks. Such\nplatforms link agent behavior to safety, external validity and\nworkflow impact the way health systems evaluate new tools.\nTables 7 and Table 8 present the Core Tasks & Subtasks\nevaluations across:\n1) Clinical Documentation & EHR Analysis\nThis sub-dimension evaluates an agent’s ability to convert\nheterogeneous clinical documentation and EHR artifacts into\ncomputable and clinician usable outputs. Characteristics of\nthis sub-dimension span chart summarization, extraction of\nclinically salient entities, mapping to codes like ICD, risk\nprediction from longitudinal signals and draft generation\nof reports like radiology impressions. An agent merits ✓\nwhen it demonstrates clear medical record processing and\nstructured information extraction on real clinical data and not\njust demos. This could include supporting core EHR analysis,\nclinical prediction tasks, reading from and writing to FHIR\nand handling specialty reports. ∆ indicates basic record gen-\neration or narrow document handling like prescriptions only\nand/or reliance on simulated rather than real documentation.\n✗ applies when no EHR analysis is present.\n2) Medical Question Answering & Decision Support\nMedical Question Answering & Decision Support deter-\nmines whether an agent can answer clinical questions and de-\nliver guideline-grounded decision support across modalities\nand workflows. Capabilities include: (i) factoid and synthesis\nquestion answering over authoritative medical sources (ii)\nretrieval-augmented recommendations (iii) multi-turn diag-\nnostic questioning (iv) task specific support such as triage\nclassification, cardiology assessment, surgical planning ad-\nvice and visual question answering. ✓ applies when either\ncapability medical question answering or decision support is\nclearly realized. This could include domain specific scientific\nquestion answering with benchmark validation, evidence-\nbased answers tied to citations, structured diagnostic pro-\ncesses, real-time recommendations, visual question answer-\ning for radiology regions and systematic reasoning traces. ∆\ncaptures narrow or limited scope especially when the demon-\nstrations are simulation-based only. Prior-authorization med-\nical necessity checks, prescription verification questions, di-\nalogue without robust guideline retrieval are good examples\nfor ∆. ✗ denotes the absence of medical question answering\nor decision support.\n3) Triage & Differential Diagnosis\nThis sub-dimension examines an agent’s capability to turn\nfirst contact information into two outputs: an acuity decision\nand a defensible list of alternatives. The agent elicits the\nchief complaint and history. It synthesizes vitals along with\ncontext and asks targeted follow-ups. It aligns to established\ntriage scales like ESI, KTAS and recommends a receiving\nservice or department with a brief rationale. In parallel, it\nconstructs a differential diagnosis, lists key discriminators\nand proposes next tests. ✓ applies when either triage or\ndifferential diagnosis is clearly implemented. This includes\nsymptom processing with ESI/KTAS-based urgency, depart-\nment recommendations with reasoning, production of ranked\nalternatives and safe routing via role agents. ∆ captures nar-\nrow diagnosis without explicit prioritization, single condition\nclassifiers, probability lists without urgency or simulation\nonly workflows. ✗ denotes the absence of these functions. In\nclinical care, these capabilities directly govern disposition,\ntime to treatment and escalation.\n4) Diagnostic Reasoning\nDiagnostic Reasoning tests an agent’s capacity to plan and\nexecute multi-step clinical inference that links targeted ev-\nidence gathering to a defensible final diagnosis. Core be-\nhaviors include: (i) eliciting and structuring new data (ii)\nselecting and sequencing diagnostic tests (iii) interpreting\nresults from structured and unstructured sources (iv) main-\ntaining and revising hypotheses (v) fusing inputs from spe-\ncialist agents (vi) aligning conclusions with guidelines (vii)\ntraversing knowledge graphs (viii) calibrating confidence\nwith explicit discriminators. An agent merits ✓ when it\nexhibits a structured reasoning workflow, performs active\ndata acquisition, ensembles specialist opinions via princi-\npled decision fusion and outputs ranked differentials with\nrationale. ∆ captures shallow or single-shot classification,\nlimited aggregation or region specific heuristics. ✗ denotes\nno diagnostic inference beyond extraction.\n5) Treatment Planning & Prescription\nTreatment Planning & Prescription concerns an agent’s abil-\nity to translate diagnostic context into an individualized\ntreatment plan and executable orders. Functionalities include\nselecting pharmacologic and non-pharmacologic options,\npersonalizing dose, checking contraindications, reconciling\nactive medications and generating patient counseling with\nstop criteria. Robust systems also account for allergies, re-\nVOLUME 14, 2026 15\n\n-- 15 of 25 --\n\nTABLE 7. Evaluation of Core Tasks & Subtasks Part 1\nPapers Clinical\nDocu-\nmentation\n& EHR\nAnalysis\nMedical\nQuestion\nAnswering\n& Decision\nSupport\nTriage &\nDifferential\nDiagnosis\nDiagnostic\nReasoning\nColacare [25] ✓ ✓ ∆ ∆\nMedagent [49] ✓ ✓ ∆ ∆\nMedaide [50] ∆ ✓ ✓ ✓\nDoctoragent [51] ✗ ✓ ✓ ✓\nAihospital [52] ✓ ∆ ∆ ✓\nMmedagent [53] ✓ ✓ ✗ ∆\nMedchat [54] ∆ ∆ ∆ ✓\nMedco [55] ∆ ∆ ∆ ✓\nMedagentpro [56] ✗ ✓ ✓ ✓\nClinicalagent [57] ✗ ✗ ✗ ✗\nKtas [58] ✗ ✓ ✓ ∆\nAgentmd [59] ✓ ✓ ∆ ∆\nMdagent [60] ✓ ✓ ✓ ✓\nAgenthospital [61] ∆ ✓ ✓ ✓\nPharmagent [62] ✗ ✗ ✗ ✗\nTxagent [63] ✗ ✓ ✗ ✗\nAutoct [64] ✗ ✗ ✗ ✗\nKg4diagnosis [65] ∆ ✓ ✓ ✓\nMdteamgpt [66] ✗ ✓ ✓ ✓\nMitigating [67] ∆ ✓ ✓ ✓\nPolaris [68] ✓ ✓ ✓ ✓\nRxstrategist [69] ∆ ∆ ✗ ∆\nClinicallab [70] ✓ ✓ ✓ ✓\nAgenticllm [71] ✓ ✗ ✗ ✗\nSurgbox [72] ∆ ✓ ✗ ✗\nAipatient [73] ✓ ✓ ✗ ✗\nHealthagent [74] ✓ ✓ ✗ ✗\nIcdcoding [75] ✓ ✗ ✗ ∆\nAha [76] ✓ ∆ ✗ ∆\nDdo [77] ✓ ∆ ✓ ✓\nBeyond [78] ✗ ✓ ∆ ✓\nZodiac [26] ✓ ✓ ∆ ✓\nFhirviz [79] ✓ ✗ ✗ ✗\nFhir [80] ✓ ✗ ✗ ✗\nFuasagent [81] ∆ ∆ ✗ ∆\nAitherapist [82] ✗ ∆ ✗ ✗\nRadiology [83] ✓ ✗ ✗ ✓\nTriageagent [24] ✓ ✓ ✓ ∆\nPiors [84] ✓ ✓ ✓ ∆\nSelfevolve [85] ∆ ✓ ∆ ✓\nDrugagent [86] ✗ ✗ ✗ ✗\nCtagent [87] ✓ ✓ ∆ ∆\nAdagent [88] ∆ ✓ ∆ ∆\nDrugpilot [89] ✗ ✗ ✗ ✗\nLlmadmin [90] ✓ ✗ ✗ ✗\nSwarm [91] ✗ ✗ ✗ ✗\nDrugdiscover [92] ✗ ✓ ✗ ✗\nRagdrug [93] ∆ ✓ ✗ ✗\nDrugreason [94] ✗ ✗ ✗ ✗\nnal function, comorbidities and formularies. They can also\nprovide shared decision prompts, record provenance and\nhand off to a pharmacist or prescriber for verification. ✓\nis warranted when either treatment planning or prescription\nis clearly implemented. Under this label, the system pro-\nvides explicit recommendations, interaction checks, agentic\npharmacist review or tool-based safety checks and rationale\ntied to patient factors. ∆ signals basic option selection or\ntemplated suggestions without dose optimization, interaction\nscreening or personalization. It can also have demonstrations\nTABLE 8. Evaluation of Core Tasks & Subtasks Part 2\nPapers Treatment\nPlanning\n& Pre-\nscription\nDrug\nDiscovery\n& Clinical\nTrial\nDesign\nPatient\nInterac-\ntion &\nMonitor-\ning\nBench-\nmarking &\nSimulation\nEnviron-\nment\nColacare [25] ✗ ✗ ✗ ✓\nMedagent [49] ✗ ✗ ✗ ✓\nMedaide [50] ✓ ✗ ∆ ✓\nDoctoragent [51] ✓ ✗ ✓ ✓\nAihospital [52] ✓ ✗ ✓ ✓\nMmedagent [53] ✗ ✗ ✗ ✓\nMedchat [54] ∆ ✗ ✓ ✗\nMedco [55] ∆ ✗ ∆ ✓\nMedagentpro [56] ✗ ✗ ✗ ✓\nClinicalagent [57] ✗ ✓ ✗ ∆\nKtas [58] ✓ ✗ ✗ ✓\nAgentmd [59] ✗ ✗ ✗ ✓\nMdagent [60] ✗ ✗ ✗ ✓\nAgenthospital [61] ✓ ✗ ✓ ✓\nPharmagent [62] ✗ ✓ ✗ ✓\nTxagent [63] ✓ ∆ ✗ ✓\nAutoct [64] ✗ ✓ ✗ ✓\nKg4diagnosis [65] ∆ ✗ ✗ ✓\nMdteamgpt [66] ∆ ✗ ✗ ✓\nMitigating [67] ✗ ✗ ✗ ✓\nPolaris [68] ∆ ✗ ✓ ✓\nRxstrategist [69] ✓ ✗ ✗ ∆\nClinicallab [70] ✓ ✗ ∆ ✓\nAgenticllm [71] ✗ ✗ ✗ ∆\nSurgbox [72] ∆ ✗ ∆ ✓\nAipatient [73] ✗ ✗ ✓ ✓\nHealthagent [74] ✗ ✗ ✓ ∆\nIcdcoding [75] ∆ ✗ ✗ ✓\nAha [76] ✗ ✗ ✗ ∆\nDdo [77] ✗ ✗ ✓ ✓\nBeyond [78] ✗ ✗ ✗ ✓\nZodiac [26] ✗ ✗ ✗ ✓\nFhirviz [79] ✗ ✗ ✗ ✓\nFhir [80] ✗ ✗ ✗ ✓\nFuasagent [81] ✓ ✗ ✗ ∆\nAitherapist [82] ∆ ✗ ✓ ∆\nRadiology [83] ✗ ✗ ✗ ✓\nTriageagent [24] ✓ ✗ ✗ ✓\nPiors [84] ✗ ✗ ✓ ✓\nSelfevolve [85] ✗ ✗ ✓ ✓\nDrugagent [86] ✗ ✓ ✗ ✓\nCtagent [87] ✗ ✗ ✗ ✓\nAdagent [88] ✗ ✗ ✗ ✓\nDrugpilot [89] ∆ ✓ ✗ ✓\nLlmadmin [90] ✗ ✗ ∆ ∆\nSwarm [91] ✗ ✓ ✗ ✓\nDrugdiscover [92] ✗ ✓ ✗ ∆\nRagdrug [93] ∆ ✓ ✗ ✓\nDrugreason [94] ✗ ✓ ✗ ✓\nlimited to simulation or education with mandatory human\ninterpretation. ✗ is assigned in cases with no treatment rec-\nommendation or prescription management.\n6) Drug Discovery & Clinical Trial Design\nDrug Discovery & Clinical Trial Design gauges an agent’s\ncapacity to accelerate translational pipelines from target\nideation to study execution. Abilities include drug-target\ninteraction prediction, toxicity screening, hypothesis gener-\nation and lead optimization. Trial functions include protocol\n16 VOLUME 14, 2026\n\n-- 16 of 25 --\n\nauthoring, eligibility criteria extraction, patient-trial match-\ning, automated feature engineering and outcome prediction.\n✓ is warranted when either drug discovery or clinical trial\ndesign is realized. The system should experiment around task\nspecific benchmarks, tool integrated workflows and multi-tier\nvalidation. ∆ is used when functionality is narrow or up-\nstream only such as database lookups, molecular captioning\nor basic analytics. It does not include any form of end-to-\nend optimization, interaction with experimental constraints\nor trial support beyond templated criteria. ✗ is reserved for\nsystems with no discovery or trial capabilities.\n7) Patient Interaction & Monitoring\nPatient Interaction & Monitoring characterizes an agent’s pa-\ntient facing functionality across communication, monitoring\nand logistics. Capabilities include two-way messaging and\neducation. It also covers symptom checkers with targeted\nfollow-ups, adherence check-ins along with reminders, ap-\npointment scheduling, device integration for physiologic sig-\nnals and personalized conversation management. ✓ is earned\nwhen at least one of the following is implemented: inter-\naction, engagement, appointment management, reminders or\nmonitoring. Evidence may include realistic dialogue behav-\nior, symptom tracking with feedback collection, department\nsupport, real-time monitoring or stress estimation. Dynamic\nsymptom simulation and safe handoffs also meet this bar. ∆\napplies to limited functionality such as basic history taking,\nscripted exchanges or intake only prompts without person-\nalized monitoring. ✗ signals no patient facing interactions,\nreminders, scheduling or monitoring.\n8) Benchmarking & Simulation Environment\nBenchmarking & Simulation Environment validates whether\nan agent is tested under rigorous and reproducible condi-\ntions that reflect clinical work. Scope includes standardized\nbenchmarks with clear protocols and metrics. It also includes\nsimulations that model roles, artifacts (notes, images, vitals)\nand multi-turn workflows. Robust studies report comparisons\nto strong baselines, ablations, error analyses and when ap-\npropriate, human expert review. Accordingly, ✓ is used for\nevaluation on a substantial benchmark or a clinically realistic\nsimulation with transparent setup, baseline comparisons and\nablation studies. ∆ is used for narrow demos or synthetic\nmicrostudies with few samples and limited metrics. ✗ is\nassigned when no meaningful benchmarking or simulation\nis presented.\nIII. EMPIRICAL FINDINGS AND DISCUSSION\nIn this section, we discuss cross-cutting trends across the 7\nevaluation dimensions introduced earlier. The following sub-\nsections briefly surface where healthcare LLM agents cluster.\nFor example, the analysis reveals strong External Knowledge\nIntegration but weak Dynamic Updates & Forgetting within\nKnowledge Management, relatively prevalent Conversational\nMode but sparse Error Recovery within Interaction Patterns\nand overall low adoption of Adaptation & Learning. We fur-\nther explain why these patterns matter for clinical reliability\nand operational readiness. This section then converges on\ncritical weak points and possible actionable directions high-\nlighting where methodological investment is most urgent.\nA. WHICH COGNITIVE SUB-DIMENSIONS REMAIN\nUNDERDEVELOPED IN HEALTHCARE AGENTS?\nFigure 5 underscores marked asymmetries in implementation\nprevalence across Cognitive Capabilities among surveyed\nLLM-based agents. ✗ dominates three sub-dimensions with\nPlanning absent in ∼45% of the surveyed papers, Meta-\nCapabilities in ∼53% and Consistency & Conflict Resolu-\ntion in ∼61%. This conveys that many systems still lack\nrobust mechanisms for long horizon task decomposition,\nself-monitoring and systematic reconciliation of conflicting\nevidence. Contrarily, ✓ appears more frequently in Percep-\ntion (∼46%) and Action (∼41%) where progress has been\ndriven by multimodal representation learning and API or\ntool execution pipelines. ∆ occupies the middle in ways\nthat signal partial maturity rather than stability. For instance,\nPerception shows ∼55% ∆ whereas Action has only ∼8%\nstressing that prototypes do demonstrate potential but fall\nshort of reliable end-to-end integration. This pattern may be\nconsequential because cognitive competencies form the foun-\ndation of trustworthy clinical agents. Without planning com-\npetence, agents risk fragmented care pathways and missed\ncontingencies in triage or prior authorization. Weak meta-\ncontrol translates into poor uncertainty calibration and absent\nescalation triggers. Scarce conflict resolution pipelines invite\ncontradictory chart updates or unsafe recommendations. Ad-\nvances in perception and execution while necessary, address\nonly surface layers. Agents that faithfully ingest EHR data\nand generate structured outputs still fail if they cannot resolve\ncontradictions or adapt plans to evolving evidence. These\ngaps suggest candidate research directions starting with for-\nmalizing planning protocols aligned to clinical workflows,\ncontinuing with embedding uncertainty aware meta-control\nloops and finally completing with developing contradiction\ndetection pipelines.\nB. ARE KNOWLEDGE MANAGEMENT CAPABILITIES\nEVENLY IMPLEMENTED ACROSS HEALTHCARE LLM\nAGENTS?\nFigure 6 shows a stratified implementation pattern across\nthree sub-dimensions including External Knowledge Integra-\ntion, Memory Module and Dynamic Updates & Forgetting.\nExternal Knowledge Integration is the most frequent with ✓\non 37/49 (∼76%), ∆ on 1/49 (∼2%) and ✗ on 11/49 (∼22%).\nThese numbers emphasize widespread pairing of parametric\nrecall with authoritative external sources at inference time.\nMemory Module skews ∆ (∼49%) with fewer ✓ systems\n(∼33%) and the remainder ✗ (∼18%). Many agents keep\nsession history or simple buffers. Yet far fewer implement\ncontroller-governed, persistent, multi-tier memory with prin-\ncipled read/write. The most conspicuous weakness is Dy-\nnamic Updates & Forgetting, which records just 1/49 (∼2%)\nVOLUME 14, 2026 17\n\n-- 17 of 25 --\n\nFIGURE 5. Distribution of Labels Across Sub-Dimensions of Cognitive Capabilities\nFIGURE 6. Distribution of Labels Across Sub-Dimensions of Knowledge\nManagement\n✓ but 33/49 (∼67%) ✗. Very few systems focus on both\nsides of knowledge maintenance allowing continuous addi-\ntions and explicit forgetting via decay and deprecation rules.\nThis distribution has a high significance because knowledge\ncurrency, coherence and hygiene underpin safe clinical use.\nAgents must align outputs with evolving guidelines, drug\nrecalls and formulary changes while suppressing stale or\ncontradictory facts to prevent drift. This analysis further\nsheds light on the need of elevating memory from buffers\nto persistent substrates, coupling retrieval with multi-tier\nmemory and enforcing deprecation detection gates.\nC. DO CURRENT AGENTS BALANCE CONVERSATION,\nEVENT TRIGGERS, HUMAN-IN-THE-LOOP AND\nRECOVERY ADEQUATELY?\nFigure 7 maps implementation distribution across four In-\nteraction Patterns sub-dimensions which are Conversational\nMode, Event-Triggered Activation, Human-in-the-Loop and\nError Recovery. The distribution is heterogeneous but lop-\nsided in key areas. Conversational Mode is relatively preva-\nlent with ✓ on ∼43% of research works and ∆ on ∼6% of\nthem. This conveys that many systems support multi-turn ex-\nFIGURE 7. Distribution of Labels Across Sub-Dimensions of Interaction\nPatterns\nchange and state carryover though nearly half still fall short.\nEvent-Triggered Activation is starkly underrepresented. It\nhas only 3/49 (∼6%) ✓ and 1/49 (∼2%) ∆ versus 45/49\n(∼92%) ✗ suggesting heavy reliance on manual prompts\nrather than external signals such as device telemetry or EHR\nevents. Human-in-the-Loop records ∼12% ✓, ∼2% ∆ but\n∼86% ✗ reflecting pipelines that lack mid-execution check-\npoints and documented escalation paths. Error Recovery is\nsimilarly thin, with just ∼4% ✓. Only few works demonstrate\nidempotent tool use, rollbacks or safe mode degradation.\nFor clinical operations, we outline potential design reme-\ndies. First, tie activation to governed event streams with\nprovenance and debouncing. Next, define Human-in-the-\nLoop checkpoints as explicit gates with approval queues and\nuncertainty thresholds. Finally, treat recovery as a first class\nsubsystem with bounded retries and circuit breakers. These\ncontrols can turn interactions into reviewable workflows that\ncan in turn work toward satisfying clinical governance and\nreducing operational risk.\n18 VOLUME 14, 2026\n\n-- 18 of 25 --\n\nFIGURE 8. Distribution of Labels Across Sub-Dimensions of Adaptation &\nLearning\nD. HOW WELL DO HEALTHCARE LLM AGENTS ADAPT?\nFigure 8 shows uniformly low adoption across Adaptation\n& Learning. ✗ or Not Implemented is the prevailing label\nacross all three sub-dimensions with only thin tails of ∆\nand rare ✓ instances. Drift Detection & Mitigation is es-\nsentially absent 0% ✓, ∼2% (1/49) ∆ and ∼98% (48/49)\n✗. This essentially means most agents would not notice,\nlet alone correct shifts in codes, templates or patient mix.\nReinforcement-Based Adaptation appears only sporadically\nwith only 5 (∼10%) studies having ✓ and 3 (∼6%) having\n∆ against 41 (∼84%) having ✗ suggesting that outcome-\nlinked updates (rewards, constraints, clinician preferences)\nare rarely operationalized. Meta-Learning & Few-Shot shows\nthe best footing but remains far from routine with 20% ✓\nand 2% ∆ but 78% ✗. These numbers reveal that most\nsystems still rely on static prompts rather than lightweight\non the fly adaptation. Looking across these studies, three\nmissing pieces recur: (i) limited lifecycle instrumentation\n(few pipelines report drift alarm rates or rollback metrics)\n(ii) underuse of preference learning and outcome-linked re-\nwards beyond ad hoc feedback (iii) scarce end-to-end designs\nthat fuse few-shot competence with continuous monitoring.\nHealthcare is non-stationary by default and without detection\nand controlled response, performance quietly erodes where\nsafety and auditability matter most. In summary, today’s\nfigures with 0-20% ✓ ranges across sub-dimensions indicate\nislands of adaptation capability. A possible path forward is\nto stitch these islands into a monitored, reversible adaptation\nloop that can keep pace with evolving practice.\nE. SAFETY & ETHICS IMPLEMENTATION MATURITY\nREMAINS LIMITED AMONG SURVEYED AGENTS\nFigure 9 profiles four Safety & Ethics sub-dimensions Safety\nGuardrails & Adversarial Robustness, Bias & Fairness,\nPrivacy-Preserving Mechanisms and Regulatory & Compli-\nance Constraints. For Safety Guardrails & Adversarial Ro-\nbustness, only 3 out of 49 papers (∼6%) earn ✓, with 14\n(∼29%) ∆ and 32 (∼65%) ✗. This tells that most agents\nFIGURE 9. Distribution of Labels Across Sub-Dimensions of Safety & Ethics\nstill lack layered, testable defenses against prompt attacks\nor unsafe tool use. Bias & Fairness looks similar with 5\nstudies (∼10%) marked ✓, 12 with (∼25%) ∆ and the rest\n32 (∼65%) having ✗ signaling that cohort-stratified eval-\nuation and mitigation are exception rather than the norm.\nPrivacy-Preserving Mechanisms shows the strongest footing\nbut remains far from universal. Their numbers include ∼18%\n✓, ∼28% ∆ and ∼53% ✗. Many papers reference hosting\ncontrols, yet fewer demonstrate agent side protections with\nmeasurable leakage tests. Regulatory & Compliance Con-\nstraints is the most underbuilt with ∼10% ✓, only ∼4%\n∆ and ∼86% ✗ reflecting procedural assurances rarely ap-\npear as verifiable runtime gates. The results in Figure 9\ntherefore do more than describe who implemented what.\nThey reveal the gap between proof of concept engineering\nand the end-to-end assurance required for clinical adoption\nwith 53-86% ✗ across sub-dimensions. A possible remedial\ndirection could be: (i) convert filters into layered guardrails\nwith adversarial suites and hold-out hazard tests (ii) replace\nmacro averages with cohort-stratified metrics (iii) elevate\nprivacy to de-identification plus traceable access (iv) turn\ncompliance from policy text into executable controls wired\ninto the agent’s workflow. Closing these gaps can move the\nfield from intent to assurance and makes safety, equity and\nauditability measurable. In short, the prevalence statistics in\nFigure 9 highlight a gradient: strong conceptual intent, par-\ntial technical realization and limited end-to-end assurance.\nBridging that gradient can help deliver safe, equitable and\nauditable LLM agents operating in real clinical workflows.\nF. FRAMEWORK TYPOLOGY TRENDS ACROSS\nLLM-BASED HEALTHCARE AGENTS\nFigure 10 charts architectural choices within Framework\nTypology by tallying ✓, ∆ and ✗ across two sub-dimensions:\nMulti-Agent Design and Centralized Orchestration. The data\nappears to have a clear asymmetry. Multi-Agent Design is\nbroadly mature: ∼82% ✓ with only ∼16% ✗ indicating that\nrole-based compositions such as planner, retriever, verifier,\nexplainer are now the default pattern. Centralized Orches-\nVOLUME 14, 2026 19\n\n-- 19 of 25 --\n\nFIGURE 10. Distribution of Labels Across Sub-Dimensions of Framework\nTypology\ntration, meanwhile, clusters in partial territory with ∼57%\n∆. This suggests that while coordination layers exist, many\ndeployments stop short of fully realized controllers with\nglobal state, policy enforcement and auditable sequencing.\nThis is not a verdict on what architectures ought to be but\na snapshot of where engineering effort has concentrated.\nTeams have prioritized modular specialization allowing eas-\nier parallelization and component swaps before hardening\norchestration for end-to-end assurance. In practice, typol-\nogy choices shape behavior under load, evolution over time\nand the visibility of control. Multi-agent compositions can\nmirror multidisciplinary care by assigning distinct agents to\nguideline conformance, evidence synthesis and patient facing\ncommunication. A central orchestrator can then align these\nsteps with institutional policy, surface uncertainty, route edge\ncases to clinicians and reconcile outputs prior to order. Nei-\nther pattern is universally superior. Effective systems often\ncombine them or opt for lighter choreography when latency,\ncost or integration constraints make a heavyweight controller\nimpractical. Read in this light, Figure 10 indicates a field that\nhas rapidly embraced modular, role-based design (high ✓ in\nMulti-Agent Design) while progressively layering coordina-\ntion mechanisms (dominant ∆ in Centralized Orchestration)\nas deployments mature.\nG. WHICH CORE TASKS CAN HEALTHCARE AGENTS\nPERFORM?\nFigure 11 depicts how often each Core Tasks & Subtasks\nsub-dimension is rated Fully Implemented ✓, Partially Im-\nplemented ∆ or Not Implemented ✗. The spread highlights\nwhere healthcare LLM agents already provide dependable\nvalue versus where they remain immature. Clinical Docu-\nmentation & EHR Analysis is a solid performer with 22/49\n(∼45%) ✓, 12/49 (∼24%) ∆ and 15/49 (∼31%) ✗ reflecting\ntractable gains in summarization and extraction. Medical\nQuestion Answering & Decision Support is stronger at 28/49\n(∼57%) ✓, showing that retrieval-grounded recommenda-\ntions are a leading use case. Conversely, Triage & Differential\nDiagnosis and Diagnostic Reasoning cluster around partial\nadoption. Their statistics contain 14/49 (∼29%) and 18/49\n(∼37%) ✓ alongside 11/49 (∼22%) and 13/49 (∼27%) ∆\nrespectively. This confirms their consistency with higher de-\nmands of structured hypothesis management and calibrated\nuncertainty. Action oriented areas remain underdeveloped.\nFor example, Treatment Planning & Prescription is only\n10/49 (∼20%) ✓ and Patient Interaction & Monitoring is\n11/49 (∼22%) with both tilting heavily toward ✗ because of\nintegration and safety challenges. Drug Discovery & Clinical\nTrial Design records just 9/49 (∼18%) ✓ against 39/49\n(∼80%) ✗ exhibiting long evaluation horizons and regulatory\nburden. Benchmarking & Simulation Environment stands out\nwith 39/49 (∼80%) ✓ and only 1/49 (∼2%) ✗ marking broad\nadoption of standardized datasets and increasingly realis-\ntic simulators that boost comparability and iteration speed.\nOverall, interpreting the numbers in Figure 11 shows infor-\nmation centric tasks like documentation, decision support\nand benchmarking leading in maturity. On the other hand,\ntasks centered around decision and action remain Partially\nImplemented whereas discovery tasks lag furthest behind.\nThis pattern maps directly to clinical workflows. Medical\nrecord summarization and question answering are ready\nfor frontline utility but safe automation of judgment and\nlongitudinal interaction still requires substantial governance\nadvances.\nIV. CONCLUSION\nLLM-based agents are showing early gains in automation,\ndecision support and research, but wider clinical adoption\nremains limited until adaptation, safety and compliance are\nmore robust. In this survey, we systematically review 49\nresearch papers and propose an integrated taxonomy for\nempirical evaluation of these agents. Our taxonomy provides\na structured lens to examine the cognitive, interactive, adap-\ntive and ethical aspects of Agentic AI-grounded LLM-based\nagents along with their core tasks and framework architec-\ntures. Through this multidimensional analysis, we surface a\nvariety of prevalent patterns showing methodological prefer-\nences, capability distributions and emerging design strategies\nwithin healthcare and medicine. While the field has made\nsignificant strides, several challenges still persist. These\nchallenges particularly revolve around ensuring reliability,\ninterpretability and ethical alignment in complex healthcare\nenvironments. By consolidating current methodologies and\nhighlighting key trends, our review offers a foundational re-\nsource for researchers aiming to advance the development of\nrobust, efficient and ethically responsible LLM-based agents.\nFuture research can build upon this framework to explore\nunderrepresented dimensions, refine agent capabilities and\naddress domain specific needs. This would ultimately con-\ntribute towards more reliable, safe and effective applications\nof LLM-based agents in healthcare and medicine. Finally,\nwhile scaling agentic systems from research prototypes to\nproduction involves orchestration, integration, latency/cost\nand reliability concerns, such deployment-scale engineering\n20 VOLUME 14, 2026\n\n-- 20 of 25 --\n\nFIGURE 11. Distribution of Labels Across Sub-Dimensions of Core Tasks & Subtasks\nlies beyond the scope of this work. We flag scalability as an\nopen challenge and an important axis for future work but\nour evidence and analysis are intentionally limited to peer-\nreviewed/archival research papers rather than production sys-\ntems.\nREFERENCES\n[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman,\nD. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “Gpt-4\ntechnical report,” arXiv preprint arXiv:2303.08774, 2023.\n[2] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\nP. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al., “Palm: Scaling\nlanguage modeling with pathways,” Journal of Machine Learning Re-\nsearch, vol. 24, no. 240, pp. 1–113, 2023.\n[3] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le,\nD. Zhou et al., “Chain-of-thought prompting elicits reasoning in large\nlanguage models,” Advances in neural information processing systems,\nvol. 35, pp. 24 824–24 837, 2022.\n[4] R. Yang, T. F. Tan, W. Lu, A. J. Thirunavukarasu, D. S. W. Ting, and\nN. Liu, “Large language models in health care: Development, applica-\ntions, and challenges,” Health Care Science, vol. 2, no. 4, pp. 255–263,\n2023.\n[5] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F.\nTan, and D. S. W. Ting, “Large language models in medicine,” Nature\nmedicine, vol. 29, no. 8, pp. 1930–1940, 2023.\n[6] Y. Li, S. Wang, H. Ding, and H. Chen, “Large language models in finance:\nA survey,” in Proceedings of the fourth ACM international conference on\nAI in finance, 2023, pp. 374–382.\n[7] L. Yan, L. Sha, L. Zhao, Y. Li, R. Martinez-Maldonado, G. Chen, X. Li,\nY. Jin, and D. Gaševi´c, “Practical and ethical challenges of large language\nmodels in education: A systematic scoping review,” British Journal of\nEducational Technology, vol. 55, no. 1, pp. 90–112, 2024.\n[8] S. Schulhoff, M. Ilie, N. Balepur, K. Kahadze, A. Liu, C. Si, Y. Li,\nA. Gupta, H. Han, S. Schulhoff et al., “The prompt report: a sys-\ntematic survey of prompt engineering techniques,” arXiv preprint\narXiv:2406.06608, 2024.\n[9] J. Gu, Z. Han, S. Chen, A. Beirami, B. He, G. Zhang, R. Liao, Y. Qin,\nV. Tresp, and P. Torr, “A systematic survey of prompt engineering on\nvision-language foundation models,” arXiv preprint arXiv:2307.12980,\n2023.\n[10] P. Sahoo, A. K. Singh, S. Saha, V. Jain, S. Mondal, and A. Chadha,\n“A systematic survey of prompt engineering in large language models:\nTechniques and applications,” arXiv preprint arXiv:2402.07927, 2024.\n[11] S. Vatsal and H. Dubey, “A survey of prompt engineering meth-\nods in large language models for different nlp tasks,” arXiv preprint\narXiv:2407.12994, 2024.\n[12] S. Vatsal, H. Dubey, and A. Singh, “Multilingual prompt engineering\nin large language models: A survey across nlp tasks,” arXiv preprint\narXiv:2505.11665, 2025.\n[13] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang,\nS. Jin, E. Zhou et al., “The rise and potential of large language model\nbased agents: A survey,” Science China Information Sciences, vol. 68,\nno. 2, p. 121101, 2025.\n[14] H. Leong, Y. Gao, S. Ji, Y. Zhang, and U. Pamuksuz, “Efficient fine-\ntuning of large language models for automated medical documentation,”\nin 2024 4th International Conference on Digital Society and Intelligent\nSystems (DSInS). IEEE, 2024, pp. 204–209.\n[15] H. P. Baker, E. Dwyer, S. Kalidoss, K. Hynes, J. Wolf, and J. A.\nStrelzow, “Chatgpt’s ability to assist with clinical documentation: a\nrandomized controlled trial,” JAAOS-Journal of the American Academy\nof Orthopaedic Surgeons, vol. 32, no. 3, pp. 123–129, 2024.\n[16] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung,\nN. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl et al., “Large language\nmodels encode clinical knowledge,” Nature, vol. 620, no. 7972, pp. 172–\n180, 2023.\n[17] S. Vatsal and A. Singh, “Can gpt redefine medical understanding? evalu-\nating gpt on biomedical machine reading comprehension,” arXiv preprint\narXiv:2405.18682, 2024.\n[18] S. Vatsal, A. Singh, and S. Tafreshi, “Can gpt improve the state of prior\nauthorization via guideline based automated question answering?” in\nAI for Health Equity and Fairness: Leveraging AI to Address Social\nDeterminants of Health. Springer, 2024, pp. 147–158.\n[19] S. Wang, Z. Zhao, X. Ouyang, T. Liu, Q. Wang, and D. Shen, “Inter-\nactive computer-aided diagnosis on medical image using large language\nmodels,” Communications Engineering, vol. 3, no. 1, p. 133, 2024.\n[20] H. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz,\n“Capabilities of gpt-4 on medical challenge problems,” arXiv preprint\narXiv:2303.13375, 2023.\n[21] C. Chakraborty, M. Bhattacharya, and S.-S. Lee, “Artificial intelligence\nenabled chatgpt and large language models in drug target discovery, drug\ndiscovery, and development,” Molecular therapy Nucleic acids, vol. 33,\npp. 866–868, 2023.\n[22] C. R. Subramanian, D. A. Yang, and R. Khanna, “Enhancing health care\ncommunication with large language models—the role, challenges, and\nfuture directions,” JAMA network open, vol. 7, no. 3, pp. e240 347–\ne240 347, 2024.\n[23] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and\nK. Narasimhan, “Tree of thoughts: Deliberate problem solving with large\nlanguage models, 2023,” URL https://arxiv. org/abs/2305.10601, vol. 3,\np. 1, 2023.\n[24] M. Lu, B. Ho, D. Ren, and X. Wang, “Triageagent: Towards better multi-\nagents collaborations for large language model-based clinical triage,”\nin Findings of the Association for Computational Linguistics: EMNLP\n2024, 2024, pp. 5747–5764.\n[25] Z. Wang, Y. Zhu, H. Zhao, X. Zheng, D. Sui, T. Wang, W. Tang,\nVOLUME 14, 2026 21\n\n-- 21 of 25 --\n\nY. Wang, E. Harrison, C. Pan et al., “Colacare: Enhancing electronic\nhealth record modeling through large language model-driven multi-agent\ncollaboration,” in Proceedings of the ACM on Web Conference 2025,\n2025, pp. 2250–2261.\n[26] Y. Zhou, P. Zhang, M. Song, A. Zheng, Y. Lu, Z. Liu, Y. Chen, and\nZ. Xi, “Zodiac: A cardiologist-level llm framework for multi-agent\ndiagnostics,” arXiv preprint arXiv:2410.02026, 2024.\n[27] P. Li, X. Zhang, E. Zhu, S. Yu, B. Sheng, Y. C. Tham, T. Y. Wong,\nand H. Ji, “Potential multidisciplinary use of large language models for\naddressing queries in cardio-oncology,” Journal of the American Heart\nAssociation, vol. 13, no. 6, p. e033584, 2024.\n[28] T. Guo, X. Chen, Y. Wang, R. Chang, S. Pei, N. V. Chawla, O. Wiest,\nand X. Zhang, “Large language model based multi-agents: A survey of\nprogress and challenges,” arXiv preprint arXiv:2402.01680, 2024.\n[29] M. A. Ferrag, N. Tihanyi, and M. Debbah, “From llm reasoning\nto autonomous ai agents: A comprehensive review,” arXiv preprint\narXiv:2504.19678, 2025.\n[30] X. Dong, X. Zhang, W. Bu, D. Zhang, and F. Cao, “A survey of llm-\nbased agents: Theories, technologies, applications and suggestions,” in\n2024 3rd International Conference on Artificial Intelligence, Internet of\nThings and Cloud Computing Technology (AIoTC). IEEE, 2024, pp.\n407–413.\n[31] X. Li, S. Wang, S. Zeng, Y. Wu, and Y. Yang, “A survey on llm-\nbased multi-agent systems: workflow, infrastructure, and challenges,”\nVicinagearth, vol. 1, no. 1, p. 9, 2024.\n[32] A. Plaat, M. van Duijn, N. van Stein, M. Preuss, P. van der Putten,\nand K. J. Batenburg, “Agentic large language models, a survey,” arXiv\npreprint arXiv:2503.23037, 2025.\n[33] P. Zhao, Z. Jin, and N. Cheng, “An in-depth survey of large language\nmodel-based artificial intelligence agents. arxiv (2023),” arXiv preprint\narXiv:2309.14365, 2023.\n[34] W. Wang, Z. Ma, Z. Wang, C. Wu, J. Ji, W. Chen, X. Li, and Y. Yuan, “A\nsurvey of llm-based agents in medicine: How far are we from baymax?”\narXiv preprint arXiv:2502.11211, 2025.\n[35] M. Yu, F. Meng, X. Zhou, S. Wang, J. Mao, L. Pang, T. Chen, K. Wang,\nX. Li, Y. Zhang et al., “A survey on trustworthy llm agents: Threats and\ncountermeasures,” arXiv preprint arXiv:2503.09648, 2025.\n[36] Z. Zhang, X. Bo, C. Ma, R. Li, X. Chen, Q. Dai, J. Zhu, Z. Dong, and\nJ.-R. Wen, “A survey on the memory mechanism of large language model\nbased agents, 2024,” URL https://arxiv. org/abs/2404.13501.\n[37] X. Li, “A review of prominent paradigms for llm-based agents: Tool use,\nplanning (including rag), and feedback learning,” in Proceedings of the\n31st International Conference on Computational Linguistics, 2025, pp.\n9760–9779.\n[38] X. Huang, W. Liu, X. Chen, X. Wang, H. Wang, D. Lian, Y. Wang,\nR. Tang, and E. Chen, “Understanding the planning of llm agents: A\nsurvey,” arXiv preprint arXiv:2402.02716, 2024.\n[39] A. Yehudai, L. Eden, A. Li, G. Uziel, Y. Zhao, R. Bar-Haim, A. Cohan,\nand M. Shmueli-Scheuer, “Survey on evaluation of llm-based agents,”\narXiv preprint arXiv:2503.16416, 2025.\n[40] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K.-W. Lee, and E.-P.\nLim, “Plan-and-solve prompting: Improving zero-shot chain-of-thought\nreasoning by large language models,” arXiv preprint arXiv:2305.04091,\n2023.\n[41] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc,\nA. Mensch, K. Millican, M. Reynolds et al., “Flamingo: a visual language\nmodel for few-shot learning,” Advances in neural information processing\nsystems, vol. 35, pp. 23 716–23 736, 2022.\n[42] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, M. Amin, L. Hou,\nK. Clark, S. R. Pfohl, H. Cole-Lewis et al., “Toward expert-level med-\nical question answering with large language models,” Nature Medicine,\nvol. 31, no. 3, pp. 943–950, 2025.\n[43] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao,\n“React: Synergizing reasoning and acting in language models,” in Inter-\nnational Conference on Learning Representations (ICLR), 2023.\n[44] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, E. Hambro,\nL. Zettlemoyer, N. Cancedda, and T. Scialom, “Toolformer: Language\nmodels can teach themselves to use tools,” Advances in Neural Informa-\ntion Processing Systems, vol. 36, pp. 68 539–68 551, 2023.\n[45] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, “Reflex-\nion: Language agents with verbal reinforcement learning,” Advances in\nNeural Information Processing Systems, vol. 36, pp. 8634–8652, 2023.\n[46] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdh-\nery, and D. Zhou, “Self-consistency improves chain of thought reasoning\nin language models,” arXiv preprint arXiv:2203.11171, 2022.\n[47] Q. Wu, G. Bansal, J. Zhang, Y. Wu, B. Li, E. Zhu, L. Jiang, X. Zhang,\nS. Zhang, J. Liu et al., “Autogen: Enabling next-gen llm applications via\nmulti-agent conversations,” in First Conference on Language Modeling,\n2024.\n[48] P. Manakul, A. Liusie, and M. J. Gales, “Selfcheckgpt: Zero-resource\nblack-box hallucination detection for generative large language models,”\narXiv preprint arXiv:2303.08896, 2023.\n[49] X. Tang, A. Zou, Z. Zhang, Z. Li, Y. Zhao, X. Zhang, A. Cohan, and\nM. Gerstein, “Medagents: Large language models as collaborators for\nzero-shot medical reasoning,” arXiv preprint arXiv:2311.10537, 2023.\n[50] D. Yang, J. Wei, M. Li, J. Liu, L. Liu, M. Hu, J. He, Y. Ju, W. Zhou, Y. Liu\net al., “Medaide: Information fusion and anatomy of medical intents via\nllm-based agent collaboration,” arXiv e-prints, pp. arXiv–2410, 2024.\n[51] Y. Feng, J. Wang, L. Zhou, and Y. Li, “Doctoragent-rl: A multi-agent\ncollaborative reinforcement learning system for multi-turn clinical dia-\nlogue,” arXiv preprint arXiv:2505.19630, 2025.\n[52] Z. Fan, J. Tang, W. Chen, S. Wang, Z. Wei, J. Xi, F. Huang, and J. Zhou,\n“Ai hospital: Benchmarking large language models in a multi-agent\nmedical interaction simulator,” arXiv preprint arXiv:2402.09742, 2024.\n[53] B. Li, T. Yan, Y. Pan, J. Luo, R. Ji, J. Ding, Z. Xu, S. Liu, H. Dong,\nZ. Lin et al., “Mmedagent: Learning to use medical tools with multi-\nmodal agent,” arXiv preprint arXiv:2407.02483, 2024.\n[54] P. R. Liu, S. Bansal, J. Dinh, A. Pawar, R. Satishkumar, S. Desai,\nN. Gupta, X. Wang, and S. Hu, “Medchat: A multi-agent framework\nfor multimodal diagnosis with large language models,” arXiv preprint\narXiv:2506.07400, 2025.\n[55] H. Wei, J. Qiu, H. Yu, and W. Yuan, “Medco: Medical education copilots\nbased on a multi-agent framework,” in European Conference on Com-\nputer Vision. Springer, 2024, pp. 119–135.\n[56] Z. Wang, J. Wu, L. Cai, C. H. Low, X. Yang, Q. Li, and Y. Jin,\n“Medagent-pro: Towards evidence-based multi-modal medical diagnosis\nvia reasoning agentic workflow,” arXiv preprint arXiv:2503.18968, 2025.\n[57] L. Yue, S. Xing, J. Chen, and T. Fu, “Clinicalagent: Clinical trial\nmulti-agent system with large language model-based reasoning,” in Pro-\nceedings of the 15th ACM International Conference on Bioinformatics,\nComputational Biology and Health Informatics, 2024, pp. 1–10.\n[58] S. Han and W. Choi, “Development of a large language model-based\nmulti-agent clinical decision support system for korean triage and acuity\nscale (ktas)-based triage and treatment planning in emergency depart-\nments,” arXiv preprint arXiv:2408.07531, 2024.\n[59] Q. Jin, Z. Wang, Y. Yang, Q. Zhu, D. Wright, T. Huang, W. J. Wilbur,\nZ. He, A. Taylor, Q. Chen et al., “Agentmd: Empowering language agents\nfor risk prediction with large-scale clinical tool learning,” arXiv preprint\narXiv:2402.13225, 2024.\n[60] Y. Kim, C. Park, H. Jeong, Y. S. Chan, X. Xu, D. McDuff, H. Lee,\nM. Ghassemi, C. Breazeal, and H. W. Park, “Mdagents: An adaptive\ncollaboration of llms for medical decision-making,” Advances in Neural\nInformation Processing Systems, vol. 37, pp. 79 410–79 452, 2024.\n[61] J. Li, Y. Lai, W. Li, J. Ren, M. Zhang, X. Kang, S. Wang, P. Li, Y.-Q.\nZhang, W. Ma et al., “Agent hospital: A simulacrum of hospital with\nevolvable medical agents,” arXiv preprint arXiv:2405.02957, 2024.\n[62] B. Gao, Y. Huang, Y. Liu, W. Xie, W.-Y. Ma, Y.-Q. Zhang, and Y. Lan,\n“Pharmagents: Building a virtual pharma with large language model\nagents,” arXiv preprint arXiv:2503.22164, 2025.\n[63] S. Gao, R. Zhu, Z. Kong, A. Noori, X. Su, C. Ginder, T. Tsiligkaridis,\nand M. Zitnik, “Txagent: An ai agent for therapeutic reasoning across a\nuniverse of tools,” arXiv preprint arXiv:2503.10970, 2025.\n[64] F. Liu, H. Wang, J. Cho, D. Roth, and A. W. Lo, “Autoct: Automating\ninterpretable clinical trial prediction with llm agents,” arXiv preprint\narXiv:2506.04293, 2025.\n[65] K. Zuo, Y. Jiang, F. Mo, and P. Lio, “Kg4diagnosis: A hierarchical multi-\nagent llm framework with knowledge graph enhancement for medical\ndiagnosis,” in AAAI Bridge Program on AI for Medicine and Healthcare.\nPMLR, 2025, pp. 195–204.\n[66] K. Chen, X. Li, T. Yang, H. Wang, W. Dong, and Y. Gao, “Mdteamgpt:\nA self-evolving llm-based multi-agent framework for multi-disciplinary\nteam medical consultation,” arXiv preprint arXiv:2503.13856, 2025.\n[67] Y. Ke, R. Yang, S. A. Lie, T. X. Y. Lim, Y. Ning, I. Li, H. R. Abdullah,\nD. S. W. Ting, and N. Liu, “Mitigating cognitive biases in clinical\ndecision-making through multi-agent conversations using large language\n22 VOLUME 14, 2026\n\n-- 22 of 25 --\n\nmodels: simulation study,” Journal of Medical Internet Research, vol. 26,\np. e59439, 2024.\n[68] S. Mukherjee, P. Gamble, M. S. Ausin, N. Kant, K. Aggarwal, N. Man-\njunath, D. Datta, Z. Liu, J. Ding, S. Busacca et al., “Polaris: A safety-\nfocused llm constellation architecture for healthcare,” arXiv preprint\narXiv:2403.13313, 2024.\n[69] P. P. Van, D. N. Minh, A. D. Ngoc, and H. P. Thanh, “Rx strate-\ngist: Prescription verification using llm agents system,” arXiv preprint\narXiv:2409.03440, 2024.\n[70] W. Yan, H. Liu, T. Wu, Q. Chen, W. Wang, H. Chai, J. Wang, W. Zhao,\nY. Zhang, R. Zhang et al., “Clinicallab: Aligning agents for multi-\ndepartmental clinical diagnostics in the real world,” arXiv preprint\narXiv:2406.13890, 2024.\n[71] M. Sudarshan, S. Shih, E. Yee, A. Yang, J. Zou, C. Chen, Q. Zhou,\nL. Chen, C. Singhal, and G. Shih, “Agentic llm workflows for generat-\ning patient-friendly medical reports,” arXiv preprint arXiv:2408.01112,\n2024.\n[72] J. Wu, X. Liang, X. Bai, and Z. Chen, “Surgbox: Agent-driven operating\nroom sandbox with surgery copilot,” in 2024 IEEE International Confer-\nence on Big Data (BigData). IEEE, 2024, pp. 2041–2048.\n[73] H. Yu, J. Zhou, L. Li, S. Chen, J. Gallifant, A. Shi, X. Li, J. He, W. Hua,\nM. Jin et al., “Simulated patient systems are intelligent when powered by\nlarge language model-based ai agents,” arXiv preprint arXiv:2409.18924,\n2024.\n[74] M. Abbasian, I. Azimi, A. M. Rahmani, and R. Jain, “Conversational\nhealth agents: A personalized llm-powered agent framework,” arXiv\npreprint arXiv:2310.02374, 2023.\n[75] R. Li, X. Wang, and H. Yu, “Exploring llm multi-agents for icd coding,”\narXiv preprint arXiv:2406.15363, 2024.\n[76] H. Pandey, A. Amod et al., “Advancing healthcare automation:\nMulti-agent system for medical necessity justification,” arXiv preprint\narXiv:2404.17977, 2024.\n[77] Z. Jia, M. Jia, J. Duan, and J. Wang, “Ddo: Dual-decision optimization\nvia multi-agent collaboration for llm-based medical consultation,” arXiv\npreprint arXiv:2505.18630, 2025.\n[78] H. Wang, S. Zhao, Z. Qiang, N. Xi, B. Qin, and T. Liu, “Beyond direct\ndiagnosis: Llm-based multi-specialist agent consultation for automatic\ndiagnosis,” arXiv preprint arXiv:2401.16107, 2024.\n[79] M. ALMutairi, L. AlKulaib, S. Wang, Z. Chen, Y. ALMutairi, T. M.\nAlenazi, K. Luther, and C.-T. Lu, “Fhirviz: Multi-agent platform for\nfhir visualization to advance healthcare analytics,” in Proceedings of the\n15th ACM International Conference on Bioinformatics, Computational\nBiology and Health Informatics, 2024, pp. 1–7.\n[80] C. De Maio, G. Fenza, D. Furno, T. Grauso, and V. Loia, “A multi-agent\narchitecture for privacy-preserving natural language interaction with\nfhir-based electronic health records,” in 2024 International Conference\non Software, Telecommunications and Computer Networks (SoftCOM).\nIEEE, 2024, pp. 1–6.\n[81] L. Zhao, J. Bai, Z. Bian, Q. Chen, Y. Li, G. Li, M. He, H. Yao, and\nZ. Zhang, “Autonomous multi-modal llm agents for treatment planning in\nfocused ultrasound ablation surgery,” arXiv preprint arXiv:2505.21418,\n2025.\n[82] R. Wasenmüller, K. Hilbert, and C. Benzmüller, “Script-based dialog pol-\nicy planning for llm-powered conversational agents: A basic architecture\nfor an\" ai therapist\",” arXiv preprint arXiv:2412.15242, 2024.\n[83] F. Zeng, Z. Lyu, Q. Li, and X. Li, “Enhancing llms for impression\ngeneration in radiology reports through a multi-agent system,” arXiv\npreprint arXiv:2412.06828, 2024.\n[84] Z. Bao, Q. Liu, Y. Guo, Z. Ye, J. Shen, S. Xie, J. Peng, X. Huang,\nand Z. Wei, “Piors: Personalized intelligent outpatient reception based\non large language model with multi-agents medical scenario simulation,”\narXiv preprint arXiv:2411.13902, 2024.\n[85] M. Almansoori, K. Kumar, and H. Cholakkal, “Self-evolving multi-\nagent simulations for realistic clinical interactions,” arXiv preprint\narXiv:2503.22678, 2025.\n[86] S. Liu, Y. Lu, S. Chen, X. Hu, J. Zhao, Y. Lu, and Y. Zhao, “Drugagent:\nAutomating ai-aided drug discovery programming through llm multi-\nagent collaboration,” arXiv preprint arXiv:2411.15692, 2024.\n[87] Y. Mao, W. Xu, Y. Qin, and Y. Gao, “Ct-agent: A multimodal-\nllm agent for 3d ct radiology question answering,” arXiv preprint\narXiv:2505.16229, 2025.\n[88] W. Hou, G. Yang, Y. Du, Y. Lau, L. Liu, J. He, L. Long, and S. Wang,\n“Adagent: Llm agent for alzheimer’s disease analysis with collaborative\ncoordinator,” arXiv preprint arXiv:2506.11150, 2025.\n[89] K. Li, Z. Wu, S. Wang, J. Wu, S. Pan, and W. Hu, “Drugpilot: Llm-\nbased parameterized reasoning agent for drug discovery,” arXiv preprint\narXiv:2505.13940, 2025.\n[90] S. A. Gebreab, K. Salah, R. Jayaraman, M. H. ur Rehman, and S. El-\nlaham, “Llm-based framework for administrative task automation in\nhealthcare,” in 2024 12th International Symposium on Digital Forensics\nand Security (ISDFS). IEEE, 2024, pp. 1–7.\n[91] K. Song, A. Trotter, and J. Y. Chen, “Llm agent swarm for hypothesis-\ndriven drug discovery,” arXiv preprint arXiv:2504.17967, 2025.\n[92] J. Ock, R. S. Meda, S. Badrinarayanan, N. S. Aluru, A. Chandrasekhar,\nand A. B. Farimani, “Large language model agent for modular task\nexecution in drug discovery,” arXiv preprint arXiv:2507.02925, 2025.\n[93] N. Lee, E. De Brouwer, E. Hajiramezanali, T. Biancalani, C. Park, and\nG. Scalia, “Rag-enhanced collaborative llm agents for drug discovery,”\narXiv preprint arXiv:2502.17506, 2025.\n[94] Y. Inoue, T. Song, X. Wang, A. Luna, and T. Fu, “Drugagent: Multi-\nagent large language model-based reasoning for drug-target interaction\nprediction,” ArXiv, pp. arXiv–2408, 2025.\n[95] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,\nH. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel et al., “Retrieval-\naugmented generation for knowledge-intensive nlp tasks,” Advances in\nneural information processing systems, vol. 33, pp. 9459–9474, 2020.\n[96] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang, “Retrieval augmented\nlanguage model pre-training,” in International conference on machine\nlearning. PMLR, 2020, pp. 3929–3938.\n[97] U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis,\n“Generalization through memorization: Nearest neighbor language mod-\nels,” arXiv preprint arXiv:1911.00172, 2019.\n[98] C. Packer, V. Fang, S. Patil, K. Lin, S. Wooders, and J. Gonzalez,\n“Memgpt: Towards llms as operating systems.” 2023.\n[99] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi, “Self-rag: Learning\nto retrieve, generate, and critique through self-reflection,” 2024.\n[100] K. Meng, A. S. Sharma, A. Andonian, Y. Belinkov, and D. Bau, “Mass-\nediting memory in a transformer,” arXiv preprint arXiv:2210.07229,\n2022.\n[101] O. Shaikh, K. Gligori´c, A. Khetan, M. Gerstgrasser, D. Yang, and\nD. Jurafsky, “Grounding gaps in language model generations,” arXiv\npreprint arXiv:2311.09144, 2023.\n[102] A. Mahmood, J. Wang, B. Yao, D. Wang, and C.-M. Huang,\n“Llm-powered conversational voice assistants: Interaction patterns,\nopportunities, challenges, and design guidelines,” arXiv preprint\narXiv:2309.13879, 2023.\n[103] C. J. Cai, E. Reif, N. Hegde, J. Hipp, B. Kim, D. Smilkov, M. Wattenberg,\nF. Viegas, G. S. Corrado, M. C. Stumpe et al., “Human-centered tools for\ncoping with imperfect algorithms during medical decision-making,” in\nProceedings of the 2019 chi conference on human factors in computing\nsystems, 2019, pp. 1–14.\n[104] S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz, and\nJ. Weston, “Chain-of-verification reduces hallucination in large language\nmodels,” arXiv preprint arXiv:2309.11495, 2023.\n[105] G. Gupta, B. Rastegarpanah, A. Iyer, J. Rubin, and K. Kenthapadi,\n“Measuring distributional shifts in text: the advantage of language model-\nbased embeddings,” arXiv preprint arXiv:2312.02337, 2023.\n[106] S. Abdelnabi, A. Fay, G. Cherubin, A. Salem, M. Fritz, and A. Paverd,\n“Get my drift? catching llm task drift with activation deltas,” in\n2025 IEEE Conference on Secure and Trustworthy Machine Learning\n(SaTML). IEEE, 2025, pp. 43–67.\n[107] L. Richter, X. He, P. Minervini, and M. J. Kusner, “An auditing\ntest to detect behavioral shift in language models,” arXiv preprint\narXiv:2410.19406, 2024.\n[108] L. L. Guo, S. R. Pfohl, J. Fries, A. E. Johnson, J. Posada, C. Aftandilian,\nN. Shah, and L. Sung, “Evaluation of domain generalization and adapta-\ntion on improving model robustness to temporal dataset shift in clinical\nmedicine,” Scientific reports, vol. 12, no. 1, p. 2726, 2022.\n[109] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen,\nA. Goldie, A. Mirhoseini, C. McKinnon et al., “Constitutional ai: Harm-\nlessness from ai feedback,” arXiv preprint arXiv:2212.08073, 2022.\n[110] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for\nfast adaptation of deep networks,” in International conference on machine\nlearning. PMLR, 2017, pp. 1126–1135.\n[111] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language models\nare few-shot learners,” Advances in neural information processing sys-\ntems, vol. 33, pp. 1877–1901, 2020.\nVOLUME 14, 2026 23\n\n-- 23 of 25 --\n\n[112] P. Chao, A. Robey, E. Dobriban, H. Hassani, G. J. Pappas, and E. Wong,\n“Jailbreaking black box large language models in twenty queries,” in\n2025 IEEE Conference on Secure and Trustworthy Machine Learning\n(SaTML). IEEE, 2025, pp. 23–42.\n[113] X. Liu, Z. Yu, Y. Zhang, N. Zhang, and C. Xiao, “Automatic and universal\nprompt injection attacks against large language models,” arXiv preprint\narXiv:2403.04957, 2024.\n[114] S. Yi, Y. Liu, Z. Sun, T. Cong, X. He, J. Song, K. Xu, and Q. Li, “Jailbreak\nattacks and defenses against large language models: A survey,” arXiv\npreprint arXiv:2407.04295, 2024.\n[115] A. Purpura, S. Wadhwa, J. Zymet, A. Gupta, A. Luo, M. K. Rad,\nS. Shinde, and M. S. Sorower, “Building safe genai applications: An\nend-to-end overview of red teaming for large language models,” arXiv\npreprint arXiv:2503.01742, 2025.\n[116] I. O. Gallegos, R. A. Rossi, J. Barrow, M. M. Tanjim, S. Kim, F. Der-\nnoncourt, T. Yu, R. Zhang, and N. K. Ahmed, “Bias and fairness in large\nlanguage models: A survey,” Computational Linguistics, vol. 50, no. 3,\npp. 1097–1179, 2024.\n[117] Y. Li, M. Du, R. Song, X. Wang, and Y. Wang, “A survey on fairness in\nlarge language models,” arXiv preprint arXiv:2308.10149, 2023.\n[118] Z. Obermeyer, B. Powers, C. Vogeli, and S. Mullainathan, “Dissecting\nracial bias in an algorithm used to manage the health of populations,”\nScience, vol. 366, no. 6464, pp. 447–453, 2019.\n[119] F. Mireshghallah, K. Goyal, A. Uniyal, T. Berg-Kirkpatrick, and\nR. Shokri, “Quantifying privacy risks of masked language models using\nmembership inference attacks,” arXiv preprint arXiv:2203.03929, 2022.\n[120] N. Rieke, J. Hancox, W. Li, F. Milletari, H. R. Roth, S. Albarqouni,\nS. Bakas, M. N. Galtier, B. A. Landman, K. Maier-Hein et al., “The\nfuture of digital health with federated learning,” NPJ digital medicine,\nvol. 3, no. 1, p. 119, 2020.\n[121] H. Van Kolfschooten and J. Van Oirschot, “The eu artificial intelligence\nact (2024): implications for healthcare,” Health Policy, vol. 149, p.\n105152, 2024.\n[122] V. Muralidharan, B. A. Adewale, C. J. Huang, M. T. Nta, P. O. Ademiju,\nP. Pathmarajah, M. K. Hang, O. Adesanya, R. O. Abdullateef, A. O.\nBabatunde et al., “A scoping review of reporting gaps in fda-approved\nai medical devices,” NPJ Digital Medicine, vol. 7, no. 1, p. 273, 2024.\n[123] X. Chen, H. Yi, M. You, W. Liu, L. Wang, H. Li, X. Zhang, Y. Guo,\nL. Fan, G. Chen et al., “Enhancing diagnostic capability with multi-\nagents conversational large language models,” NPJ digital medicine,\nvol. 8, no. 1, p. 159, 2025.\n[124] W. Zhang, C. Cui, Y. Zhao, R. Hu, Y. Liu, Y. Zhou, and B. An, “Agen-\ntorchestra: A hierarchical multi-agent framework for general-purpose\ntask solving,” arXiv preprint arXiv:2506.12508, 2025.\n[125] Y. Yang, H. Chai, S. Shao, Y. Song, S. Qi, R. Rui, and W. Zhang,\n“Agentnet: Decentralized evolutionary coordination for llm-based multi-\nagent systems,” arXiv preprint arXiv:2504.00587, 2025.\n[126] W. Shi, R. Xu, Y. Zhuang, Y. Yu, J. Zhang, H. Wu, Y. Zhu, J. Ho, C. Yang,\nand M. D. Wang, “Ehragent: Code empowers large language models\nfor few-shot complex tabular reasoning on electronic health records,”\nin Proceedings of the Conference on Empirical Methods in Natural\nLanguage Processing. Conference on Empirical Methods in Natural\nLanguage Processing, vol. 2024, 2024, p. 22315.\n[127] D. Rose, C.-C. Hung, M. Lepri, I. Alqassem, K. Gashteovski,\nand C. Lawrence, “Meddxagent: A unified modular agent frame-\nwork for explainable automatic differential diagnosis,” arXiv preprint\narXiv:2502.19175, 2025.\n[128] R. C. Deo, S. Goto, T. Jain, S. Meier, and R. Patel, “A conversational\nartificial intelligence agent for medication reconciliation and review,”\nmedRxiv, pp. 2025–06, 2025.\n[129] A. M. Bran, S. Cox, O. Schilter, C. Baldassari, A. D. White, and\nP. Schwaller, “Augmenting large language models with chemistry tools,”\nNature Machine Intelligence, vol. 6, no. 5, pp. 525–535, 2024.\n[130] H. Shi, J. Zhang, and K. Zhang, “Enhancing clinical trial patient match-\ning through knowledge augmentation and reasoning with multi-agents,”\narXiv preprint arXiv:2411.14637, 2024.\n[131] Q. Jin, Z. Wang, C. S. Floudas, F. Chen, C. Gong, D. Bracken-Clarke,\nE. Xue, Y. Yang, J. Sun, and Z. Lu, “Matching patients to clinical trials\nwith large language models,” Nature communications, vol. 15, no. 1, p.\n9074, 2024.\n[132] M. A. Merrill, A. Paruchuri, N. Rezaei, G. Kovacs, J. Perez, Y. Liu,\nE. Schenck, N. Hammerquist, J. Sunshine, S. Tailor et al., “Transforming\nwearable data into health insights using large language model agents,”\narXiv preprint arXiv:2406.06464, 2024.\n[133] S. Schmidgall, R. Ziaei, C. Harris, E. Reis, J. Jopling, and M. Moor,\n“Agentclinic: a multimodal agent benchmark to evaluate ai in simulated\nclinical environments,” arXiv preprint arXiv:2405.07960, 2024.\n[134] Y. Jiang, K. C. Black, G. Geng, D. Park, J. Zou, A. Y. Ng, and J. H.\nChen, “Medagentbench: A realistic virtual ehr environment to benchmark\nmedical llm agents,” arXiv preprint arXiv:2501.14654, 2025.\nSHUBHAM VATSAL received the M.S. degree\nin Computer Science from New York University,\nNew York, USA, in 2023. He is currently a Re-\nsearch Data Scientist in the healthcare domain\nbased in New York, where he applies Machine\nLearning and Natural Language Processing (NLP)\nto real-world clinical and operational problems.\nFrom 2017 to 2021, he was with Samsung Re-\nsearch, focusing on Natural Language Processing\n(NLP) and Machine Learning. He subsequently\nworked at IBM Research on Multimodal learning. He has also contributed\nto a couple of startups, helping build data-driven products using Machine\nlearning and Deep learning. His research interests include Large Language\nModels (LLMs), Agentic AI, applications of LLMs in healthcare and NLP,\nDeep Learning and Machine Learning.\nHARSH DUBEY received the M.S. degree in\ncomputer science from New York University,\nCourant Institute of Mathematical Sciences, New\nYork, USA, in 2023. He is currently based in\nNew York, where he focuses on agentic plat-\nforms, multi-agent frameworks, context memory\nmanagement, and large-scale workflow execution.\nHis work centers on AI agents, large language\nmodels (LLMs), retrieval and memory systems,\nand the design of scalable, distributed systems for\nenterprise applications. From 2018 to 2020, he held engineering positions at\nOctro and IndusOS, where he worked on backend systems, personalization,\nand data-driven product development. His professional experience spans\nmachine learning, distributed infrastructure, and applied artificial intelli-\ngence. His research interests include agentic AI, large language models,\nretrieval-augmented generation, computational cognitive modeling, machine\nlearning, and distributed systems\n24 VOLUME 14, 2026\n\n-- 24 of 25 --\n\nADITI SINGH received the M.S. degree in Com-\nputer Science from Kent State University, Kent,\nOH, USA, in 2017, and the Ph.D. degree in\nComputer Science from Kent State University in\n2023. She is an Assistant College Lecturer with\nthe Department of Computer Science, Cleveland\nState University, Cleveland, OH, USA. Previously,\nshe served as a Graduate Assistant at Kent State\nUniversity (2017–2023). Her research interests\ninclude artificial intelligence and large language\nmodels, with emphasis on agentic systems, retrieval and memory for LLMs,\ntext-to-SQL, the Model Context Protocol (MCP), and computer vision\nfor human–robot interaction. She has authored over 30 publications (with\nseveral in press) and has received multiple recognitions, including Best\nPresenter Awards at IEEE CCWC (2023–2025), a Best Paper Award (CCWC\n2025), and selection as a Finalist for the Women in AI (Research) Award in\n2025. She serves on program committees for venues such as AIET, AIED,\nEDM, and ICR and reviews for international conferences and journals,\nincluding NeurIPS and ACM Computing Surveys. She is a Senior Member\nof the IEEE and chairs the Cleveland ACM-W professional chapter.\nVOLUME 14, 2026 25\n\n-- 25 of 25 --\n\n",
      "analysis_json": "{\"title\":\"Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents\",\"authors\":[\"Shubham Vatsal\",\"Harsh Dubey\",\"Aditi Singh\"],\"publication_year\":\"2026\",\"summary\":\"This study conducts a systematic review of 49 research papers on LLM-based agents in healthcare, proposing a comprehensive Seven-Dimensional Taxonomy to evaluate their capabilities. The authors analyze agents across Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation & Learning, Safety & Ethics, Framework Typology, and Core Tasks. The empirical analysis reveals that while current agents excel at External Knowledge Integration (RAG) and Multi-Agent architectures, they critically lack capabilities in Drift Detection, Event-Triggered Activation, and robust Error Recovery, highlighting a gap between academic prototypes and dependable clinical systems.\",\"breakthrough_score\":60,\"breakthrough_reasoning\":\"This paper represents a solid contribution as a meta-analysis and structural framework rather than a technical invention. Its value lies in organizing a fragmented field and empirically quantifying specific gaps (e.g., the 98% lack of drift detection) that prevent clinical deployment. It provides a roadmap for future research rather than a new algorithm.\",\"key_claims\":[\"External Knowledge Integration is the most mature capability, fully implemented in ~76% of surveyed studies.\",\"Adaptation & Learning is the weakest dimension, with Drift Detection & Mitigation absent in ~98% of agents.\",\"Event-Triggered Activation (autonomous monitoring) is largely missing (~92% not implemented), limiting agents to passive chatbot roles.\",\"Multi-Agent Design is the dominant architectural pattern (~82%), preferring specialized sub-agents over monolithic models.\",\"There is a functional asymmetry where information-centric tasks (QA) are advanced, but action-oriented tasks (Treatment Planning) remain immature.\"],\"testable_hypotheses\":[{\"hypothesis\":\"Agents incorporating explicit Drift Detection mechanisms maintain diagnostic accuracy over time significantly better than static RAG-based agents when clinical guidelines evolve.\",\"how_to_test\":\"Simulate a longitudinal timeline where medical guidelines for a specific condition change. Compare a standard RAG agent against an agent with the paper's defined 'Drift Detection' module. Measure accuracy pre- and post-guideline update without manual prompting.\",\"expected_outcome\":\"The Drift-aware agent will identify the distributional shift in knowledge and update its reasoning, while the standard agent will continue to provide obsolete advice based on static weights or stale indices.\"},{\"hypothesis\":\"Multi-agent debate architectures significantly reduce hallucination rates in complex differential diagnosis compared to single-agent Chain-of-Thought approaches.\",\"how_to_test\":\"Run a benchmark on complex medical cases using two setups: a single GPT-4 agent using CoT, and a Multi-Agent system (as defined in the Framework Typology) with separate 'Proposer' and 'Critique' agents. Measure the rate of fabricated citations or physiological impossibilities.\",\"expected_outcome\":\"The Multi-Agent system will yield higher factual consistency scores due to the 'Consistency & Conflict Resolution' sub-dimension mechanisms.\"},{\"hypothesis\":\"Event-Triggered Activation reduces time-to-intervention in critical care monitoring compared to user-initiated polling.\",\"how_to_test\":\"Connect an agent to a stream of simulated patient vitals. Compare a 'Conversational Mode' agent (reacts only when asked 'How is the patient?') vs. an 'Event-Triggered' agent (monitors stream). Introduce a critical tachycardia event.\",\"expected_outcome\":\"The Event-Triggered agent will initiate an alert sequence immediately upon threshold breach, whereas the Conversational agent will fail to act until prompted.\"}],\"key_equations\":[{\"name\":\"Taxonomy Implementation Score\",\"latex\":\"S_{agent} = \\\\sum_{d=1}^{7} \\\\sum_{s \\\\in d} w_s \\\\cdot I(s)\",\"description\":\"A conceptual scoring model based on the paper's rubric to quantify the maturity of a healthcare agent. It sums the implementation status across the 7 dimensions and 29 sub-dimensions.\",\"variables\":[{\"name\":\"d\",\"description\":\"The 7 core dimensions (Cognitive, Knowledge, Interaction, etc.)\",\"typical_range\":\"1 to 7\"},{\"name\":\"I(s)\",\"description\":\"Implementation status of sub-dimension s (1 for Fully Implemented, 0.5 for Partial, 0 for Not Implemented)\",\"typical_range\":\"{0, 0.5, 1}\"}]},{\"name\":\"Multi-Agent Consensus Function\",\"latex\":\"C = \\\\text{MajorityVote}(\\\\{A_1(x), A_2(x), ..., A_n(x)\\\\}) + \\\\text{Critique}(H)\",\"description\":\"Represents the dominant architectural pattern found in the survey (82% of papers), where final clinical output C is derived from multiple agent outputs A_n and refined via critique H.\",\"variables\":[{\"name\":\"A_n\",\"description\":\"Individual specialist agents (e.g., Diagnosis Agent, Pharmacology Agent)\",\"typical_range\":\"2-5 agents\"},{\"name\":\"Critique(H)\",\"description\":\"A validation step often performed by a separate 'Supervisor' agent to ensure safety\",\"typical_range\":\"Binary Pass/Fail or Text Refinement\"}]}],\"simulation_possibilities\":[{\"title\":\"The 'Missing Link' Clinical Simulator\",\"description\":\"An interactive scenario demonstrating why the gaps identified in the paper (Drift, Events, Error Recovery) cause failure. The user plays the role of a doctor supervising two agents: 'Standard Agent' (based on current averages) and 'Advanced Agent' (fully implemented taxonomy).\",\"complexity\":\"Medium\",\"variables\":[\"Patient Vital Stream (Normal -> Critical)\",\"Clinical Guideline Version (Old -> New)\",\"API Availability (Online -> Error)\"],\"expected_insights\":\"Users will see the Standard Agent fail to notice a heart rate spike (missing Event-Trigger), prescribe a recalled drug (missing Drift Detection), and crash when the EHR API times out (missing Error Recovery), while the Advanced Agent handles all three seamlessly.\",\"visualization_type\":\"interactive\"},{\"title\":\"Agent Architecture Assembler\",\"description\":\"A visual tool allowing users to construct an agent by toggling the 29 sub-dimensions. As users toggle features (e.g., turning on 'Memory Module' or 'Reflection'), a capability radar chart updates, and a simulated 'Reliability Score' changes.\",\"complexity\":\"Low\",\"variables\":[\"Sub-dimensions (On/Off)\",\"Task Complexity (Triage vs. Surgery Planning)\"],\"expected_insights\":\"Demonstrates that high-stakes tasks like Surgery Planning require high scores in Safety & Ethics and Perception, whereas Triage requires high Interaction Patterns.\",\"visualization_type\":\"chart\"},{\"title\":\"Drift Detection Visualizer\",\"description\":\"A data visualization showing how an LLM's answers to medical questions degrade over time as 'ground truth' changes (e.g., new COVID protocols), compared to an agent with the 'Drift Detection & Mitigation' sub-dimension enabled.\",\"complexity\":\"Medium\",\"variables\":[\"Time Steps\",\"Knowledge Volatility Rate\",\"Mitigation Strategy (None vs. Retraining vs. RAG update)\"],\"expected_insights\":\"Visual proof of the paper's claim that static agents become unsafe; illustrates the specific mechanism of how parametric knowledge conflicts with new external evidence.\",\"visualization_type\":\"animation\"}],\"field\":\"Healthcare AI & Medical Informatics\",\"related_fields\":[\"Agentic AI\",\"Large Language Models\",\"Clinical Decision Support Systems\",\"Multi-Agent Systems\"],\"limitations\":[\"The taxonomy definitions are qualitative and subjective, potentially leading to inconsistent scoring by different evaluators.\",\"The review is limited to 49 papers published between late 2023 and mid-2025, potentially missing very recent proprietary developments.\",\"The 'Breakthrough' nature is organizational; it does not present a new model architecture or training dataset itself.\"],\"difficulty_to_understand\":\"Intermediate\",\"prerequisites\":[\"Understanding of Large Language Models (LLMs)\",\"Basic knowledge of RAG (Retrieval-Augmented Generation)\",\"Familiarity with clinical workflows (EHR, Triage, Diagnosis)\"]}",
      "created_at": 1770316734987
    },
    "bdfaa68d8984f0dc02beaca527b76f207d99b666d31d1da728ee0728182df697": {
      "hash": "bdfaa68d8984f0dc02beaca527b76f207d99b666d31d1da728ee0728182df697",
      "filename": "1706.03762v7.pdf",
      "raw_text": "Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani∗\nGoogle Brain\navaswani@google.com\nNoam Shazeer∗\nGoogle Brain\nnoam@google.com\nNiki Parmar∗\nGoogle Research\nnikip@google.com\nJakob Uszkoreit∗\nGoogle Research\nusz@google.com\nLlion Jones∗\nGoogle Research\nllion@google.com\nAidan N. Gomez∗ †\nUniversity of Toronto\naidan@cs.toronto.edu\nŁukasz Kaiser∗\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n†Work performed while at Google Brain.\n‡Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7 [cs.CL] 2 Aug 2023\n\n-- 1 of 15 --\n\n1 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35 , 2 , 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [ 21 ] and conditional\ncomputation [ 32 ], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [ 2, 19 ]. In all but a few cases [ 27 ], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16 ], ByteNet [ 18 ] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [12 ]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5, 2 , 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2\n\n-- 2 of 15 --\n\nFigure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [ 11 ] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3\n\n-- 3 of 15 --\n\nScaled Dot-Product Attention Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv . We compute the dot products of the\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V ) = softmax( QKT\n√dk\n)V (1)\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof 1\t√dk\n. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3 ]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1\t√dk\n.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv -dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\ni=1 qiki, has mean 0 and variance dk .\n4\n\n-- 4 of 15 --\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\nwhere headi = Attention(QW Q\ni , KW K\ni , V W V\ni )\nWhere the projections are parameter matrices W Q\ni ∈ Rdmodel×dk , W K\ni ∈ Rdmodel×dk , W V\ni ∈ Rdmodel×dv\nand W O ∈ Rhdv ×dmodel .\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndf f = 2048.\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30 ]. In the embedding layers, we multiply those weights by √dmodel.\n5\n\n-- 5 of 15 --\n\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2 · d) O(1) O(1)\nRecurrent O(n · d2) O(n) O(n)\nConvolutional O(k · n · d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nP E(pos,2i) = sin(pos/100002i/dmodel )\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\nP Epos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6\n\n-- 6 of 15 --\n\nlength n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38 ] and byte-pair [31 ] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [6 ], however, decrease the complexity\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38 ]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d−0.5\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4 Regularization\nWe employ three types of regularization during training:\n7\n\n-- 7 of 15 --\n\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel BLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\nTransformer (base model) 27.3 38.1 3.3 · 1018\nTransformer (big) 28.4 41.8 2.3 · 1019\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [ 36 ]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty α = 0.6 [ 38 ]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8\n\n-- 8 of 15 --\n\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN dmodel dff h dk dv Pdrop ϵls train PPL BLEU params\nsteps (dev) (dev) ×106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B) 16 5.16 25.1 58\n32 5.01 25.4 60\n(C)\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)\n0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37 ]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9\n\n-- 9 of 15 --\n\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al. (2013) [40] semi-supervised 91.3\nHuang & Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10\n\n-- 10 of 15 --\n\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770–778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735–1780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832–841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n11\n\n-- 11 of 15 --\n\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152–159. ACL, June 2006.\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):1929–1958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434–443. ACL, August 2013.\n12\n\n-- 12 of 15 --\n\nAttention VisualizationsInput-Input Layer5\nIt\t\nis\nin\nthis\nspirit\nthat\na\t\nmajority\t\nof\nAmerican\t\ngovernments\t\nhave\npassed\t\nnew\nlaws\nsince\n2009\nmaking\t\nthe\nregistration\t\nor\nvoting\t\nprocess\t\nmore\ndifficult\t\n.\t\n<EOS>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\nIt\t\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\n13\n\n-- 13 of 15 --\n\nInput-Input Layer5\nThe\nLaw\nwill\nnever\t\nbe\nperfect\t\n,\t\nbut\nits\napplication\t\nshould\t\nbe\njust\n-\t\nthis\nis\t\nwhat\nwe\nare\nmissing\t\n,\t\nin\nmy\nopinion\t\n.\t\n<EOS>\t\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\t\n<pad>Input-Input Layer5\nThe\nLaw\nwill\nnever\t\nbe\nperfect\t\n,\t\nbut\nits\napplication\t\nshould\t\nbe\njust\n-\t\nthis\nis\t\nwhat\nwe\nare\nmissing\t\n,\t\nin\nmy\nopinion\t\n.\t\n<EOS>\t\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\t\n<pad>\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14\n\n-- 14 of 15 --\n\nInput-Input Layer5\nThe\nLaw\nwill\nnever\t\nbe\nperfect\t\n,\t\nbut\nits\napplication\t\nshould\t\nbe\njust\n-\t\nthis\nis\t\nwhat\nwe\nare\nmissing\t\n,\t\nin\nmy\nopinion\t\n.\t\n<EOS>\t\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\t\n<pad>Input-Input Layer5\nThe\nLaw\nwill\nnever\t\nbe\nperfect\t\n,\t\nbut\nits\napplication\t\nshould\t\nbe\njust\n-\t\nthis\nis\t\nwhat\nwe\nare\nmissing\t\n,\t\nin\nmy\nopinion\t\n.\t\n<EOS>\t\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\t\n<pad>\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15\n\n-- 15 of 15 --\n\n",
      "analysis_json": "{\"title\":\"Attention Is All You Need\",\"authors\":[\"Ashish Vaswani\",\"Noam Shazeer\",\"Niki Parmar\",\"Jakob Uszkoreit\",\"Llion Jones\",\"Aidan N. Gomez\",\"Łukasz Kaiser\",\"Illia Polosukhin\"],\"publication_year\":\"2017\",\"summary\":\"This seminal paper introduces the 'Transformer,' a novel neural network architecture based solely on attention mechanisms, dispensing with the recurrence and convolutions used in previous state-of-the-art models (RNNs/CNNs). By utilizing 'Multi-Head Self-Attention,' the model captures global dependencies between input and output regardless of distance. The architecture allows for massive parallelization during training, significantly reducing training time while achieving superior results on English-to-German and English-to-French translation tasks. It establishes the foundation for modern Large Language Models.\",\"breakthrough_score\":100,\"breakthrough_reasoning\":\"This is a paradigm-shifting paper. It effectively deprecated Recurrent Neural Networks (RNNs/LSTMs) for sequence modeling by solving the sequential computation bottleneck. The introduction of the Transformer architecture laid the direct groundwork for BERT, GPT, and the entire current generation of Generative AI. It moved the field from sequential processing to parallelized self-attention, enabling the training of models on data scales previously impossible.\",\"key_claims\":[\"Recurrence and convolutions are not strictly necessary for sequence transduction; attention mechanisms alone are sufficient.\",\"The Transformer architecture allows for significantly more parallelization than recurrent models, reducing training time by orders of magnitude.\",\"Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\",\"Sinusoidal positional encodings allow the model to make use of the order of the sequence without recurrence.\"],\"testable_hypotheses\":[{\"hypothesis\":\"Dot-product attention yields large values in high-dimensional spaces that push softmax into regions with extremely small gradients.\",\"how_to_test\":\"Create a simulation comparing 'Standard Dot-Product' vs 'Scaled Dot-Product' attention. Allow the user to increase the dimension size (dk) and observe the magnitude of the dot products and the resulting gradients of the softmax function.\",\"expected_outcome\":\"As dimension size increases, standard dot-products grow large, causing softmax gradients to vanish. The scaled version (dividing by sqrt(dk)) maintains stable gradients.\"},{\"hypothesis\":\"Different attention heads learn distinct linguistic functions (syntax vs. semantic relationships).\",\"how_to_test\":\"Visualize the attention weights of an 8-head model processing a sentence with ambiguous references (e.g., 'The animal didn't cross the street because it was too tired'). Toggle between heads to see which words 'it' attends to.\",\"expected_outcome\":\"Some heads will focus on local neighbors (syntax), while specific heads will strongly link 'it' to 'animal', effectively resolving the anaphora.\"},{\"hypothesis\":\"Self-attention has a constant path length between any two positions, unlike RNNs which have O(n).\",\"how_to_test\":\"Simulate signal propagation in an RNN vs. Transformer. Click a token at the start of a long sequence and a token at the end, and visualize the number of 'steps' required to relate them.\",\"expected_outcome\":\"The RNN simulation will show a sequential hop through every intermediate token. The Transformer simulation will show a direct, single-step connection regardless of distance.\"}],\"key_equations\":[{\"name\":\"Scaled Dot-Product Attention\",\"latex\":\"Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V\",\"description\":\"The core mechanism where queries (Q) are matched against keys (K) to create weights, which are then applied to values (V). The scaling factor prevents gradient vanishing.\",\"variables\":[{\"name\":\"Q\",\"description\":\"Query matrix (what we are looking for)\",\"typical_range\":\"Matrix of size [sequence_length, 64]\"},{\"name\":\"K\",\"description\":\"Key matrix (what we match against)\",\"typical_range\":\"Matrix of size [sequence_length, 64]\"},{\"name\":\"V\",\"description\":\"Value matrix (the actual content to extract)\",\"typical_range\":\"Matrix of size [sequence_length, 64]\"},{\"name\":\"d_k\",\"description\":\"Dimension of the keys (scaling factor)\",\"typical_range\":\"64\"}]},{\"name\":\"Positional Encoding\",\"latex\":\"PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\",\"description\":\"Injects information about the relative or absolute position of tokens in the sequence, since the model has no recurrence.\",\"variables\":[{\"name\":\"pos\",\"description\":\"The position of the word in the sentence\",\"typical_range\":\"0 to 512\"},{\"name\":\"i\",\"description\":\"The dimension index within the embedding\",\"typical_range\":\"0 to 256 (half of d_model)\"},{\"name\":\"d_model\",\"description\":\"Total dimension of the model/embedding\",\"typical_range\":\"512\"}]},{\"name\":\"Multi-Head Attention\",\"latex\":\"MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O\",\"description\":\"Splits the input into 'h' different 'heads' allowing the model to attend to different parts of the history simultaneously.\",\"variables\":[{\"name\":\"h\",\"description\":\"Number of parallel attention heads\",\"typical_range\":\"8\"},{\"name\":\"W^O\",\"description\":\"Output projection matrix\",\"typical_range\":\"Learned Weights\"}]}],\"simulation_possibilities\":[{\"title\":\"Interactive Self-Attention Heatmap\",\"description\":\"A text input interface where users type a sentence. The visualization displays the self-attention matrix (Q x K^T). Hovering over a word highlights which other words it is 'looking at' and how strongly. Users can switch between the 8 different 'heads' to see how different heads focus on grammar, recent words, or long-term dependencies.\",\"complexity\":\"Medium\",\"variables\":[\"Input Text\",\"Selected Head (0-7)\",\"Layer Index (0-5)\"],\"expected_insights\":\"Users will intuitively understand that 'attention' is just a weighted sum of other words, and that different heads specialize in different linguistic features.\",\"visualization_type\":\"interactive\"},{\"title\":\"Positional Encoding Wave visualizer\",\"description\":\"A 2D heatmap showing the sinusoidal positional encodings. The X-axis is the embedding dimension, Y-axis is the token position. Users can hover to see the specific sin/cos values. A slider allows users to add this encoding to a standard word embedding to see how it modifies the vector space to encode 'order'.\",\"complexity\":\"Low\",\"variables\":[\"Sequence Length\",\"d_model\"],\"expected_insights\":\"Visualizes how the model tells the difference between 'The dog bit the man' and 'The man bit the dog' without processing them in order.\",\"visualization_type\":\"chart\"},{\"title\":\"The Q-K-V Matrix Multiplication Step-Through\",\"description\":\"A step-by-step animation of the math inside a single attention head. Visually represented vectors for Query, Key, and Value. Step 1: Show dot product of Q and K. Step 2: Show scaling and Softmax. Step 3: Show multiplication with V. Users can adjust the vector values manually to see how 'similarity' drives the attention score.\",\"complexity\":\"High\",\"variables\":[\"Vector Values for Q, K, V\",\"d_k scaling\"],\"expected_insights\":\"Demystifies the matrix math, showing that attention is essentially a 'soft dictionary lookup' based on vector similarity.\",\"visualization_type\":\"animation\"}],\"field\":\"Deep Learning / Natural Language Processing\",\"related_fields\":[\"Machine Translation\",\"Computer Vision (Vision Transformers)\",\"Sequence Modeling\",\"Generative AI\"],\"limitations\":[\"Computational complexity is quadratic O(n^2) with respect to sequence length, making very long sequences expensive.\",\"The model is not inherently sequential, requiring positional encodings to understand order.\",\"Requires significant amounts of training data and compute resources compared to smaller statistical models.\"],\"difficulty_to_understand\":\"Advanced\",\"prerequisites\":[\"Matrix Multiplication\",\"Softmax Function\",\"Neural Networks (Basics)\",\"Encoder-Decoder Architectures\"]}",
      "created_at": 1770308018318
    }
  },
  "prototypes": {
    "the-q-k-v-matrix-multiplication-step-through-1770308237941": {
      "id": "the-q-k-v-matrix-multiplication-step-through-1770308237941",
      "paper_hash": "bdfaa68d8984f0dc02beaca527b76f207d99b666d31d1da728ee0728182df697",
      "title": "The Q-K-V Matrix Multiplication Step-Through",
      "description": "A step-by-step animation of the math inside a single attention head. Visually represented vectors for Query, Key, and Value. Step 1: Show dot product of Q and K. Step 2: Show scaling and Softmax. Step 3: Show multiplication with V. Users can adjust the vector values manually to see how 'similarity' drives the attention score.",
      "code": "import React, { useState, useMemo } from 'react';\nimport { motion, AnimatePresence } from 'framer-motion';\nimport { ChevronRight, ChevronLeft, Info, Play, RefreshCw, Zap, Sliders } from 'lucide-react';\n\n// --- Math Helpers ---\n\nconst softmax = (arr) => {\n  const max = Math.max(...arr);\n  const exps = arr.map(x => Math.exp(x - max));\n  const sum = exps.reduce((a, b) => a + b, 0);\n  return exps.map(x => x / sum);\n};\n\nconst dotProduct = (v1, v2) => v1.reduce((acc, curr, i) => acc + curr * v2[i], 0);\n\n// --- Components ---\n\nconst MatrixCell = ({ value, label, colorClass, highlight = false, size = \"w-12 h-12\" }) => (\n  <motion.div\n    layout\n    className={`${size} flex flex-col items-center justify-center rounded border border-white/10 text-xs font-mono \n      ${highlight ? 'bg-blue-500/30 border-blue-400' : 'bg-gray-800/50'} \n      ${colorClass || 'text-white'}`}\n  >\n    <span className=\"opacity-50 text-[10px]\">{label}</span>\n    <span className=\"font-bold\">{value.toFixed(2)}</span>\n  </motion.div>\n);\n\nconst Matrix = ({ data, title, colorClass, labels, activeIndices }) => (\n  <div className=\"flex flex-col gap-2\">\n    <h3 className=\"text-sm font-semibold text-gray-400 uppercase tracking-wider\">{title}</h3>\n    <div className=\"grid gap-1\" style={{ gridTemplateColumns: `repeat(${data[0]?.length || 1}, minmax(0, 1fr))` }}>\n      {data.map((row, i) => \n        row.map((val, j) => (\n          <MatrixCell \n            key={`${i}-${j}`} \n            value={val} \n            label={labels ? labels[i][j] : `[${i},${j}]`}\n            colorClass={colorClass}\n            highlight={activeIndices?.some(([ri, rj]) => ri === i && rj === j)}\n          />\n        ))\n      )}\n    </div>\n  </div>\n);\n\nexport default function App() {\n  const [step, setStep] = useState(0);\n  const [dk] = useState(4); // Dimension of keys/queries\n  const [seqLen] = useState(3); // Sequence length\n  \n  // Initialize Q, K, V Matrices\n  const [Q, setQ] = useState([\n    [1.0, 0.0, 1.0, 0.0],\n    [0.0, 2.0, 0.0, 1.0],\n    [1.0, 1.0, 1.0, 1.0]\n  ]);\n  const [K, setK] = useState([\n    [1.0, 0.0, 1.0, 0.0],\n    [1.0, 1.0, 0.0, 0.0],\n    [0.0, 0.0, 1.0, 1.0]\n  ]);\n  const [V, setV] = useState([\n    [10, 0, 5, 0],\n    [0, 10, 0, 5],\n    [5, 5, 5, 5]\n  ]);\n\n  // Calculations\n  const calculations = useMemo(() => {\n    // 1. Q * K^T\n    const scores = Q.map(qRow => \n      K.map(kRow => dotProduct(qRow, kRow))\n    );\n\n    // 2. Scaled Scores\n    const scale = Math.sqrt(dk);\n    const scaledScores = scores.map(row => row.map(s => s / scale));\n\n    // 3. Softmax\n    const attentionWeights = scaledScores.map(row => softmax(row));\n\n    // 4. Output (Weights * V)\n    const output = attentionWeights.map(wRow => {\n      const outRow = new Array(V[0].length).fill(0);\n      for (let j = 0; j < V[0].length; j++) {\n        for (let i = 0; i < wRow.length; i++) {\n          outRow[j] += wRow[i] * V[i][j];\n        }\n      }\n      return outRow;\n    });\n\n    return { scores, scaledScores, attentionWeights, output, scale };\n  }, [Q, K, V, dk]);\n\n  const steps = [\n    { \n      title: \"Input Matrices\", \n      desc: \"Queries (Q), Keys (K), and Values (V) are projections of the input embeddings.\",\n      formula: \"Inputs: Q, K, V\",\n      icon: <Sliders className=\"w-5 h-5 text-blue-400\" />\n    },\n    { \n      title: \"1. Dot Product (Compatibility)\", \n      desc: \"Measure similarity between each Query and all Keys. Higher dot product = higher similarity.\",\n      formula: \"Score = Q \\u00B7 K\\u1D40\",\n      icon: <Zap className=\"w-5 h-5 text-yellow-400\" />\n    },\n    { \n      title: \"2. Scaling\", \n      desc: `Divide by \\u221A d\\u2094 (\\u221A${dk}=${calculations.scale.toFixed(2)}) to prevent large values from vanishing gradients in Softmax.`,\n      formula: \"Scaled = Score / \\u221A d\\u2094\",\n      icon: <Info className=\"w-5 h-5 text-emerald-400\" />\n    },\n    { \n      title: \"3. Softmax (Attention Weights)\", \n      desc: \"Convert scores to probabilities. Each row sums to 1.0. This determines 'how much' to focus on each value.\",\n      formula: \"Weights = Softmax(Scaled)\",\n      icon: <Play className=\"w-5 h-5 text-purple-400\" />\n    },\n    { \n      title: \"4. Weighted Sum (Final Value)\", \n      desc: \"Multiply weights by Values (V). The result is a new representation for each token, informed by its context.\",\n      formula: \"Output = Weights \\u00B7 V\",\n      icon: <RefreshCw className=\"w-5 h-5 text-pink-400\" />\n    }\n  ];\n\n  const updateMatrix = (matrix, setter, i, j, val) => {\n    const newM = [...matrix];\n    newM[i] = [...newM[i]];\n    newM[i][j] = parseFloat(val);\n    setter(newM);\n  };\n\n  return (\n    <div className=\"min-h-screen bg-gray-950 text-white p-4 md:p-8 font-sans\">\n      {/* Header */}\n      <div className=\"max-w-6xl mx-auto mb-8\">\n        <div className=\"flex items-center gap-3 mb-2\">\n          <div className=\"p-2 bg-blue-500 rounded-lg\">\n            <Zap className=\"w-6 h-6 text-white\" />\n          </div>\n          <h1 className=\"text-2xl font-bold\">Scaled Dot-Product Attention</h1>\n        </div>\n        <p className=\"text-gray-400\">Step-by-step validation of the mechanism proposed in \"Attention Is All You Need\".</p>\n        \n        <div className=\"mt-6 flex flex-wrap gap-4\">\n          <div className=\"bg-gray-900 border border-white/10 rounded-xl p-4 flex-1 min-w-[300px]\">\n            <div className=\"text-xs uppercase text-gray-500 font-bold mb-2\">Core Equation</div>\n            <div className=\"text-xl font-mono text-blue-400\">\n              Attention(Q, K, V) = softmax( <span className=\"underline\">Q Kᵀ</span> / √dₖ ) V\n            </div>\n          </div>\n        </div>\n      </div>\n\n      <div className=\"max-w-6xl mx-auto grid grid-cols-1 lg:grid-cols-3 gap-8\">\n        \n        {/* Left: Interactive Controls */}\n        <div className=\"lg:col-span-1 space-y-6\">\n          <div className=\"bg-gray-900 border border-white/10 rounded-2xl p-6\">\n            <div className=\"flex justify-between items-center mb-6\">\n              <h2 className=\"font-bold flex items-center gap-2\">\n                {steps[step].icon}\n                Step {step}: {steps[step].title}\n              </h2>\n            </div>\n            \n            <p className=\"text-sm text-gray-300 mb-4 leading-relaxed\">\n              {steps[step].desc}\n            </p>\n\n            <div className=\"bg-black/40 rounded-lg p-3 font-mono text-center text-blue-300 border border-blue-500/20 mb-6\">\n              {steps[step].formula}\n            </div>\n\n            <div className=\"flex gap-2\">\n              <button \n                onClick={() => setStep(Math.max(0, step - 1))}\n                disabled={step === 0}\n                className=\"flex-1 py-2 bg-gray-800 hover:bg-gray-700 disabled:opacity-30 rounded-lg transition flex items-center justify-center gap-1\"\n              >\n                <ChevronLeft className=\"w-4 h-4\" /> Back\n              </button>\n              <button \n                onClick={() => setStep(Math.min(steps.length - 1, step + 1))}\n                disabled={step === steps.length - 1}\n                className=\"flex-1 py-2 bg-blue-600 hover:bg-blue-500 disabled:opacity-30 rounded-lg transition flex items-center justify-center gap-1\"\n              >\n                Next <ChevronRight className=\"w-4 h-4\" />\n              </button>\n            </div>\n          </div>\n\n          <div className=\"bg-gray-900 border border-white/10 rounded-2xl p-6\">\n            <h3 className=\"text-sm font-bold text-gray-400 uppercase mb-4\">Live Parameter Tuning</h3>\n            <div className=\"space-y-4\">\n              <div>\n                <label className=\"text-xs text-gray-500 block mb-2\">Query Vector 0, Dim 1</label>\n                <input \n                  type=\"range\" min=\"0\" max=\"5\" step=\"0.1\" \n                  value={Q[0][1]} \n                  onChange={(e) => updateMatrix(Q, setQ, 0, 1, e.target.value)}\n                  className=\"w-full accent-blue-500\"\n                />\n              </div>\n              <div>\n                <label className=\"text-xs text-gray-500 block mb-2\">Key Vector 0, Dim 0</label>\n                <input \n                  type=\"range\" min=\"0\" max=\"5\" step=\"0.1\" \n                  value={K[0][0]} \n                  onChange={(e) => updateMatrix(K, setK, 0, 0, e.target.value)}\n                  className=\"w-full accent-orange-500\"\n                />\n              </div>\n              <div className=\"text-[10px] text-gray-500 italic mt-4\">\n                * Adjusting these values changes the dot product similarity and updates the attention weights in real-time.\n              </div>\n            </div>\n          </div>\n        </div>\n\n        {/* Right: Visualization Canvas */}\n        <div className=\"lg:col-span-2 bg-gray-900/50 border border-white/10 rounded-2xl p-6 overflow-x-auto\">\n          <div className=\"flex flex-col gap-12 min-w-[600px]\">\n            \n            {/* Row 1: Q and K (Step 0-1) */}\n            <div className=\"flex items-start justify-between gap-8\">\n              <Matrix data={Q} title=\"Queries (Q)\" colorClass=\"text-blue-400\" />\n              <div className=\"mt-12 text-2xl font-bold text-gray-700\">×</div>\n              <Matrix data={K} title=\"Keys (K)\" colorClass=\"text-orange-400\" />\n              \n              <AnimatePresence>\n                {step >= 1 && (\n                  <motion.div \n                    initial={{ opacity: 0, x: 20 }} animate={{ opacity: 1, x: 0 }}\n                    className=\"flex items-center gap-8\"\n                  >\n                    <div className=\"mt-12 text-2xl font-bold text-gray-700\">=</div>\n                    <Matrix \n                      data={calculations.scores} \n                      title=\"Raw Scores\" \n                      colorClass=\"text-yellow-400\"\n                      highlight={step === 1}\n                    />\n                  </motion.div>\n                )}\n              </AnimatePresence>\n            </div>\n\n            {/* Row 2: Scaling and Softmax (Step 2-3) */}\n            <AnimatePresence>\n              {step >= 2 && (\n                <motion.div \n                  initial={{ opacity: 0, y: 20 }} animate={{ opacity: 1, y: 0 }}\n                  className=\"flex items-center justify-center gap-12 border-t border-white/5 pt-12\"\n                >\n                  <Matrix \n                    data={calculations.scaledScores} \n                    title=\"Scaled Scores (/√dk)\" \n                    colorClass=\"text-emerald-400\"\n                    highlight={step === 2}\n                  />\n                  <div className=\"mt-12 flex flex-col items-center\">\n                    <ChevronRight className=\"text-gray-600 mb-1\" />\n                    <span className=\"text-[10px] uppercase text-gray-600 font-bold tracking-widest\">Softmax</span>\n                  </div>\n                  <Matrix \n                    data={calculations.attentionWeights} \n                    title=\"Attention Weights\" \n                    colorClass=\"text-purple-400\"\n                    highlight={step === 3}\n                  />\n                </motion.div>\n              )}\n            </AnimatePresence>\n\n            {/* Row 3: Final Output (Step 4) */}\n            <AnimatePresence>\n              {step >= 4 && (\n                <motion.div \n                  initial={{ opacity: 0, y: 20 }} animate={{ opacity: 1, y: 0 }}\n                  className=\"flex items-start justify-between gap-8 border-t border-white/5 pt-12\"\n                >\n                  <div className=\"flex items-start gap-8\">\n                    <Matrix data={calculations.attentionWeights} title=\"Weights\" colorClass=\"text-purple-400\" />\n                    <div className=\"mt-12 text-2xl font-bold text-gray-700\">×</div>\n                    <Matrix data={V} title=\"Values (V)\" colorClass=\"text-pink-400\" />\n                  </div>\n                  <div className=\"mt-12 text-2xl font-bold text-gray-700\">=</div>\n                  <Matrix \n                    data={calculations.output} \n                    title=\"Output (Contextualized)\" \n                    colorClass=\"text-white\"\n                    highlight={true}\n                    size=\"w-16 h-12\"\n                  />\n                </motion.div>\n              )}\n            </AnimatePresence>\n\n          </div>\n        </div>\n      </div>\n\n      {/* Logic Validation Footer */}\n      <div className=\"max-w-6xl mx-auto mt-8 grid grid-cols-1 md:grid-cols-2 gap-4\">\n        <div className=\"bg-emerald-500/10 border border-emerald-500/20 rounded-xl p-4\">\n          <h4 className=\"text-emerald-400 text-sm font-bold flex items-center gap-2 mb-2\">\n            <Zap className=\"w-4 h-4\" /> Why Scaling Matters?\n          </h4>\n          <p className=\"text-xs text-gray-400 leading-relaxed\">\n            As dk grows, the magnitude of dot products increases. This pushes softmax into regions where gradients are extremely small. \n            Dividing by √dk (currently {calculations.scale.toFixed(2)}) stabilizes training. Try increasing the input values above to see how \n            the \"Attention Weights\" shift significantly.\n          </p>\n        </div>\n        <div className=\"bg-blue-500/10 border border-blue-500/20 rounded-xl p-4\">\n          <h4 className=\"text-blue-400 text-sm font-bold flex items-center gap-2 mb-2\">\n            <Zap className=\"w-4 h-4\" /> Context Injection\n          </h4>\n          <p className=\"text-xs text-gray-400 leading-relaxed\">\n            Note how the Output matrix values are a blend of the Values (V). If Weight [0,0] is high, \n            Output row 0 will look mostly like Value row 0. This is how the model \"attends\" to specific information.\n          </p>\n        </div>\n      </div>\n    </div>\n  );\n}\n",
      "algorithm_info": "",
      "history": [
        {
          "role": "user",
          "parts": [
            {
              "text": "Task: Create a prototype for \"The Q-K-V Matrix Multiplication Step-Through\".\nContext: A step-by-step animation of the math inside a single attention head. Visually represented vectors for Query, Key, and Value. Step 1: Show dot product of Q and K. Step 2: Show scaling and Softmax. Step 3: Show multiplication with V. Users can adjust the vector values manually to see how 'similarity' drives the attention score.\n\nFULL PAPER CONTENT:\nProvided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani∗\nGoogle Brain\navaswani@google.com\nNoam Shazeer∗\nGoogle Brain\nnoam@google.com\nNiki Parmar∗\nGoogle Research\nnikip@google.com\nJakob Uszkoreit∗\nGoogle Research\nusz@google.com\nLlion Jones∗\nGoogle Research\nllion@google.com\nAidan N. Gomez∗ †\nUniversity of Toronto\naidan@cs.toronto.edu\nŁukasz Kaiser∗\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n†Work performed while at Google Brain.\n‡Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7 [cs.CL] 2 Aug 2023\n\n-- 1 of 15 --\n\n1 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35 , 2 , 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [ 21 ] and conditional\ncomputation [ 32 ], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [ 2, 19 ]. In all but a few cases [ 27 ], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16 ], ByteNet [ 18 ] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [12 ]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5, 2 , 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2\n\n-- 2 of 15 --\n\nFigure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [ 11 ] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3\n\n-- 3 of 15 --\n\nScaled Dot-Product Attention Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv . We compute the dot products of the\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V ) = softmax( QKT\n√dk\n)V (1)\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof 1\t√dk\n. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3 ]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1\t√dk\n.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv -dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\ni=1 qiki, has mean 0 and variance dk .\n4\n\n-- 4 of 15 --\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\nwhere headi = Attention(QW Q\ni , KW K\ni , V W V\ni )\nWhere the projections are parameter matrices W Q\ni ∈ Rdmodel×dk , W K\ni ∈ Rdmodel×dk , W V\ni ∈ Rdmodel×dv\nand W O ∈ Rhdv ×dmodel .\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndf f = 2048.\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30 ]. In the embedding layers, we multiply those weights by √dmodel.\n5\n\n-- 5 of 15 --\n\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2 · d) O(1) O(1)\nRecurrent O(n · d2) O(n) O(n)\nConvolutional O(k · n · d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nP E(pos,2i) = sin(pos/100002i/dmodel )\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\nP Epos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6\n\n-- 6 of 15 --\n\nlength n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38 ] and byte-pair [31 ] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [6 ], however, decrease the complexity\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38 ]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d−0.5\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4 Regularization\nWe employ three types of regularization during training:\n7\n\n-- 7 of 15 --\n\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel BLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\nTransformer (base model) 27.3 38.1 3.3 · 1018\nTransformer (big) 28.4 41.8 2.3 · 1019\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [ 36 ]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty α = 0.6 [ 38 ]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8\n\n-- 8 of 15 --\n\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN dmodel dff h dk dv Pdrop ϵls train PPL BLEU params\nsteps (dev) (dev) ×106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B) 16 5.16 25.1 58\n32 5.01 25.4 60\n(C)\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)\n0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37 ]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9\n\n-- 9 of 15 --\n\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al. (2013) [40] semi-supervised 91.3\nHuang & Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10\n\n-- 10 of 15 --\n\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770–778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735–1780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832–841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n11\n\n-- 11 of 15 --\n\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152–159. ACL, June 2006.\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):1929–1958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434–443. ACL, August 2013.\n12\n\n-- 12 of 15 --\n\nAttention VisualizationsInput-Input Layer5\nIt\t\nis\nin\nthis\nspirit\nthat\na\t\nmajority\t\nof\nAmerican\t\ngovernments\t\nhave\npassed\t\nnew\nlaws\nsince\n2009\nmaking\t\nthe\nregistration\t\nor\nvoting\t\nprocess\t\nmore\ndifficult\t\n.\t\n<EOS>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\nIt\t\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\n13\n\n-- 13 of 15 --\n\nInput-Input Layer5\nThe\nLaw\nwill\nnever\t\nbe\nperfect\t\n,\t\nbut\nits\napplication\t\nshould\t\nbe\njust\n-\t\nthis\nis\t\nwhat\nwe\nare\nmissing\t\n,\t\nin\nmy\nopinion\t\n.\t\n<EOS>\t\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\t\n<pad>Input-Input Layer5\nThe\nLaw\nwill\nnever\t\nbe\nperfect\t\n,\t\nbut\nits\napplication\t\nshould\t\nbe\njust\n-\t\nthis\nis\t\nwhat\nwe\nare\nmissing\t\n,\t\nin\nmy\nopinion\t\n.\t\n<EOS>\t\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\t\n<pad>\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14\n\n-- 14 of 15 --\n\nInput-Input Layer5\nThe\nLaw\nwill\nnever\t\nbe\nperfect\t\n,\t\nbut\nits\napplication\t\nshould\t\nbe\njust\n-\t\nthis\nis\t\nwhat\nwe\nare\nmissing\t\n,\t\nin\nmy\nopinion\t\n.\t\n<EOS>\t\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\t\n<pad>Input-Input Layer5\nThe\nLaw\nwill\nnever\t\nbe\nperfect\t\n,\t\nbut\nits\napplication\t\nshould\t\nbe\njust\n-\t\nthis\nis\t\nwhat\nwe\nare\nmissing\t\n,\t\nin\nmy\nopinion\t\n.\t\n<EOS>\t\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\t\n<pad>\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15\n\n-- 15 of 15 --\n\n"
            }
          ]
        },
        {
          "role": "model",
          "parts": [
            {
              "functionCall": {
                "name": "render_prototype",
                "args": {
                  "code": "import React, { useState, useMemo } from 'react';\nimport { motion, AnimatePresence } from 'framer-motion';\nimport { ChevronRight, ChevronLeft, Info, Play, RefreshCw, Zap, Sliders } from 'lucide-react';\n\n// --- Math Helpers ---\n\nconst softmax = (arr) => {\n  const max = Math.max(...arr);\n  const exps = arr.map(x => Math.exp(x - max));\n  const sum = exps.reduce((a, b) => a + b, 0);\n  return exps.map(x => x / sum);\n};\n\nconst dotProduct = (v1, v2) => v1.reduce((acc, curr, i) => acc + curr * v2[i], 0);\n\n// --- Components ---\n\nconst MatrixCell = ({ value, label, colorClass, highlight = false, size = \"w-12 h-12\" }) => (\n  <motion.div\n    layout\n    className={`${size} flex flex-col items-center justify-center rounded border border-white/10 text-xs font-mono \n      ${highlight ? 'bg-blue-500/30 border-blue-400' : 'bg-gray-800/50'} \n      ${colorClass || 'text-white'}`}\n  >\n    <span className=\"opacity-50 text-[10px]\">{label}</span>\n    <span className=\"font-bold\">{value.toFixed(2)}</span>\n  </motion.div>\n);\n\nconst Matrix = ({ data, title, colorClass, labels, activeIndices }) => (\n  <div className=\"flex flex-col gap-2\">\n    <h3 className=\"text-sm font-semibold text-gray-400 uppercase tracking-wider\">{title}</h3>\n    <div className=\"grid gap-1\" style={{ gridTemplateColumns: `repeat(${data[0]?.length || 1}, minmax(0, 1fr))` }}>\n      {data.map((row, i) => \n        row.map((val, j) => (\n          <MatrixCell \n            key={`${i}-${j}`} \n            value={val} \n            label={labels ? labels[i][j] : `[${i},${j}]`}\n            colorClass={colorClass}\n            highlight={activeIndices?.some(([ri, rj]) => ri === i && rj === j)}\n          />\n        ))\n      )}\n    </div>\n  </div>\n);\n\nexport default function App() {\n  const [step, setStep] = useState(0);\n  const [dk] = useState(4); // Dimension of keys/queries\n  const [seqLen] = useState(3); // Sequence length\n  \n  // Initialize Q, K, V Matrices\n  const [Q, setQ] = useState([\n    [1.0, 0.0, 1.0, 0.0],\n    [0.0, 2.0, 0.0, 1.0],\n    [1.0, 1.0, 1.0, 1.0]\n  ]);\n  const [K, setK] = useState([\n    [1.0, 0.0, 1.0, 0.0],\n    [1.0, 1.0, 0.0, 0.0],\n    [0.0, 0.0, 1.0, 1.0]\n  ]);\n  const [V, setV] = useState([\n    [10, 0, 5, 0],\n    [0, 10, 0, 5],\n    [5, 5, 5, 5]\n  ]);\n\n  // Calculations\n  const calculations = useMemo(() => {\n    // 1. Q * K^T\n    const scores = Q.map(qRow => \n      K.map(kRow => dotProduct(qRow, kRow))\n    );\n\n    // 2. Scaled Scores\n    const scale = Math.sqrt(dk);\n    const scaledScores = scores.map(row => row.map(s => s / scale));\n\n    // 3. Softmax\n    const attentionWeights = scaledScores.map(row => softmax(row));\n\n    // 4. Output (Weights * V)\n    const output = attentionWeights.map(wRow => {\n      const outRow = new Array(V[0].length).fill(0);\n      for (let j = 0; j < V[0].length; j++) {\n        for (let i = 0; i < wRow.length; i++) {\n          outRow[j] += wRow[i] * V[i][j];\n        }\n      }\n      return outRow;\n    });\n\n    return { scores, scaledScores, attentionWeights, output, scale };\n  }, [Q, K, V, dk]);\n\n  const steps = [\n    { \n      title: \"Input Matrices\", \n      desc: \"Queries (Q), Keys (K), and Values (V) are projections of the input embeddings.\",\n      formula: \"Inputs: Q, K, V\",\n      icon: <Sliders className=\"w-5 h-5 text-blue-400\" />\n    },\n    { \n      title: \"1. Dot Product (Compatibility)\", \n      desc: \"Measure similarity between each Query and all Keys. Higher dot product = higher similarity.\",\n      formula: \"Score = Q \\u00B7 K\\u1D40\",\n      icon: <Zap className=\"w-5 h-5 text-yellow-400\" />\n    },\n    { \n      title: \"2. Scaling\", \n      desc: `Divide by \\u221A d\\u2094 (\\u221A${dk}=${calculations.scale.toFixed(2)}) to prevent large values from vanishing gradients in Softmax.`,\n      formula: \"Scaled = Score / \\u221A d\\u2094\",\n      icon: <Info className=\"w-5 h-5 text-emerald-400\" />\n    },\n    { \n      title: \"3. Softmax (Attention Weights)\", \n      desc: \"Convert scores to probabilities. Each row sums to 1.0. This determines 'how much' to focus on each value.\",\n      formula: \"Weights = Softmax(Scaled)\",\n      icon: <Play className=\"w-5 h-5 text-purple-400\" />\n    },\n    { \n      title: \"4. Weighted Sum (Final Value)\", \n      desc: \"Multiply weights by Values (V). The result is a new representation for each token, informed by its context.\",\n      formula: \"Output = Weights \\u00B7 V\",\n      icon: <RefreshCw className=\"w-5 h-5 text-pink-400\" />\n    }\n  ];\n\n  const updateMatrix = (matrix, setter, i, j, val) => {\n    const newM = [...matrix];\n    newM[i] = [...newM[i]];\n    newM[i][j] = parseFloat(val);\n    setter(newM);\n  };\n\n  return (\n    <div className=\"min-h-screen bg-gray-950 text-white p-4 md:p-8 font-sans\">\n      {/* Header */}\n      <div className=\"max-w-6xl mx-auto mb-8\">\n        <div className=\"flex items-center gap-3 mb-2\">\n          <div className=\"p-2 bg-blue-500 rounded-lg\">\n            <Zap className=\"w-6 h-6 text-white\" />\n          </div>\n          <h1 className=\"text-2xl font-bold\">Scaled Dot-Product Attention</h1>\n        </div>\n        <p className=\"text-gray-400\">Step-by-step validation of the mechanism proposed in \"Attention Is All You Need\".</p>\n        \n        <div className=\"mt-6 flex flex-wrap gap-4\">\n          <div className=\"bg-gray-900 border border-white/10 rounded-xl p-4 flex-1 min-w-[300px]\">\n            <div className=\"text-xs uppercase text-gray-500 font-bold mb-2\">Core Equation</div>\n            <div className=\"text-xl font-mono text-blue-400\">\n              Attention(Q, K, V) = softmax( <span className=\"underline\">Q Kᵀ</span> / √dₖ ) V\n            </div>\n          </div>\n        </div>\n      </div>\n\n      <div className=\"max-w-6xl mx-auto grid grid-cols-1 lg:grid-cols-3 gap-8\">\n        \n        {/* Left: Interactive Controls */}\n        <div className=\"lg:col-span-1 space-y-6\">\n          <div className=\"bg-gray-900 border border-white/10 rounded-2xl p-6\">\n            <div className=\"flex justify-between items-center mb-6\">\n              <h2 className=\"font-bold flex items-center gap-2\">\n                {steps[step].icon}\n                Step {step}: {steps[step].title}\n              </h2>\n            </div>\n            \n            <p className=\"text-sm text-gray-300 mb-4 leading-relaxed\">\n              {steps[step].desc}\n            </p>\n\n            <div className=\"bg-black/40 rounded-lg p-3 font-mono text-center text-blue-300 border border-blue-500/20 mb-6\">\n              {steps[step].formula}\n            </div>\n\n            <div className=\"flex gap-2\">\n              <button \n                onClick={() => setStep(Math.max(0, step - 1))}\n                disabled={step === 0}\n                className=\"flex-1 py-2 bg-gray-800 hover:bg-gray-700 disabled:opacity-30 rounded-lg transition flex items-center justify-center gap-1\"\n              >\n                <ChevronLeft className=\"w-4 h-4\" /> Back\n              </button>\n              <button \n                onClick={() => setStep(Math.min(steps.length - 1, step + 1))}\n                disabled={step === steps.length - 1}\n                className=\"flex-1 py-2 bg-blue-600 hover:bg-blue-500 disabled:opacity-30 rounded-lg transition flex items-center justify-center gap-1\"\n              >\n                Next <ChevronRight className=\"w-4 h-4\" />\n              </button>\n            </div>\n          </div>\n\n          <div className=\"bg-gray-900 border border-white/10 rounded-2xl p-6\">\n            <h3 className=\"text-sm font-bold text-gray-400 uppercase mb-4\">Live Parameter Tuning</h3>\n            <div className=\"space-y-4\">\n              <div>\n                <label className=\"text-xs text-gray-500 block mb-2\">Query Vector 0, Dim 1</label>\n                <input \n                  type=\"range\" min=\"0\" max=\"5\" step=\"0.1\" \n                  value={Q[0][1]} \n                  onChange={(e) => updateMatrix(Q, setQ, 0, 1, e.target.value)}\n                  className=\"w-full accent-blue-500\"\n                />\n              </div>\n              <div>\n                <label className=\"text-xs text-gray-500 block mb-2\">Key Vector 0, Dim 0</label>\n                <input \n                  type=\"range\" min=\"0\" max=\"5\" step=\"0.1\" \n                  value={K[0][0]} \n                  onChange={(e) => updateMatrix(K, setK, 0, 0, e.target.value)}\n                  className=\"w-full accent-orange-500\"\n                />\n              </div>\n              <div className=\"text-[10px] text-gray-500 italic mt-4\">\n                * Adjusting these values changes the dot product similarity and updates the attention weights in real-time.\n              </div>\n            </div>\n          </div>\n        </div>\n\n        {/* Right: Visualization Canvas */}\n        <div className=\"lg:col-span-2 bg-gray-900/50 border border-white/10 rounded-2xl p-6 overflow-x-auto\">\n          <div className=\"flex flex-col gap-12 min-w-[600px]\">\n            \n            {/* Row 1: Q and K (Step 0-1) */}\n            <div className=\"flex items-start justify-between gap-8\">\n              <Matrix data={Q} title=\"Queries (Q)\" colorClass=\"text-blue-400\" />\n              <div className=\"mt-12 text-2xl font-bold text-gray-700\">×</div>\n              <Matrix data={K} title=\"Keys (K)\" colorClass=\"text-orange-400\" />\n              \n              <AnimatePresence>\n                {step >= 1 && (\n                  <motion.div \n                    initial={{ opacity: 0, x: 20 }} animate={{ opacity: 1, x: 0 }}\n                    className=\"flex items-center gap-8\"\n                  >\n                    <div className=\"mt-12 text-2xl font-bold text-gray-700\">=</div>\n                    <Matrix \n                      data={calculations.scores} \n                      title=\"Raw Scores\" \n                      colorClass=\"text-yellow-400\"\n                      highlight={step === 1}\n                    />\n                  </motion.div>\n                )}\n              </AnimatePresence>\n            </div>\n\n            {/* Row 2: Scaling and Softmax (Step 2-3) */}\n            <AnimatePresence>\n              {step >= 2 && (\n                <motion.div \n                  initial={{ opacity: 0, y: 20 }} animate={{ opacity: 1, y: 0 }}\n                  className=\"flex items-center justify-center gap-12 border-t border-white/5 pt-12\"\n                >\n                  <Matrix \n                    data={calculations.scaledScores} \n                    title=\"Scaled Scores (/√dk)\" \n                    colorClass=\"text-emerald-400\"\n                    highlight={step === 2}\n                  />\n                  <div className=\"mt-12 flex flex-col items-center\">\n                    <ChevronRight className=\"text-gray-600 mb-1\" />\n                    <span className=\"text-[10px] uppercase text-gray-600 font-bold tracking-widest\">Softmax</span>\n                  </div>\n                  <Matrix \n                    data={calculations.attentionWeights} \n                    title=\"Attention Weights\" \n                    colorClass=\"text-purple-400\"\n                    highlight={step === 3}\n                  />\n                </motion.div>\n              )}\n            </AnimatePresence>\n\n            {/* Row 3: Final Output (Step 4) */}\n            <AnimatePresence>\n              {step >= 4 && (\n                <motion.div \n                  initial={{ opacity: 0, y: 20 }} animate={{ opacity: 1, y: 0 }}\n                  className=\"flex items-start justify-between gap-8 border-t border-white/5 pt-12\"\n                >\n                  <div className=\"flex items-start gap-8\">\n                    <Matrix data={calculations.attentionWeights} title=\"Weights\" colorClass=\"text-purple-400\" />\n                    <div className=\"mt-12 text-2xl font-bold text-gray-700\">×</div>\n                    <Matrix data={V} title=\"Values (V)\" colorClass=\"text-pink-400\" />\n                  </div>\n                  <div className=\"mt-12 text-2xl font-bold text-gray-700\">=</div>\n                  <Matrix \n                    data={calculations.output} \n                    title=\"Output (Contextualized)\" \n                    colorClass=\"text-white\"\n                    highlight={true}\n                    size=\"w-16 h-12\"\n                  />\n                </motion.div>\n              )}\n            </AnimatePresence>\n\n          </div>\n        </div>\n      </div>\n\n      {/* Logic Validation Footer */}\n      <div className=\"max-w-6xl mx-auto mt-8 grid grid-cols-1 md:grid-cols-2 gap-4\">\n        <div className=\"bg-emerald-500/10 border border-emerald-500/20 rounded-xl p-4\">\n          <h4 className=\"text-emerald-400 text-sm font-bold flex items-center gap-2 mb-2\">\n            <Zap className=\"w-4 h-4\" /> Why Scaling Matters?\n          </h4>\n          <p className=\"text-xs text-gray-400 leading-relaxed\">\n            As dk grows, the magnitude of dot products increases. This pushes softmax into regions where gradients are extremely small. \n            Dividing by √dk (currently {calculations.scale.toFixed(2)}) stabilizes training. Try increasing the input values above to see how \n            the \"Attention Weights\" shift significantly.\n          </p>\n        </div>\n        <div className=\"bg-blue-500/10 border border-blue-500/20 rounded-xl p-4\">\n          <h4 className=\"text-blue-400 text-sm font-bold flex items-center gap-2 mb-2\">\n            <Zap className=\"w-4 h-4\" /> Context Injection\n          </h4>\n          <p className=\"text-xs text-gray-400 leading-relaxed\">\n            Note how the Output matrix values are a blend of the Values (V). If Weight [0,0] is high, \n            Output row 0 will look mostly like Value row 0. This is how the model \"attends\" to specific information.\n          </p>\n        </div>\n      </div>\n    </div>\n  );\n}\n",
                  "title": "The Q-K-V Matrix Multiplication Step-Through"
                }
              }
            }
          ]
        }
      ],
      "created_at": 1770310074849,
      "updated_at": 1770310721837
    },
    "interactive-self-attention-heatmap-1770313097235": {
      "id": "interactive-self-attention-heatmap-1770313097235",
      "paper_hash": "bdfaa68d8984f0dc02beaca527b76f207d99b666d31d1da728ee0728182df697",
      "title": "Interactive Transformer Self-Attention Heatmap",
      "description": "A text input interface where users type a sentence. The visualization displays the self-attention matrix (Q x K^T). Hovering over a word highlights which other words it is 'looking at' and how strongly. Users can switch between the 8 different 'heads' to see how different heads focus on grammar, recent words, or long-term dependencies.",
      "code": "import React, { useState, useMemo, useEffect } from 'react';\nimport { motion, AnimatePresence } from 'framer-motion';\nimport { Brain, Settings, Info, ChevronRight, Zap, Layers, MousePointer2 } from 'lucide-react';\n\n// Pseudo-random deterministic generator for consistent \"learned\" weights\nconst mulberry32 = (a) => {\n  return () => {\n    let t = a += 0x6D2B79F5;\n    t = Math.imul(t ^ t >>> 15, t | 1);\n    t ^= t + Math.imul(t ^ t >>> 7, t | 61);\n    return ((t ^ t >>> 14) >>> 0) / 4294967296;\n  };\n};\n\n// Generate deterministic vector for a token\nconst getWordVector = (word, dim, seed = 0) => {\n  const hash = word.split('').reduce((acc, char) => acc + char.charCodeAt(0), 0);\n  const rand = mulberry32(hash + seed);\n  return Array.from({ length: dim }, () => rand() * 2 - 1);\n};\n\n// Main Attention Calculation Implementation\n// Based on: Attention(Q, K, V) = softmax(QK^T / sqrt(dk))\nfunction calculateAttention(tokens, headIndex, dk = 64) {\n  const n = tokens.length;\n  if (n === 0) return [];\n\n  // 1. Generate Query and Key vectors for each token\n  // In a real model, these are learned projections Wq, Wk. \n  // Here we simulate the projection by using head-specific seeds.\n  const Q = tokens.map(t => getWordVector(t, dk, headIndex * 100));\n  const K = tokens.map(t => getWordVector(t, dk, headIndex * 200 + 5));\n\n  // 2. Compute QK^T\n  let scores = Array(n).fill(0).map(() => Array(n).fill(0));\n  for (let i = 0; i < n; i++) {\n    for (let j = 0; j < n; j++) {\n      let dotProduct = 0;\n      for (let k = 0; k < dk; k++) {\n        dotProduct += Q[i][k] * K[j][k];\n      }\n      // Scale by sqrt(dk) as per paper section 3.2.1\n      scores[i][j] = dotProduct / Math.sqrt(dk);\n    }\n  }\n\n  // 3. Apply Softmax across rows\n  const attentionMatrix = scores.map(row => {\n    const maxVal = Math.max(...row); // For numerical stability\n    const exps = row.map(v => Math.exp(v - maxVal));\n    const sumExps = exps.reduce((a, b) => a + b, 0);\n    return exps.map(v => v / sumExps);\n  });\n\n  return attentionMatrix;\n}\n\nconst HEAD_DESCRIPTIONS = [\n  { name: \"Global Context\", behavior: \"Looks at all tokens broadly to understand the overall theme.\" },\n  { name: \"Local Syntax\", behavior: \"Focuses on immediately adjacent words (n-1, n+1).\" },\n  { name: \"Self-Identity\", behavior: \"Strong focus on the token itself. Acts as a pass-through.\" },\n  { name: \"Long-Range\", behavior: \"Attends to tokens far back in the sentence (anaphora resolution).\" },\n  { name: \"Noun-Verb\", behavior: \"Learned dependency mapping between subjects and actions.\" },\n  { name: \"Delimiter\", behavior: \"Focuses on punctuation and sentence boundaries.\" },\n  { name: \"Positional\", behavior: \"Attends to specific relative offsets regardless of content.\" },\n  { name: \"Redundancy\", behavior: \"Cross-checks information already processed in earlier layers.\" }\n];\n\nexport default function App() {\n  const [inputText, setInputText] = useState(\"The animal didn't cross the street because it was too tired.\");\n  const [activeHead, setActiveHead] = useState(0);\n  const [hoveredWordIndex, setHoveredWordIndex] = useState(null);\n  const [showValues, setShowValues] = useState(false);\n\n  const tokens = useMemo(() => {\n    return inputText.trim().split(/\\s+/).filter(t => t.length > 0);\n  }, [inputText]);\n\n  const attentionMatrix = useMemo(() => {\n    return calculateAttention(tokens, activeHead);\n  }, [tokens, activeHead]);\n\n  const getColor = (val) => {\n    // Transformer visualization standard: Blue-to-White or Purple-to-White\n    return `rgba(96, 165, 250, ${Math.pow(val, 0.7)})`; // Using power to emphasize lower values\n  };\n\n  return (\n    <div className=\"min-h-screen bg-gray-950 text-slate-200 font-sans selection:bg-blue-500/30\">\n      {/* Header */}\n      <header className=\"border-b border-white/10 bg-gray-900/50 backdrop-blur-md sticky top-0 z-50\">\n        <div className=\"max-w-6xl mx-auto px-6 py-4 flex items-center justify-between\">\n          <div className=\"flex items-center gap-3\">\n            <div className=\"p-2 bg-blue-600 rounded-lg shadow-lg shadow-blue-500/20\">\n              <Brain className=\"w-6 h-6 text-white\" />\n            </div>\n            <div>\n              <h1 className=\"text-xl font-bold bg-clip-text text-transparent bg-gradient-to-r from-white to-gray-400\">\n                Transformer Self-Attention\n              </h1>\n              <p className=\"text-xs text-gray-500 font-medium tracking-wider uppercase\">Attention Is All You Need (2017)</p>\n            </div>\n          </div>\n          <div className=\"flex items-center gap-4 text-sm font-medium\">\n            <a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\" className=\"text-gray-400 hover:text-white transition-colors\">ArXiv Paper</a>\n          </div>\n        </div>\n      </header>\n\n      <main className=\"max-w-6xl mx-auto px-6 py-8 grid grid-cols-1 lg:grid-cols-12 gap-8\">\n        \n        {/* Left Column: Controls & Input */}\n        <div className=\"lg:col-span-4 space-y-6\">\n          <section className=\"bg-gray-900/50 border border-white/5 rounded-2xl p-5 shadow-xl\">\n            <div className=\"flex items-center gap-2 mb-4 text-blue-400\">\n              <Zap className=\"w-4 h-4\" />\n              <h2 className=\"text-sm font-bold uppercase tracking-widest\">Input Sequence</h2>\n            </div>\n            <textarea\n              value={inputText}\n              onChange={(e) => setInputText(e.target.value)}\n              className=\"w-full h-32 bg-gray-950 border border-white/10 rounded-xl p-4 text-sm focus:ring-2 focus:ring-blue-500 focus:border-transparent outline-none transition-all resize-none\"\n              placeholder=\"Type a sentence to visualize attention...\"\n            />\n            <div className=\"mt-2 text-xs text-gray-500 flex justify-between\">\n              <span>{tokens.length} tokens detected</span>\n              <button onClick={() => setInputText(\"The animal didn't cross the street because it was too tired.\")} className=\"hover:underline\">Reset to example</button>\n            </div>\n          </section>\n\n          <section className=\"bg-gray-900/50 border border-white/5 rounded-2xl p-5 shadow-xl\">\n            <div className=\"flex items-center gap-2 mb-4 text-purple-400\">\n              <Layers className=\"w-4 h-4\" />\n              <h2 className=\"text-sm font-bold uppercase tracking-widest\">Attention Heads (h=8)</h2>\n            </div>\n            <div className=\"grid grid-cols-4 gap-2 mb-6\">\n              {Array.from({ length: 8 }).map((_, i) => (\n                <button\n                  key={i}\n                  onClick={() => setActiveHead(i)}\n                  className={`py-2 rounded-lg text-sm font-bold transition-all ${\n                    activeHead === i \n                    ? 'bg-blue-600 text-white shadow-lg shadow-blue-500/20' \n                    : 'bg-gray-800 text-gray-400 hover:bg-gray-700'\n                  }`}\n                >\n                  {i + 1}\n                </button>\n              ))}\n            </div>\n            <div className=\"bg-blue-500/5 border border-blue-500/20 rounded-xl p-4\">\n              <h3 className=\"text-blue-400 text-sm font-bold mb-1 flex items-center gap-2\">\n                <Info className=\"w-3 h-3\" />\n                {HEAD_DESCRIPTIONS[activeHead].name}\n              </h3>\n              <p className=\"text-xs text-gray-400 leading-relaxed italic\">\n                \"{HEAD_DESCRIPTIONS[activeHead].behavior}\"\n              </p>\n            </div>\n          </section>\n\n          <div className=\"bg-gray-900/50 border border-white/5 rounded-2xl p-5\">\n            <label className=\"flex items-center gap-3 cursor-pointer group\">\n              <div className={`w-10 h-5 rounded-full p-1 transition-colors ${showValues ? 'bg-blue-600' : 'bg-gray-700'}`}>\n                <div className={`w-3 h-3 bg-white rounded-full transition-transform ${showValues ? 'translate-x-5' : 'translate-x-0'}`} />\n              </div>\n              <input type=\"checkbox\" className=\"hidden\" checked={showValues} onChange={() => setShowValues(!showValues)} />\n              <span className=\"text-sm font-medium text-gray-400 group-hover:text-white transition-colors\">Show numerical values</span>\n            </label>\n          </div>\n        </div>\n\n        {/* Right Column: Visualization */}\n        <div className=\"lg:col-span-8 space-y-6\">\n          <div className=\"bg-gray-900/50 border border-white/5 rounded-2xl p-6 shadow-xl relative overflow-hidden\">\n            {/* Legend */}\n            <div className=\"absolute top-6 right-6 flex items-center gap-4 text-[10px] text-gray-500 font-bold uppercase tracking-widest\">\n              <span>Weak Attention</span>\n              <div className=\"flex gap-0.5\">\n                {[0.1, 0.3, 0.5, 0.7, 0.9].map(o => (\n                  <div key={o} className=\"w-4 h-2 rounded-sm\" style={{ backgroundColor: `rgba(59, 130, 246, ${o})` }} />\n                ))}\n              </div>\n              <span>Strong</span>\n            </div>\n\n            <h2 className=\"text-sm font-bold uppercase tracking-widest mb-8 flex items-center gap-2\">\n              <MousePointer2 className=\"w-4 h-4 text-blue-400\" />\n              Interactive Heatmap (Head {activeHead + 1})\n            </h2>\n\n            {/* Matrix Container */}\n            <div className=\"relative overflow-x-auto pb-4 custom-scrollbar\">\n              <div \n                className=\"grid gap-1\"\n                style={{ \n                  gridTemplateColumns: `auto repeat(${tokens.length}, minmax(40px, 1fr))`,\n                  minWidth: `${tokens.length * 40 + 100}px`\n                }}\n              >\n                {/* Header corner */}\n                <div className=\"h-10 w-24\"></div>\n                {/* Column Headers */}\n                {tokens.map((token, idx) => (\n                  <div key={`col-${idx}`} className=\"h-10 text-[10px] font-bold text-gray-500 uppercase flex items-center justify-center -rotate-45 origin-bottom-left translate-y-[-5px]\">\n                    {token.length > 8 ? token.substring(0, 6) + '..' : token}\n                  </div>\n                ))}\n\n                {/* Rows */}\n                {tokens.map((rowToken, i) => (\n                  <React.Fragment key={`row-${i}`}>\n                    {/* Row Label */}\n                    <div className=\"h-10 w-24 flex items-center justify-end pr-4 text-xs font-bold text-gray-400 truncate\">\n                      {rowToken}\n                    </div>\n                    {/* Attention Cells */}\n                    {tokens.map((colToken, j) => {\n                      const value = attentionMatrix[i][j] || 0;\n                      const isTarget = hoveredWordIndex === i;\n                      const isSource = hoveredWordIndex === j;\n                      \n                      return (\n                        <motion.div\n                          key={`${i}-${j}`}\n                          onMouseEnter={() => setHoveredWordIndex(i)}\n                          onMouseLeave={() => setHoveredWordIndex(null)}\n                          className={`h-10 rounded-sm flex items-center justify-center text-[8px] font-mono transition-all duration-200 cursor-crosshair\n                            ${isTarget ? 'ring-1 ring-blue-400/50 z-10' : ''}\n                            ${isSource ? 'ring-1 ring-white/20' : ''}\n                          `}\n                          style={{ \n                            backgroundColor: getColor(value),\n                            color: value > 0.5 ? 'white' : 'rgba(255,255,255,0.4)'\n                          }}\n                        >\n                          {showValues && value.toFixed(2)}\n                        </motion.div>\n                      );\n                    })}\n                  </React.Fragment>\n                ))}\n              </div>\n            </div>\n\n            {/* Hover Explainer */}\n            <AnimatePresence>\n              {hoveredWordIndex !== null && (\n                <motion.div \n                  initial={{ opacity: 0, y: 10 }}\n                  animate={{ opacity: 1, y: 0 }}\n                  exit={{ opacity: 0, y: 10 }}\n                  className=\"mt-8 p-4 bg-gray-950 border border-blue-500/30 rounded-xl\"\n                >\n                  <div className=\"flex items-center gap-3 mb-3\">\n                    <span className=\"text-xs font-bold text-gray-500 uppercase tracking-widest\">Analyzing:</span>\n                    <span className=\"px-3 py-1 bg-blue-600 rounded-md text-white font-bold text-sm\">\n                      {tokens[hoveredWordIndex]}\n                    </span>\n                    <ChevronRight className=\"w-4 h-4 text-gray-600\" />\n                    <span className=\"text-xs text-gray-400\">This word is \"looking at\":</span>\n                  </div>\n                  <div className=\"flex flex-wrap gap-2\">\n                    {tokens.map((token, idx) => {\n                      const weight = attentionMatrix[hoveredWordIndex][idx];\n                      if (weight < 0.05) return null; // Only show significant attention\n                      return (\n                        <div \n                          key={idx} \n                          className=\"flex flex-col items-center bg-gray-900 px-3 py-2 rounded-lg border border-white/5\"\n                          style={{ borderColor: weight > 0.3 ? `rgba(59, 130, 246, ${weight})` : 'transparent' }}\n                        >\n                          <span className=\"text-xs font-bold mb-1\">{token}</span>\n                          <div className=\"w-12 h-1.5 bg-gray-800 rounded-full overflow-hidden\">\n                            <div className=\"h-full bg-blue-500\" style={{ width: `${weight * 100}%` }} />\n                          </div>\n                          <span className=\"text-[10px] text-gray-500 mt-1\">{(weight * 100).toFixed(0)}%</span>\n                        </div>\n                      );\n                    })}\n                  </div>\n                </motion.div>\n              )}\n            </AnimatePresence>\n          </div>\n\n          {/* Math Insights */}\n          <div className=\"grid grid-cols-1 md:grid-cols-2 gap-4\">\n            <div className=\"bg-blue-600/10 border border-blue-500/20 rounded-2xl p-5\">\n              <h3 className=\"text-sm font-bold text-blue-400 uppercase mb-3 flex items-center gap-2\">\n                <Zap className=\"w-4 h-4\" />\n                Scaled Dot-Product\n              </h3>\n              <p className=\"text-xs text-slate-300 leading-relaxed\">\n                The score between two words is computed via <strong>dot product</strong>. \n                Large dk values cause the softmax gradient to vanish, so we scale by \n                <span className=\"font-mono text-blue-300 ml-1\">1/√dk (1/√64 = 0.125)</span>.\n              </p>\n            </div>\n            <div className=\"bg-purple-600/10 border border-purple-500/20 rounded-2xl p-5\">\n              <h3 className=\"text-sm font-bold text-purple-400 uppercase mb-3 flex items-center gap-2\">\n                <Layers className=\"w-4 h-4\" />\n                Multi-Head Logic\n              </h3>\n              <p className=\"text-xs text-slate-300 leading-relaxed\">\n                \"Multi-head attention allows the model to jointly attend to information from \n                different representation subspaces at different positions.\" — <em>Paper Sec 3.2.2</em>\n              </p>\n            </div>\n          </div>\n        </div>\n      </main>\n\n      <style dangerouslySetInnerHTML={{ __html: `\n        .custom-scrollbar::-webkit-scrollbar {\n          height: 6px;\n        }\n        .custom-scrollbar::-webkit-scrollbar-track {\n          background: rgba(255, 255, 255, 0.05);\n          border-radius: 10px;\n        }\n        .custom-scrollbar::-webkit-scrollbar-thumb {\n          background: rgba(255, 255, 255, 0.1);\n          border-radius: 10px;\n        }\n        .custom-scrollbar::-webkit-scrollbar-thumb:hover {\n          background: rgba(255, 255, 255, 0.2);\n        }\n      `}} />\n    </div>\n  );\n}\n",
      "algorithm_info": "",
      "history": [
        {
          "role": "user",
          "parts": [
            {
              "text": "Task: Create a prototype for \"Interactive Self-Attention Heatmap\".\nContext: A text input interface where users type a sentence. The visualization displays the self-attention matrix (Q x K^T). Hovering over a word highlights which other words it is 'looking at' and how strongly. Users can switch between the 8 different 'heads' to see how different heads focus on grammar, recent words, or long-term dependencies.\n\nFULL PAPER CONTENT:\nProvided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani∗\nGoogle Brain\navaswani@google.com\nNoam Shazeer∗\nGoogle Brain\nnoam@google.com\nNiki Parmar∗\nGoogle Research\nnikip@google.com\nJakob Uszkoreit∗\nGoogle Research\nusz@google.com\nLlion Jones∗\nGoogle Research\nllion@google.com\nAidan N. Gomez∗ †\nUniversity of Toronto\naidan@cs.toronto.edu\nŁukasz Kaiser∗\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n†Work performed while at Google Brain.\n‡Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7 [cs.CL] 2 Aug 2023\n\n-- 1 of 15 --\n\n1 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35 , 2 , 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [ 21 ] and conditional\ncomputation [ 32 ], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [ 2, 19 ]. In all but a few cases [ 27 ], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16 ], ByteNet [ 18 ] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [12 ]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5, 2 , 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2\n\n-- 2 of 15 --\n\nFigure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [ 11 ] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3\n\n-- 3 of 15 --\n\nScaled Dot-Product Attention Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv . We compute the dot products of the\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V ) = softmax( QKT\n√dk\n)V (1)\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof 1\t√dk\n. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3 ]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1\t√dk\n.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv -dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\ni=1 qiki, has mean 0 and variance dk .\n4\n\n-- 4 of 15 --\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\nwhere headi = Attention(QW Q\ni , KW K\ni , V W V\ni )\nWhere the projections are parameter matrices W Q\ni ∈ Rdmodel×dk , W K\ni ∈ Rdmodel×dk , W V\ni ∈ Rdmodel×dv\nand W O ∈ Rhdv ×dmodel .\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndf f = 2048.\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30 ]. In the embedding layers, we multiply those weights by √dmodel.\n5\n\n-- 5 of 15 --\n\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2 · d) O(1) O(1)\nRecurrent O(n · d2) O(n) O(n)\nConvolutional O(k · n · d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nP E(pos,2i) = sin(pos/100002i/dmodel )\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\nP Epos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6\n\n-- 6 of 15 --\n\nlength n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38 ] and byte-pair [31 ] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [6 ], however, decrease the complexity\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38 ]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d−0.5\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4 Regularization\nWe employ three types of regularization during training:\n7\n\n-- 7 of 15 --\n\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel BLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\nTransformer (base model) 27.3 38.1 3.3 · 1018\nTransformer (big) 28.4 41.8 2.3 · 1019\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [ 36 ]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty α = 0.6 [ 38 ]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8\n\n-- 8 of 15 --\n\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN dmodel dff h dk dv Pdrop ϵls train PPL BLEU params\nsteps (dev) (dev) ×106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B) 16 5.16 25.1 58\n32 5.01 25.4 60\n(C)\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)\n0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37 ]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9\n\n-- 9 of 15 --\n\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al. (2013) [40] semi-supervised 91.3\nHuang & Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10\n\n-- 10 of 15 --\n\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770–778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735–1780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832–841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n11\n\n-- 11 of 15 --\n\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152–159. ACL, June 2006.\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):1929–1958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434–443. ACL, August 2013.\n12\n\n-- 12 of 15 --\n\nAttention VisualizationsInput-Input Layer5\nIt\t\nis\nin\nthis\nspirit\nthat\na\t\nmajority\t\nof\nAmerican\t\ngovernments\t\nhave\npassed\t\nnew\nlaws\nsince\n2009\nmaking\t\nthe\nregistration\t\nor\nvoting\t\nprocess\t\nmore\ndifficult\t\n.\t\n<EOS>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\nIt\t\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\n13\n\n-- 13 of 15 --\n\nInput-Input Layer5\nThe\nLaw\nwill\nnever\t\nbe\nperfect\t\n,\t\nbut\nits\napplication\t\nshould\t\nbe\njust\n-\t\nthis\nis\t\nwhat\nwe\nare\nmissing\t\n,\t\nin\nmy\nopinion\t\n.\t\n<EOS>\t\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\t\n<pad>Input-Input Layer5\nThe\nLaw\nwill\nnever\t\nbe\nperfect\t\n,\t\nbut\nits\napplication\t\nshould\t\nbe\njust\n-\t\nthis\nis\t\nwhat\nwe\nare\nmissing\t\n,\t\nin\nmy\nopinion\t\n.\t\n<EOS>\t\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\t\n<pad>\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14\n\n-- 14 of 15 --\n\nInput-Input Layer5\nThe\nLaw\nwill\nnever\t\nbe\nperfect\t\n,\t\nbut\nits\napplication\t\nshould\t\nbe\njust\n-\t\nthis\nis\t\nwhat\nwe\nare\nmissing\t\n,\t\nin\nmy\nopinion\t\n.\t\n<EOS>\t\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\t\n<pad>Input-Input Layer5\nThe\nLaw\nwill\nnever\t\nbe\nperfect\t\n,\t\nbut\nits\napplication\t\nshould\t\nbe\njust\n-\t\nthis\nis\t\nwhat\nwe\nare\nmissing\t\n,\t\nin\nmy\nopinion\t\n.\t\n<EOS>\t\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\t\n<pad>\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15\n\n-- 15 of 15 --\n\n"
            }
          ]
        },
        {
          "role": "model",
          "parts": [
            {
              "functionCall": {
                "name": "render_prototype",
                "args": {
                  "code": "import React, { useState, useMemo, useEffect } from 'react';\nimport { motion, AnimatePresence } from 'framer-motion';\nimport { Brain, Settings, Info, ChevronRight, Zap, Layers, MousePointer2 } from 'lucide-react';\n\n// Pseudo-random deterministic generator for consistent \"learned\" weights\nconst mulberry32 = (a) => {\n  return () => {\n    let t = a += 0x6D2B79F5;\n    t = Math.imul(t ^ t >>> 15, t | 1);\n    t ^= t + Math.imul(t ^ t >>> 7, t | 61);\n    return ((t ^ t >>> 14) >>> 0) / 4294967296;\n  };\n};\n\n// Generate deterministic vector for a token\nconst getWordVector = (word, dim, seed = 0) => {\n  const hash = word.split('').reduce((acc, char) => acc + char.charCodeAt(0), 0);\n  const rand = mulberry32(hash + seed);\n  return Array.from({ length: dim }, () => rand() * 2 - 1);\n};\n\n// Main Attention Calculation Implementation\n// Based on: Attention(Q, K, V) = softmax(QK^T / sqrt(dk))\nfunction calculateAttention(tokens, headIndex, dk = 64) {\n  const n = tokens.length;\n  if (n === 0) return [];\n\n  // 1. Generate Query and Key vectors for each token\n  // In a real model, these are learned projections Wq, Wk. \n  // Here we simulate the projection by using head-specific seeds.\n  const Q = tokens.map(t => getWordVector(t, dk, headIndex * 100));\n  const K = tokens.map(t => getWordVector(t, dk, headIndex * 200 + 5));\n\n  // 2. Compute QK^T\n  let scores = Array(n).fill(0).map(() => Array(n).fill(0));\n  for (let i = 0; i < n; i++) {\n    for (let j = 0; j < n; j++) {\n      let dotProduct = 0;\n      for (let k = 0; k < dk; k++) {\n        dotProduct += Q[i][k] * K[j][k];\n      }\n      // Scale by sqrt(dk) as per paper section 3.2.1\n      scores[i][j] = dotProduct / Math.sqrt(dk);\n    }\n  }\n\n  // 3. Apply Softmax across rows\n  const attentionMatrix = scores.map(row => {\n    const maxVal = Math.max(...row); // For numerical stability\n    const exps = row.map(v => Math.exp(v - maxVal));\n    const sumExps = exps.reduce((a, b) => a + b, 0);\n    return exps.map(v => v / sumExps);\n  });\n\n  return attentionMatrix;\n}\n\nconst HEAD_DESCRIPTIONS = [\n  { name: \"Global Context\", behavior: \"Looks at all tokens broadly to understand the overall theme.\" },\n  { name: \"Local Syntax\", behavior: \"Focuses on immediately adjacent words (n-1, n+1).\" },\n  { name: \"Self-Identity\", behavior: \"Strong focus on the token itself. Acts as a pass-through.\" },\n  { name: \"Long-Range\", behavior: \"Attends to tokens far back in the sentence (anaphora resolution).\" },\n  { name: \"Noun-Verb\", behavior: \"Learned dependency mapping between subjects and actions.\" },\n  { name: \"Delimiter\", behavior: \"Focuses on punctuation and sentence boundaries.\" },\n  { name: \"Positional\", behavior: \"Attends to specific relative offsets regardless of content.\" },\n  { name: \"Redundancy\", behavior: \"Cross-checks information already processed in earlier layers.\" }\n];\n\nexport default function App() {\n  const [inputText, setInputText] = useState(\"The animal didn't cross the street because it was too tired.\");\n  const [activeHead, setActiveHead] = useState(0);\n  const [hoveredWordIndex, setHoveredWordIndex] = useState(null);\n  const [showValues, setShowValues] = useState(false);\n\n  const tokens = useMemo(() => {\n    return inputText.trim().split(/\\s+/).filter(t => t.length > 0);\n  }, [inputText]);\n\n  const attentionMatrix = useMemo(() => {\n    return calculateAttention(tokens, activeHead);\n  }, [tokens, activeHead]);\n\n  const getColor = (val) => {\n    // Transformer visualization standard: Blue-to-White or Purple-to-White\n    return `rgba(96, 165, 250, ${Math.pow(val, 0.7)})`; // Using power to emphasize lower values\n  };\n\n  return (\n    <div className=\"min-h-screen bg-gray-950 text-slate-200 font-sans selection:bg-blue-500/30\">\n      {/* Header */}\n      <header className=\"border-b border-white/10 bg-gray-900/50 backdrop-blur-md sticky top-0 z-50\">\n        <div className=\"max-w-6xl mx-auto px-6 py-4 flex items-center justify-between\">\n          <div className=\"flex items-center gap-3\">\n            <div className=\"p-2 bg-blue-600 rounded-lg shadow-lg shadow-blue-500/20\">\n              <Brain className=\"w-6 h-6 text-white\" />\n            </div>\n            <div>\n              <h1 className=\"text-xl font-bold bg-clip-text text-transparent bg-gradient-to-r from-white to-gray-400\">\n                Transformer Self-Attention\n              </h1>\n              <p className=\"text-xs text-gray-500 font-medium tracking-wider uppercase\">Attention Is All You Need (2017)</p>\n            </div>\n          </div>\n          <div className=\"flex items-center gap-4 text-sm font-medium\">\n            <a href=\"https://arxiv.org/abs/1706.03762\" target=\"_blank\" className=\"text-gray-400 hover:text-white transition-colors\">ArXiv Paper</a>\n          </div>\n        </div>\n      </header>\n\n      <main className=\"max-w-6xl mx-auto px-6 py-8 grid grid-cols-1 lg:grid-cols-12 gap-8\">\n        \n        {/* Left Column: Controls & Input */}\n        <div className=\"lg:col-span-4 space-y-6\">\n          <section className=\"bg-gray-900/50 border border-white/5 rounded-2xl p-5 shadow-xl\">\n            <div className=\"flex items-center gap-2 mb-4 text-blue-400\">\n              <Zap className=\"w-4 h-4\" />\n              <h2 className=\"text-sm font-bold uppercase tracking-widest\">Input Sequence</h2>\n            </div>\n            <textarea\n              value={inputText}\n              onChange={(e) => setInputText(e.target.value)}\n              className=\"w-full h-32 bg-gray-950 border border-white/10 rounded-xl p-4 text-sm focus:ring-2 focus:ring-blue-500 focus:border-transparent outline-none transition-all resize-none\"\n              placeholder=\"Type a sentence to visualize attention...\"\n            />\n            <div className=\"mt-2 text-xs text-gray-500 flex justify-between\">\n              <span>{tokens.length} tokens detected</span>\n              <button onClick={() => setInputText(\"The animal didn't cross the street because it was too tired.\")} className=\"hover:underline\">Reset to example</button>\n            </div>\n          </section>\n\n          <section className=\"bg-gray-900/50 border border-white/5 rounded-2xl p-5 shadow-xl\">\n            <div className=\"flex items-center gap-2 mb-4 text-purple-400\">\n              <Layers className=\"w-4 h-4\" />\n              <h2 className=\"text-sm font-bold uppercase tracking-widest\">Attention Heads (h=8)</h2>\n            </div>\n            <div className=\"grid grid-cols-4 gap-2 mb-6\">\n              {Array.from({ length: 8 }).map((_, i) => (\n                <button\n                  key={i}\n                  onClick={() => setActiveHead(i)}\n                  className={`py-2 rounded-lg text-sm font-bold transition-all ${\n                    activeHead === i \n                    ? 'bg-blue-600 text-white shadow-lg shadow-blue-500/20' \n                    : 'bg-gray-800 text-gray-400 hover:bg-gray-700'\n                  }`}\n                >\n                  {i + 1}\n                </button>\n              ))}\n            </div>\n            <div className=\"bg-blue-500/5 border border-blue-500/20 rounded-xl p-4\">\n              <h3 className=\"text-blue-400 text-sm font-bold mb-1 flex items-center gap-2\">\n                <Info className=\"w-3 h-3\" />\n                {HEAD_DESCRIPTIONS[activeHead].name}\n              </h3>\n              <p className=\"text-xs text-gray-400 leading-relaxed italic\">\n                \"{HEAD_DESCRIPTIONS[activeHead].behavior}\"\n              </p>\n            </div>\n          </section>\n\n          <div className=\"bg-gray-900/50 border border-white/5 rounded-2xl p-5\">\n            <label className=\"flex items-center gap-3 cursor-pointer group\">\n              <div className={`w-10 h-5 rounded-full p-1 transition-colors ${showValues ? 'bg-blue-600' : 'bg-gray-700'}`}>\n                <div className={`w-3 h-3 bg-white rounded-full transition-transform ${showValues ? 'translate-x-5' : 'translate-x-0'}`} />\n              </div>\n              <input type=\"checkbox\" className=\"hidden\" checked={showValues} onChange={() => setShowValues(!showValues)} />\n              <span className=\"text-sm font-medium text-gray-400 group-hover:text-white transition-colors\">Show numerical values</span>\n            </label>\n          </div>\n        </div>\n\n        {/* Right Column: Visualization */}\n        <div className=\"lg:col-span-8 space-y-6\">\n          <div className=\"bg-gray-900/50 border border-white/5 rounded-2xl p-6 shadow-xl relative overflow-hidden\">\n            {/* Legend */}\n            <div className=\"absolute top-6 right-6 flex items-center gap-4 text-[10px] text-gray-500 font-bold uppercase tracking-widest\">\n              <span>Weak Attention</span>\n              <div className=\"flex gap-0.5\">\n                {[0.1, 0.3, 0.5, 0.7, 0.9].map(o => (\n                  <div key={o} className=\"w-4 h-2 rounded-sm\" style={{ backgroundColor: `rgba(59, 130, 246, ${o})` }} />\n                ))}\n              </div>\n              <span>Strong</span>\n            </div>\n\n            <h2 className=\"text-sm font-bold uppercase tracking-widest mb-8 flex items-center gap-2\">\n              <MousePointer2 className=\"w-4 h-4 text-blue-400\" />\n              Interactive Heatmap (Head {activeHead + 1})\n            </h2>\n\n            {/* Matrix Container */}\n            <div className=\"relative overflow-x-auto pb-4 custom-scrollbar\">\n              <div \n                className=\"grid gap-1\"\n                style={{ \n                  gridTemplateColumns: `auto repeat(${tokens.length}, minmax(40px, 1fr))`,\n                  minWidth: `${tokens.length * 40 + 100}px`\n                }}\n              >\n                {/* Header corner */}\n                <div className=\"h-10 w-24\"></div>\n                {/* Column Headers */}\n                {tokens.map((token, idx) => (\n                  <div key={`col-${idx}`} className=\"h-10 text-[10px] font-bold text-gray-500 uppercase flex items-center justify-center -rotate-45 origin-bottom-left translate-y-[-5px]\">\n                    {token.length > 8 ? token.substring(0, 6) + '..' : token}\n                  </div>\n                ))}\n\n                {/* Rows */}\n                {tokens.map((rowToken, i) => (\n                  <React.Fragment key={`row-${i}`}>\n                    {/* Row Label */}\n                    <div className=\"h-10 w-24 flex items-center justify-end pr-4 text-xs font-bold text-gray-400 truncate\">\n                      {rowToken}\n                    </div>\n                    {/* Attention Cells */}\n                    {tokens.map((colToken, j) => {\n                      const value = attentionMatrix[i][j] || 0;\n                      const isTarget = hoveredWordIndex === i;\n                      const isSource = hoveredWordIndex === j;\n                      \n                      return (\n                        <motion.div\n                          key={`${i}-${j}`}\n                          onMouseEnter={() => setHoveredWordIndex(i)}\n                          onMouseLeave={() => setHoveredWordIndex(null)}\n                          className={`h-10 rounded-sm flex items-center justify-center text-[8px] font-mono transition-all duration-200 cursor-crosshair\n                            ${isTarget ? 'ring-1 ring-blue-400/50 z-10' : ''}\n                            ${isSource ? 'ring-1 ring-white/20' : ''}\n                          `}\n                          style={{ \n                            backgroundColor: getColor(value),\n                            color: value > 0.5 ? 'white' : 'rgba(255,255,255,0.4)'\n                          }}\n                        >\n                          {showValues && value.toFixed(2)}\n                        </motion.div>\n                      );\n                    })}\n                  </React.Fragment>\n                ))}\n              </div>\n            </div>\n\n            {/* Hover Explainer */}\n            <AnimatePresence>\n              {hoveredWordIndex !== null && (\n                <motion.div \n                  initial={{ opacity: 0, y: 10 }}\n                  animate={{ opacity: 1, y: 0 }}\n                  exit={{ opacity: 0, y: 10 }}\n                  className=\"mt-8 p-4 bg-gray-950 border border-blue-500/30 rounded-xl\"\n                >\n                  <div className=\"flex items-center gap-3 mb-3\">\n                    <span className=\"text-xs font-bold text-gray-500 uppercase tracking-widest\">Analyzing:</span>\n                    <span className=\"px-3 py-1 bg-blue-600 rounded-md text-white font-bold text-sm\">\n                      {tokens[hoveredWordIndex]}\n                    </span>\n                    <ChevronRight className=\"w-4 h-4 text-gray-600\" />\n                    <span className=\"text-xs text-gray-400\">This word is \"looking at\":</span>\n                  </div>\n                  <div className=\"flex flex-wrap gap-2\">\n                    {tokens.map((token, idx) => {\n                      const weight = attentionMatrix[hoveredWordIndex][idx];\n                      if (weight < 0.05) return null; // Only show significant attention\n                      return (\n                        <div \n                          key={idx} \n                          className=\"flex flex-col items-center bg-gray-900 px-3 py-2 rounded-lg border border-white/5\"\n                          style={{ borderColor: weight > 0.3 ? `rgba(59, 130, 246, ${weight})` : 'transparent' }}\n                        >\n                          <span className=\"text-xs font-bold mb-1\">{token}</span>\n                          <div className=\"w-12 h-1.5 bg-gray-800 rounded-full overflow-hidden\">\n                            <div className=\"h-full bg-blue-500\" style={{ width: `${weight * 100}%` }} />\n                          </div>\n                          <span className=\"text-[10px] text-gray-500 mt-1\">{(weight * 100).toFixed(0)}%</span>\n                        </div>\n                      );\n                    })}\n                  </div>\n                </motion.div>\n              )}\n            </AnimatePresence>\n          </div>\n\n          {/* Math Insights */}\n          <div className=\"grid grid-cols-1 md:grid-cols-2 gap-4\">\n            <div className=\"bg-blue-600/10 border border-blue-500/20 rounded-2xl p-5\">\n              <h3 className=\"text-sm font-bold text-blue-400 uppercase mb-3 flex items-center gap-2\">\n                <Zap className=\"w-4 h-4\" />\n                Scaled Dot-Product\n              </h3>\n              <p className=\"text-xs text-slate-300 leading-relaxed\">\n                The score between two words is computed via <strong>dot product</strong>. \n                Large dk values cause the softmax gradient to vanish, so we scale by \n                <span className=\"font-mono text-blue-300 ml-1\">1/√dk (1/√64 = 0.125)</span>.\n              </p>\n            </div>\n            <div className=\"bg-purple-600/10 border border-purple-500/20 rounded-2xl p-5\">\n              <h3 className=\"text-sm font-bold text-purple-400 uppercase mb-3 flex items-center gap-2\">\n                <Layers className=\"w-4 h-4\" />\n                Multi-Head Logic\n              </h3>\n              <p className=\"text-xs text-slate-300 leading-relaxed\">\n                \"Multi-head attention allows the model to jointly attend to information from \n                different representation subspaces at different positions.\" — <em>Paper Sec 3.2.2</em>\n              </p>\n            </div>\n          </div>\n        </div>\n      </main>\n\n      <style dangerouslySetInnerHTML={{ __html: `\n        .custom-scrollbar::-webkit-scrollbar {\n          height: 6px;\n        }\n        .custom-scrollbar::-webkit-scrollbar-track {\n          background: rgba(255, 255, 255, 0.05);\n          border-radius: 10px;\n        }\n        .custom-scrollbar::-webkit-scrollbar-thumb {\n          background: rgba(255, 255, 255, 0.1);\n          border-radius: 10px;\n        }\n        .custom-scrollbar::-webkit-scrollbar-thumb:hover {\n          background: rgba(255, 255, 255, 0.2);\n        }\n      `}} />\n    </div>\n  );\n}\n",
                  "title": "Interactive Transformer Self-Attention Heatmap"
                }
              }
            }
          ]
        }
      ],
      "created_at": 1770313135994,
      "updated_at": 1770313135994
    }
  }
}