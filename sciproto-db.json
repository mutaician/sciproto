{
  "bdfaa68d8984f0dc02beaca527b76f207d99b666d31d1da728ee0728182df697": {
    "hash": "bdfaa68d8984f0dc02beaca527b76f207d99b666d31d1da728ee0728182df697",
    "filename": "reanalysis.pdf",
    "raw_text": "Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani∗\nGoogle Brain\navaswani@google.com\nNoam Shazeer∗\nGoogle Brain\nnoam@google.com\nNiki Parmar∗\nGoogle Research\nnikip@google.com\nJakob Uszkoreit∗\nGoogle Research\nusz@google.com\nLlion Jones∗\nGoogle Research\nllion@google.com\nAidan N. Gomez∗ †\nUniversity of Toronto\naidan@cs.toronto.edu\nŁukasz Kaiser∗\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n†Work performed while at Google Brain.\n‡Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7 [cs.CL] 2 Aug 2023\n\n-- 1 of 15 --\n\n1 Introduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35 , 2 , 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [ 21 ] and conditional\ncomputation [ 32 ], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [ 2, 19 ]. In all but a few cases [ 27 ], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16 ], ByteNet [ 18 ] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [12 ]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5, 2 , 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2\n\n-- 2 of 15 --\n\nFigure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [ 11 ] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3\n\n-- 3 of 15 --\n\nScaled Dot-Product Attention Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv . We compute the dot products of the\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V ) = softmax( QKT\n√dk\n)V (1)\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof 1\t√dk\n. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3 ]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1\t√dk\n.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv -dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\ni=1 qiki, has mean 0 and variance dk .\n4\n\n-- 4 of 15 --\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\nwhere headi = Attention(QW Q\ni , KW K\ni , V W V\ni )\nWhere the projections are parameter matrices W Q\ni ∈ Rdmodel×dk , W K\ni ∈ Rdmodel×dk , W V\ni ∈ Rdmodel×dv\nand W O ∈ Rhdv ×dmodel .\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndf f = 2048.\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30 ]. In the embedding layers, we multiply those weights by √dmodel.\n5\n\n-- 5 of 15 --\n\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2 · d) O(1) O(1)\nRecurrent O(n · d2) O(n) O(n)\nConvolutional O(k · n · d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nP E(pos,2i) = sin(pos/100002i/dmodel )\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\nP Epos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6\n\n-- 6 of 15 --\n\nlength n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38 ] and byte-pair [31 ] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [6 ], however, decrease the complexity\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38 ]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d−0.5\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4 Regularization\nWe employ three types of regularization during training:\n7\n\n-- 7 of 15 --\n\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel BLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\nTransformer (base model) 27.3 38.1 3.3 · 1018\nTransformer (big) 28.4 41.8 2.3 · 1019\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [ 36 ]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty α = 0.6 [ 38 ]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8\n\n-- 8 of 15 --\n\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN dmodel dff h dk dv Pdrop ϵls train PPL BLEU params\nsteps (dev) (dev) ×106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)\n1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B) 16 5.16 25.1 58\n32 5.01 25.4 60\n(C)\n2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)\n0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37 ]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9\n\n-- 9 of 15 --\n\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al. (2013) [40] semi-supervised 91.3\nHuang & Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10\n\n-- 10 of 15 --\n\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770–778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735–1780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832–841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n11\n\n-- 11 of 15 --\n\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152–159. ACL, June 2006.\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):1929–1958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434–443. ACL, August 2013.\n12\n\n-- 12 of 15 --\n\nAttention VisualizationsInput-Input Layer5\nIt\t\nis\nin\nthis\nspirit\nthat\na\t\nmajority\t\nof\nAmerican\t\ngovernments\t\nhave\npassed\t\nnew\nlaws\nsince\n2009\nmaking\t\nthe\nregistration\t\nor\nvoting\t\nprocess\t\nmore\ndifficult\t\n.\t\n<EOS>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\nIt\t\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\t\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\n13\n\n-- 13 of 15 --\n\nInput-Input Layer5\nThe\nLaw\nwill\nnever\t\nbe\nperfect\t\n,\t\nbut\nits\napplication\t\nshould\t\nbe\njust\n-\t\nthis\nis\t\nwhat\nwe\nare\nmissing\t\n,\t\nin\nmy\nopinion\t\n.\t\n<EOS>\t\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\t\n<pad>Input-Input Layer5\nThe\nLaw\nwill\nnever\t\nbe\nperfect\t\n,\t\nbut\nits\napplication\t\nshould\t\nbe\njust\n-\t\nthis\nis\t\nwhat\nwe\nare\nmissing\t\n,\t\nin\nmy\nopinion\t\n.\t\n<EOS>\t\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\t\n<pad>\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14\n\n-- 14 of 15 --\n\nInput-Input Layer5\nThe\nLaw\nwill\nnever\t\nbe\nperfect\t\n,\t\nbut\nits\napplication\t\nshould\t\nbe\njust\n-\t\nthis\nis\t\nwhat\nwe\nare\nmissing\t\n,\t\nin\nmy\nopinion\t\n.\t\n<EOS>\t\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\t\n<pad>Input-Input Layer5\nThe\nLaw\nwill\nnever\t\nbe\nperfect\t\n,\t\nbut\nits\napplication\t\nshould\t\nbe\njust\n-\t\nthis\nis\t\nwhat\nwe\nare\nmissing\t\n,\t\nin\nmy\nopinion\t\n.\t\n<EOS>\t\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\t\n<pad>\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15\n\n-- 15 of 15 --\n\n",
    "analysis_json": "{\"title\":\"Attention Is All You Need: The Transformer Architecture\",\"summary\":\"This seminal paper introduces the Transformer, a novel neural network architecture for sequence transduction that relies entirely on self-attention mechanisms, eliminating the need for recurrent or convolutional layers. It achieves superior parallelization, faster training times, and state-of-the-art performance in machine translation and constituency parsing.\",\"key_claims\":[\"Recurrent and convolutional operations can be entirely replaced by self-attention without loss of performance.\",\"Self-attention reduces the path length between long-range dependencies to a constant number of operations O(1).\",\"Multi-head attention allows the model to jointly attend to information from different representation subspaces.\",\"The Transformer achieves state-of-the-art BLEU scores on translation tasks with significantly lower training costs (FLOPs) than existing models.\"],\"simulation_possibilities\":[{\"title\":\"Scaled Dot-Product Attention Calculator\",\"description\":\"An interactive simulation of the core attention equation. Users can adjust the dimensionality of keys (dk) and observe how the scaling factor (1/sqrt(dk)) prevents the softmax from entering regions with extremely small gradients. It would visualize the Query, Key, and Value matrices and the resulting attention weights.\",\"complexity\":\"Medium\",\"variables\":[\"dk (Key Dimension)\",\"Query Vector Magnitude\",\"Key Vector Magnitude\",\"Softmax Temperature (Scaling Factor)\"]},{\"title\":\"Multi-Head Attention Subspace Visualizer\",\"description\":\"A prototype showing how a single input sequence is projected into 'h' different subspaces. Users can toggle between heads to see how 'head 1' might focus on syntax while 'head 2' focuses on semantic dependencies, demonstrating the benefit of parallel attention layers.\",\"complexity\":\"High\",\"variables\":[\"h (Number of heads)\",\"d_model (Total dimension)\",\"dk/dv (Per-head dimension)\",\"Input sequence length\"]},{\"title\":\"Sinusoidal Positional Encoding Generator\",\"description\":\"A visualization of the sine and cosine functions used for positional encoding. Users can manipulate the sequence position and the dimension index to see how unique periodic signatures are generated for every token, allowing the model to determine relative positions.\",\"complexity\":\"Low\",\"variables\":[\"pos (Token Position)\",\"d_model (Embedding Dimension)\",\"Geometric Frequency Base (10000)\"]},{\"title\":\"Transformer Optimizer Schedule Simulator\",\"description\":\"An interactive graph of the learning rate schedule used for training. Users can adjust the warmup steps and the d_model constant to see how the learning rate increases linearly then decays according to the inverse square root of the step number.\",\"complexity\":\"Low\",\"variables\":[\"warmup_steps\",\"d_model\",\"Total training steps\"]},{\"title\":\"Encoder-Decoder Cross-Attention Flow\",\"description\":\"A schematic simulation showing the data flow between the encoder and decoder. Users can input a source sequence and observe how the decoder's cross-attention layer queries the encoder's memory keys and values at each auto-regressive step.\",\"complexity\":\"High\",\"variables\":[\"N (Number of layers)\",\"Sequence length\",\"Masking (On/Off)\"]}]}",
    "created_at": 1770277758458
  },
  "605be323c58ade3a9ca930da5c58b8a64b0a788480b5046b7c7c0941ec1b867b": {
    "hash": "605be323c58ade3a9ca930da5c58b8a64b0a788480b5046b7c7c0941ec1b867b",
    "filename": "reanalysis.pdf",
    "raw_text": "Attention Is Not What You Need:\nGrassmann Flows as an Attention-Free Alternative for\nSequence Modeling\nZHANG CHONG\nAbstract\nSelf-attention has become the de facto primitive for modern sequence models. It is often\nimplicitly assumed that strong natural language performance requires attending over all token\npairs via a dense or approximate attention mechanism. In this paper we question that assumption.\nWe reinterpret self-attention as a particular instance of tensor lifting: a hidden vector\nis mapped into a high-dimensional space of pairwise interactions, and learning proceeds by\nconstraining this lifted tensor through gradient descent. This lifting is extremely expressive\nbut also difficult to trace mathematically; across many layers and heads, it becomes almost\nimpossible to describe the model’s behavior with a small family of explicit invariants. From this\nperspective, a central source of “uninterpretability” in large Transformer models is not merely\ntheir size, but the fact that their core operation is a high-dimensional tensor lift whose global\neffect is analytically opaque.\nAs a contrasting design, we propose an attention-free sequence model built around Grassmann\nflows. Instead of forming an L×L attention matrix, we (i) reduce token states to a low-dimensional\nspace, (ii) interpret local token pairs as two-dimensional subspaces on a Grassmann manifold\nGr(2, r), (iii) embed these subspaces via Pl¨ucker coordinates into a finite-dimensional projective\nspace, and (iv) fuse the resulting geometric features back into the hidden states through a gated\nmixing block. Information flows through the sequence not by explicit pairwise weights, but by\ncontrolled deformations of low-rank subspaces across layers and multi-scale local windows.\nWe evaluate this Causal Grassmann architecture on language modeling (Wikitext-2) and\nnatural language inference (SNLI). On Wikitext-2, a purely Grassmann-based language model\nwith 13–18M parameters achieves validation perplexities within 10–15% of a size-matched\nTransformer baseline. On SNLI, a Grassmann-based classification head on top of DistilBERT\nslightly outperforms a Transformer head (best validation accuracy 0.8550 vs. 0.8545; test accuracy\n0.8538 vs. 0.8511). Complexity analysis shows that our mixing mechanism scales linearly in\nsequence length for fixed rank, in contrast to the quadratic cost of full self-attention.\nOur aim is not to claim that attention is obsolete, but to de-center it. We argue that what\none fundamentally needs is not attention itself, but a sufficiently expressive geometric evolution\nmechanism for hidden representations. Grassmann flows show that competitive performance can\nbe obtained without any explicit attention weights, while moving the core of the model into a\nmathematical setting—flows on a finite-dimensional manifold—that is more amenable to explicit\ninvariants and geometric analysis.\n1 Introduction\nTransformers[ 1 , 2, 4 , 5 ] have reshaped sequence modeling by making self-attention the central\nprimitive. Given a sequence of hidden states H ∈ RL×d, self-attention constructs queries, keys, and\nvalues\nQ = HWQ, K = HWK , V = HWV\n1\narXiv:2512.19428v1 [cs.LG] 22 Dec 2025\n\n-- 1 of 17 --\n\nand computes an L × L matrix of pairwise compatibilities QK⊤, normalized by a softmax to form\nweights, which are then used to mix the values. This operation is expressive, parallelizable, and has\nbecome so ubiquitous that it is often treated as indispensable.\nIn this work we take a different stance. Rather than asking how to make attention cheaper,\nsparser, or more scalable, we ask a more basic question:\nIs explicit self-attention, as an L × L tensor of weights, really the fundamental ingredient we need\nfor strong sequence modeling and reasoning?\nOur answer is “no”. Attention is one particular way to implement a geometric lifting of the\nrepresentation; it is not the only way.\n1.1 Attention as tensor lifting\nConceptually, self-attention performs a form of tensor lifting. From a hidden vector ht ∈ Rd at\nposition t, we move to a space that encodes pairwise relations between tokens—an L×L compatibility\ntensor (per head) and its normalized weights. This lifting has several key properties:\n• It is extremely fine-grained: every pair of positions (i, j), in every head, in every layer,\nreceives a separate learned weight.\n• It is high-dimensional: the effective state of the model involves not only the token embeddings\nbut also an evolving cloud of attention tensors across layers.\n• It is hard to compress: there is no obvious small set of global invariants that summarize\nattention behavior across all layers and heads.\nFrom a geometric viewpoint, one can say that self-attention lifts the sequence from a manifold of\ntoken representations into a much larger space of pairwise interactions, performs operations there,\nand then projects back. The success of Transformers suggests that this lifting is powerful; but it\nalso suggests a reason why they are so difficult to interpret:\nClaim. A major source of large-model “uninterpretability” is that the tensor lifting performed by\nattention is mathematically non-traceable: the degrees of freedom introduced at each layer and\nhead are so large that we lack a small, explicit family of invariants that can describe the global\neffect of the model.\nIn other words, the core of the model lives in a high-dimensional tensor space that resists concise\nanalytic description. Visualizing individual attention maps is possible, but aggregating them into a\ncoherent global picture is not.\n1.2 Reasoning as geometry on a semantic manifold\nAn alternative starting point is to think of reasoning as geometry on a semantic manifold. At a\nhigh level, we take the following view:\n1. Hidden states of a language model can be seen as points on a high-dimensional semantic\nmanifold. Each forward pass traces a path on this manifold.\n2. Attention implements a specific kind of tensor lifting: it takes the vector ht and, via dot\nproducts with other positions, lifts its information into a space of pairwise interactions to\nuncover richer local geometry.\n2\n\n-- 2 of 17 --\n\n3. The effectiveness of Transformers relies on how subsequent neural layers align and constrain\nthis lifted geometry. The network learns to restrict the otherwise wild degrees of freedom of\nattention to flows that support useful behavior.\n4. Reasoning, in this picture, is the process of repeatedly sampling and refining the intrinsic\ngeometric structure of the semantic manifold: we apply layer after layer of operators that\ndeform the representation in structured ways.\nWithin this view, the central issue is not whether we lift to a tensor space, but how we design\nthe geometric evolution rule for representations. Self-attention is one such rule, but it is opaque: the\nlifted tensor is so rich that its long-range behavior becomes extremely hard to trace in mathematical\nterms.\nThis suggests the following sharper philosophical claim:\nClaim. The non-interpretability of large Transformers is not merely due to depth or parameter\ncount; it is rooted in the choice of tensor lifting as the core operation. Once we lift to an L × L\nattention tensor, we have already lost the possibility of a simple, explicit, global description in terms\nof a small set of invariants.\nIf we want a more mathematically structured view of reasoning, we may want to replace this\nopaque tensor lifting with a more controlled geometric object.\n1.3 Grassmann flows as a controlled alternative\nThis paper explores an alternative design: instead of lifting to an attention tensor, we lift to a\nGrassmann manifold. The construction is straightforward:\n• We first apply a learned linear map Wred to reduce ht ∈ Rd to zt ∈ Rr, with r ≪ d.\n• For local windows (e.g., pairs (t, t + ∆)), we consider the subspace spanned by {zt, zt+∆} in\nRr and treat it as a point on the Grassmann manifold Gr(2, r).\n• Using the Pl¨ucker embedding, each such 2D subspace is mapped to a coordinate vector in\nR(r\n2), subject to known algebraic relations.\n• These Pl¨ucker coordinates become geometric features that we project back into Rd and fuse\nwith the original hidden states via learned gating.\nThe key difference from attention is that we now constrain the model to operate on a manifold\nwith explicit, finite-dimensional structure:\n• The degrees of freedom are controlled by r and the choice of window sizes; there is no L × L\ntensor.\n• The representation of local geometry lives in a space with explicit algebraic constraints\n(Grassmannian + Pl¨ucker relations)[11, 12, 13, 14].\n• We can in principle study the evolution of these Grassmann features across layers as a\nfinite-dimensional flow on a manifold.\nWe refer to the resulting architecture as a Causal Grassmann model or Grassmann flow.\nImportantly, it is fully attention-free: there is no step in which we construct an attention matrix or\ncompute softmax-normalized tensor weights.\n3\n\n-- 3 of 17 --\n\n1.4 Contributions\nThe contributions of this paper are:\n1. A conceptual critique of self-attention. We frame attention as a tensor lifting mechanism\nand argue that a major source of Transformer uninterpretability is that this lifting is mathe-\nmatically non-traceable: the model’s core computations live in an extremely high-dimensional\nspace with no small set of invariants.\n2. An attention-free architecture based on Grassmann flows. We propose a Causal\nGrassmann mixing layer that (i) reduces token states, (ii) encodes local pairs as points\non Gr(2, r) via Pl¨ucker coordinates, and (iii) fuses these geometric features back into the\nrepresentation, all without explicit attention weights.\n3. Empirical evidence on Wikitext-2 and SNLI. We show that a pure Grassmann language\nmodel with 13–18M parameters is competitive with a Transformer baseline on Wikitext-2,\nand that a Grassmann-based NLI head slightly outperforms a Transformer head on SNLI.\n4. Complexity and interpretability analysis. We analyze the asymptotic complexity of\nGrassmann mixing and show that, for fixed rank and window sizes, it scales linearly in sequence\nlength, in contrast to the quadratic scaling of full attention. We also argue that operating\non a Grassmann manifold makes it more realistic to define global invariants over the model’s\nbehavior.\nOur goal is not to replace attention everywhere, but to open up a different region of the design\nspace: architectures where the core sequence interaction is governed by explicit geometric flows and\nmanifolds rather than opaque tensor lifting.\n2 Attention as Geometric Lifting and Grassmann Background\nIn this section we make the geometric perspective on attention more precise, and introduce the\nGrassmannian structures that underlie our model.\n2.1 Self-attention as geometric lifting\nGiven a sequence of hidden states H ∈ RL×d, a standard multi-head self-attention layer computes\nfor each head h:\nQh = HW (h)\nQ , (1)\nKh = HW (h)\nK , (2)\nVh = HW (h)\nV , (3)\nwith W (h)\nQ , W (h)\nK , W (h)\nV ∈ Rd×dh and typically dh = d/Hheads.\nFor each head, we then compute\nAh = softmax\n\u0012 QhK⊤\nh\t√dh\n\u0013\n∈ RL×L,\nand obtain the head output\nOh = AhVh ∈ RL×dh .\n4\n\n-- 4 of 17 --\n\nThe outputs Oh are concatenated and linearly projected back to Rd.\nThis process can be interpreted as follows:\n• The linear maps W (h)\nQ , W (h)\nK embed the sequence into a representation where dot products\nencode compatibilities between positions.\n• The matrix QhK⊤\nh is a lift from H into a space of pairwise interactions: each token is\nrepresented not just by a vector, but by its similarities to all other tokens.\n• The softmax and multiplication by Vh implement a specific geometric operation on this lifted\nstructure, redistributing information according to these compatibilities.\nGeometrically, the model is no longer simply moving along a manifold of hidden states; it is also\nmodifying a cloud of pairwise relations whose dimensionality grows quadratically with sequence\nlength. Across multiple heads and layers, this cloud becomes extremely complex. While we can\nvisualize individual attention maps, the global behavior of the model is governed by a composition\nof many such lifts, making it hard to summarize with concise invariants.\n2.2 Grassmann manifolds and Pl¨\tucker coordinates\nThe Grassmann manifold Gr(k, r) is the set of all k-dimensional linear subspaces of Rr. It is a smooth\nmanifold of dimension k(r − k). For our purposes, we focus on k = 2, so Gr(2, r) parameterizes all\n2D subspaces in Rr, with dimension 2(r − 2).\nThere are several standard ways to represent points on a Grassmannian. We use the Pl¨ucker\nembedding, which maps each k-dimensional subspace to a point in projective space:\n• Given a basis (u1, . . . , uk) of a k-dimensional subspace U ⊂ Rr, we form the exterior product\nu1 ∧ · · · ∧ uk in the k-th exterior power ΛkRr.\n• In coordinates, u1 ∧ · · · ∧ uk can be represented as a vector in R(r\nk) whose entries are the k × k\nminors of the matrix [u1 . . . uk].\n• For k = 2, this is particularly simple: if u, v ∈ Rr span a 2D subspace, then u ∧ v is given by\nall pairwise determinants\npij = uivj − uj vi, 1 ≤ i < j ≤ r,\nforming a vector p ∈ R(r\n2).\nThe image of Gr (2, r) under this embedding is not all of R(r\n2) ; it is an algebraic variety defined\nby the quadratic Pl¨ucker relations. Nonetheless, for our purposes we can simply regard p as a\nnormalized feature vector encoding the subspace spanned by u and v. Different bases spanning the\nsame subspace yield proportional Pl¨ucker vectors, reflecting the projective nature of the embedding.\nWe will use this representation to encode the local geometry of pairs of token vectors after a\nlinear dimensionality reduction.\n2.3 Why Grassmann for sequence modeling?\nWhy choose Grassmann manifolds as the backbone of our mixing rule, instead of, say, a different\nmanifold or a more ad hoc feature transform? There are several reasons.\n5\n\n-- 5 of 17 --\n\nLocal linear structure. On a smooth manifold, local geometry can be captured by tangent spaces\nand their subspaces. Grassmannians naturally parameterize families of linear subspaces, making\nthem well suited to represent local linear approximations of more complex structures. When we\ntake pairs of reduced hidden states and form their span, we are effectively encoding local directions\nand planes in the semantic manifold.\nFinite-dimensional algebraic structure. The Grassmann manifold is finite-dimensional and\nsits inside a projective space via the Pl¨ucker embedding. This means we can encode geometric\ninformation in a fixed-dimensional feature vector subject to known algebraic constraints. Neural\nnetworks can operate on these features while the underlying object remains a subspace with clear\ngeometric meaning.\nCompatibility with approximation theorems. From the perspective of real analysis, we may\nidealize the semantic space as a manifold M and the model as an operator Φ : M → M . Classic\nuniversal approximation theorems tell us that neural networks can approximate such operators\ngiven enough capacity, but they do not prescribe any particular geometry. By constraining our\nmixing rule to first encode local neighborhoods into Gr(2, r) and then act on that manifold, we are\neffectively requiring that the model’s dynamics factor through a structured manifold with controlled\ndegrees of freedom. Universal approximation still applies, but the approximation now unfolds on a\nspace whose structure we can analyze.\nTaken together, these properties make Grassmannians a natural choice if we want to replace\nunstructured tensor lifting with a more controlled, interpretable, geometric primitive.\n3 Methods: A Causal Grassmann Transformer without Attention\nWe now describe the architecture in detail. Our design follows the broad outline of a Transformer\nencoder, but replaces each self-attention block with a Causal Grassmann mixing block that:\n1. reduces hidden states to a low-dimensional space,\n2. constructs local pairs and encodes them as Pl¨ucker vectors in R(r\n2), and\n3. mixes these geometric features back into the original hidden states via gating and a feed-forward\nnetwork.\n3.1 Token and positional embeddings\nFor language modeling, we consider a standard next-token LM setup over a vocabulary of size V .\nGiven a sequence of tokens (x1, . . . , xL), we embed them into Rd using a learned embedding matrix\nE ∈ RV ×d and add positional embeddings P ∈ RLmax×d:\nh(0)\nt = E(xt) + Pt, t = 1, . . . , L.\nThroughout our experiments we use d = 256. The resulting sequence H(0) = (h(0)\n1 , . . . , h(0)\nL ) is fed\nthrough N stacked Causal Grassmann Transformer layers.\n3.2 Causal Grassmann mixing layer\nEach layer takes as input H ∈ RL×d and outputs an updated sequence ˜\tH ∈ RL×d. The core\noperations within the layer are:\n6\n\n-- 6 of 17 --\n\n1. Linear reduction. We first reduce each hidden state to a low-dimensional vector:\nzt = Wredht + bred, Wred ∈ Rr×d, bred ∈ Rr.\nTypically r ≪ d; in our experiments we use r = 32. This yields Z = (z1, . . . , zL) ∈ RL×r.\n2. Multi-scale local pairing. We define a set of window sizes (offsets) W = {∆1, . . . , ∆m}, e.g.,\nW = {1, 2, 4, 8, 12, 16}\nor a multi-layer schedule such as (1, 1, 2, 2, 4, 4, 8, 8, 12, 12, 16, 16) for deeper models. For each\nposition t and offset ∆ ∈ W such that t + ∆ ≤ L, we form a pair (zt, zt+∆).\nFor a given t, this produces up to m pairs:\n(zt, zt+∆1 ), (zt, zt+∆2 ), . . . .\nWe treat these as local neighborhoods at multiple scales. Note that the pairing is causal : we only\npair t with positions strictly to its right (future), consistent with left-to-right language modeling.\n3. Grassmann / Pl¨ucker encoding. For each pair (zt, zt+∆), we consider the 2D subspace\nspanned by these vectors in Rr. We form the Pl¨ucker vector p(∆)\nt ∈ R(r\n2) whose entries are:\np(∆)\nij (t) = zt,izt+∆,j − zt,j zt+∆,i, 1 ≤ i < j ≤ r.\nWe then apply an optional normalization, e.g.,\nˆp(∆)\nt = p(∆)\nt\nmax(∥p(∆)\nt ∥2, ε)\nfor numerical stability. This yields a set of Pl¨ucker features for each t and ∆.\n4. Projection back to model space. We project these Grassmann features back into the model\ndimension via a learned linear map:\ng(∆)\nt = Wpl¨u ˆp(∆)\nt + bpl¨u, Wpl¨u ∈ Rd×(r\n2).\nWe then aggregate across offsets, e.g. by summation or averaging:\ngt = 1\n|Wt|\nX\n∆∈Wt\ng(∆)\nt ,\nwhere Wt = {∆ ∈ W : t + ∆ ≤ L} is the set of valid offsets at position t.\nThe vector gt ∈ Rd captures multi-scale local Grassmann geometry around position t.\n5. Gated fusion. We concatenate the original hidden state and the Grassmann feature and\ncompute a gate:\nut = [ht; gt] ∈ R2d,\nαt = σ(Wgateut + bgate), Wgate ∈ Rd×2d.\nThe mixed representation is ˜hmix\nt = αt ⊙ ht + (1 − αt) ⊙ gt,\nfollowed by a layer normalization and dropout:\nˆht = LayerNorm(˜hmix);\nt ˆht = Dropout(ˆht).\n7\n\n-- 7 of 17 --\n\n6. Feed-forward block. As in a standard Transformer, we apply a position-wise feed-forward\nnetwork:\nϕt = W2 σ(W1ˆht + b1) + b2,\nwith W1 ∈ Rdff×d, W2 ∈ Rd×dff , dff = 4d, and a nonlinearity σ (we use GELU). Another residual\nconnection and layer normalization complete the layer:\nh′\nt = LayerNorm(ˆht + ϕt).\nStacking N such layers yields the full Causal Grassmann Transformer.\n3.3 Comparison to self-attention\nFor a sequence of length L and hidden dimension d, a standard multi-head self-attention layer has\ntime complexity\nO(Ld2 + L2dhead),\nwhere dhead is the per-head dimension. The first term arises from computing Q, K, V , and the\nsecond from the matrix multiplication QK⊤ (size L2) and the subsequent multiplication of the\nL × L attention matrix by V .\nIn our Grassmann mixing layer, the main costs are:\n• Linear reduction: HW ⊤\nred costs O(Ldr).\n• Pl¨ucker computation: for each position and offset, forming p(∆)\nt costs O(r2). With m = |W|\noffsets, this contributes O(Lmr2).\n• Projection to model space: Wpl¨u ˆp(∆)\nt costs O(d\u0000r\n2\n\u0001) per pair, giving O(Lmdr2).\n• Gating and feed-forward: both are O(Ld2), as in a standard Transformer.\nIf we treat r and m as fixed hyperparameters (which they are in our experiments), r ≪ d, and\nnote that r2 is modest, then the Pl¨ucker and projection costs can be absorbed into the O(Ld2) term.\nCrucially, there is no L2 term: the complexity is linear in L for fixed r and m:\nCausal Grassmann: O(Ld2) vs. Self-attention: O(L2dhead + Ld2).\nIn practice, our current implementation is slower per step than highly optimized GPU attention\nkernels for moderate L, due to overheads in computing Pl¨ucker coordinates and handling reshapes.\nHowever, asymptotically in L, and with further engineering, the Grassmann layer can in principle\nbe more scalable.\n4 Experimental Setup\nWe evaluate the proposed Causal Grassmann architecture on two standard NLP benchmarks:\nWikitext-2 for language modeling and SNLI for natural language inference [6, 9].\n4.1 Wikitext-2 language modeling\nData and tokenization. We use the Wikitext-2-raw dataset. Sequences are formed by contiguous\nchunks of text of fixed length L (block size). In our main experiments, we consider L = 128 and\nL = 256. We use a WordPiece-like vocabulary of size V ≈ 30,522, matching a BERT-style tokenizer.\n8\n\n-- 8 of 17 --\n\nModels. We compare:\n• TransformerLM: a standard decoder-only Transformer with N layers, model dimension\nd = 256, feed-forward dimension dff = 1024, and multi-head self-attention with 4 heads.\n• GrassmannLM: the same backbone (embeddings, number of layers, d, dff), but with each\nself-attention block replaced by a Causal Grassmann mixing block as described above.\nWe explore two layer depths:\n• Shallow : N = 6 layers; GrassmannLM has ∼ 13.0M parameters, TransformerLM ∼ 12.6M.\n• Deeper : N = 12 layers; GrassmannLM has ∼ 18.2M parameters, TransformerLM ∼ 17.3M.\nFor GrassmannLM we set the reduced dimension r = 32 and use multi-scale windows such as\nW = {1, 2, 4, 8, 12, 16}\nfor 6-layer models, and repeated patterns (1, 1, 2, 2, 4, 4, 8, 8, 12, 12, 16, 16) across depth for 12-layer\nmodels.\nTraining. We train both models with the same optimizer and learning rate schedule in a shared\nscript, differing only in the choice of mixing block. All models are trained for 30 epochs, and we\nreport the best validation perplexity over training. Batch size is 32 for L = 128 and 16 for L = 256\nin our main configurations.\n4.2 SNLI natural language inference\nData. We use the SNLI dataset, which consists of sentence pairs labeled as entailment, contradic-\ntion, or neutral. We follow a standard train/validation/test split.\nBackbone. For fair comparison, we fix a DistilBERT-base-uncased backbone as a feature extractor.\nThe backbone produces contextualized token embeddings up to a maximum sequence length of\n48 tokens per sentence (after tokenization and truncation). We then apply pooling to obtain\nsentence-level representations.\nClassification heads. On top of the DistilBERT backbone we compare:\n• Transformer head: a 2-layer Transformer-style classifier with self-attention over the pooled\nfeatures and a final linear layer for 3-way classification.\n• Grassmann–Pl¨ucker head: our proposed Grassmann-based head (GrassmannPluckerNLIModel),\nwhich applies a Grassmann mixing module over multi-scale windows to the projected features\nand then uses a feed-forward classifier.\nThe Grassmann head uses the following hyperparameters: reduced dimension dproj = 64, window\nsize 8 with stride 8 over the token sequence, dmodel = 256, 2 mixing layers, 4 mixing heads (for\ngrouping pairs), dff = 512, and dropout 0.1. Both heads have comparable parameter counts, and\nboth are trained for 20 epochs from the same initialization of the backbone.\n9\n\n-- 9 of 17 --\n\nModel Layers Params (M) Val PPL\nTransformerLM (block size 128) 6 12.59 248.4\nGrassmannLM (block size 128) 6 13.00 275.7\nTransformerLM (block size 128) 6 12.59 253.6\nGrassmannLM (block size 128) 6 13.00 282.3\nTable 1: Wikitext-2: 6-layer models with block size 128 and two different multi-scale window\nschedules. The GrassmannLM trails the TransformerLM by about 10–15% in validation perplexity\nbut remains in the same overall regime.\nModel Layers Params (M) Val PPL\nTransformerLM (block size 256) 12 17.32 235.2\nGrassmannLM (block size 256) 12 18.16 261.1\nTable 2: Wikitext-2: 12-layer models with block size 256 and repeated multi-scale windows\n(1, 1, 2, 2, 4, 4, 8, 8, 12, 12, 16, 16) over depth. The GrassmannLM is again within roughly 10% of the\nTransformerLM.\nMetrics. We report classification accuracy on the validation and test sets, along with training\nloss curves.\n5 Results\n5.1 Wikitext-2 language modeling\nTables 1 and 2 summarize our main language modeling results. We report parameter counts and\nbest validation perplexity over 30 epochs.\nFor 6-layer models with block size 128 and multi-scale windows W = {1, 2, 4, 8, 12, 16}, the\nGrassmannLM attains a best validation perplexity of approximately 275.7, compared to 241.0–253.6\nfor the TransformerLM under matched training conditions. With a slightly different window schedule\n(e.g., {1, 2, 4, 8, 8, 8}), we see similar gaps: GrassmannLM at 282.3 vs. TransformerLM at 248.4.\nFor deeper 12-layer models with block size 256 and a repeated multi-scale window pattern, the\nGrassmannLM reaches a best validation perplexity of 261.1, while the TransformerLM reaches 235.2.\nThe relative gap is smaller than in the 6-layer setting, suggesting that additional depth helps the\nGrassmann model compensate for its more localized mixing.\nOverall, across these configurations:\n• The GrassmannLM is consistently within 10–15% of a size-matched TransformerLM in valida-\ntion perplexity, despite using no attention.\n• The gap appears to narrow as depth increases, which is consistent with the view that repeated\nlocal Grassmann mixing can approximate richer interactions.\n• Parameter counts remain comparable: the GrassmannLM has slightly more parameters due to\nthe Pl¨ucker projection and gating layers, but the difference is on the order of ∼ 3–5%.\nThese results are not intended to be competitive with state-of-the-art language models, but to\ndemonstrate that “attention-free” sequence modeling via Grassmann flows is viable at moderate\nscales.\n10\n\n-- 10 of 17 --\n\nHead type Val accuracy Test accuracy\nTransformer head 0.8545 0.8511\nGrassmann–Pl¨ucker head 0.8550 0.8538\nTable 3: SNLI classification accuracy with DistilBERT backbone. The Grassmann–Pl¨ucker head\nslightly outperforms the Transformer head on both validation and test sets.\n5.2 SNLI natural language inference\nTable 3 summarizes our SNLI results. Recall that both models share the same DistilBERT backbone\nand differ only in the classification head.\nThe Grassmann head achieves a best validation accuracy of 0.8550 and test accuracy of 0.8538,\nslightly outperforming the Transformer head, which reaches 0.8545 validation and 0.8511 test\naccuracy. Training curves show similar convergence rates, with the Grassmann head exhibiting\nslightly lower validation loss late in training.\nAlthough the margin is small, this result is important conceptually:\n• It shows that on a downstream reasoning task, injecting explicit geometric structure into the\nclassification head can match or slightly exceed a Transformer head, even when the backbone\nis fixed.\n• It indicates that the Grassmann mechanism is not merely a theoretical curiosity but can\ncontribute positively to performance in practical settings.\n5.3 Complexity and empirical runtime\nAs discussed earlier, the asymptotic complexity of the Causal Grassmann layer is linear in sequence\nlength L for fixed reduced dimension r and number of windows m, whereas self-attention scales\nquadratically in L due to the L × L attention matrix.\nIn our current implementation, however, the empirical per-step runtime of the pure Grassmann\nmodel is slower than that of the Transformer baseline for sequence lengths up to 256. This is\nexpected:\n• GPU libraries provide highly optimized kernels for dense matrix multiplications and attention\nmechanisms.\n• Our Pl¨ucker computation involves explicit element-wise operations and reshaping that do not\nyet exploit low-level kernel fusion or custom CUDA implementations.\nThe experiments reported here should therefore be interpreted as a proof of concept for the\narchitecture and its complexity profile, not as an optimized engineering solution. A dedicated\nimplementation that fuses the Grassmann operations and leverages the structure of Gr (2, r) would\nbe required to fully realize the potential linear scaling benefits in practice.\n6 Discussion\n6.1 What does Grassmann mixing actually buy us?\nOur experiments demonstrate that a purely geometric, locality-based mixing rule can support\nnon-trivial language modeling and natural language inference, without relying on explicit self-\n11\n\n-- 11 of 17 --\n\nattention. With relatively small models and modest context lengths, the proposed Causal Grassmann\narchitecture:\n• remains competitive with a size-matched Transformer on Wikitext-2, and\n• slightly outperforms a Transformer classification head on SNLI when used as a DistilBERT-\nbased NLI model.\nFrom an engineering perspective, this shows that attention is not strictly necessary for competent\nsequence modeling at this scale. From a conceptual perspective, it supports a more subtle claim:\nClaim. As long as the model is endowed with a geometrically rich enough local evolution rule,\nsemantic reasoning can emerge even without explicit attention weights.\nSelf-attention lets each token see every other token via a learned L×L weight matrix. Grassmann\nmixing, by contrast, constructs a sequence of local subspace updates: information flows by rotating\nand bending low-rank subspaces over multi-scale windows. Both mechanisms accumulate higher-order\ngeometric structure across layers, but with different primitives:\n• Self-attention uses tensor lifting and global pairwise interactions;\n• Grassmann mixing uses low-rank subspaces and local windows as a controlled flow on a\nmanifold.\nAt our current scale, the Grassmann models do not surpass Transformers on language modeling;\nthey are slightly behind. This is not surprising given the simplicity of our design and the lack of\nextensive hyperparameter tuning. Nevertheless, the SNLI results show that when the backbone is\nfixed and we focus on the head, adding explicit geometry can yield measurable gains. This suggests\nthat the geometric perspective is not only philosophically appealing but practically useful.\n6.2 Interpretability: from tensor lifting to finite-dimensional flows\nWe argued in the introduction that a central reason for Transformer uninterpretability is the nature\nof attention as tensor lifting. Each layer lifts the representation into a high-dimensional space of\npairwise interactions; the overall model is a composition of such lifts. Although each individual\nattention map is visible, the global behavior is difficult to summarize in terms of a small set of\ninvariants.\nIn contrast, the Grassmann architecture deliberately compresses the relevant degrees of freedom\ninto a finite-dimensional, mathematically rigid manifold:\n• The reduced states zt ∈ Rr capture local directions in a lower-dimensional space.\n• Pairs (zt, zt+∆) define points on Gr(2, r); these points are encoded by Pl¨ucker vectors with\nfixed dimension \u0000r\n2\n\u0001.\n• The mixing process is constrained to local deformations of these low-rank subspaces, rather\nthan arbitrary manipulations of an L × L tensor.\nThis suggests a more hopeful interpretability story. After training, one can treat the Pl¨ucker\nvectors or other Grassmann descriptors as candidate explanatory invariants:\n• They are finite in number and obey explicit algebraic relations.\n12\n\n-- 12 of 17 --\n\n• They are comparable across layers.\n• They can be studied with tools from differential geometry and algebraic geometry.\nThis does not make interpretability trivial. However, it moves the core of the model into a\ndomain where there is at least a realistic prospect of defining and computing global invariants.\nInstead of trying to summarize an evolving collection of attention tensors, we can try to summarize\nan evolving trajectory on a manifold Gr(2, r).\n6.3 Why Grassmann? A link to approximation theorems\nFrom the perspective of approximation theory, we may idealize a sequence model as approximating\nan operator Φ on a semantic manifold M ⊂ Rd. Universal approximation theorems ensure that,\nunder mild conditions, neural networks can approximate such operators arbitrarily well.\nThose theorems, however, are agnostic about the architecture’s geometric structure. They do\nnot distinguish between models that operate on unstructured tensors and those that operate on\nstructured manifolds. Our choice of Grassmann manifolds can be viewed as imposing an additional,\ngeometry-aware bias:\n• We first encode local neighborhoods of M into subspaces in Gr (2, r) via linear reduction and\nwedge products.\n• We then approximate the induced transformation on the Grassmann manifold using MLPs\nand gating.\n• Finally, we map back to the original representation space.\nIn this sense, we are not changing the fundamental approximation capacity—the network\nremains universal in principle—but we are constraining the way it realizes that capacity. All\nnon-local interactions must factor through a finite-dimensional manifold with explicit structure.\nThis is precisely what attention does not enforce: attention allows a very free exploration of a\nhigh-dimensional tensor space.\n6.4 Global and long-range invariants as the next step\nThe present Causal Grassmann design uses only local windows. Long-range dependencies are\nmodeled implicitly through depth and multi-scale windows. This suffices for the tasks studied here,\nbut it suggests a natural next step, in line with the intuition that motivated this work:\nConstruct explicit global or long-range invariants of the sequence-level Grassmann flow and feed\nthem back as features.\nFor example, one could compute:\n• A “mean Grassmann direction” summarizing the overall trajectory of subspaces across a\nsequence.\n• Sequence-level statistics of Pl¨ucker coordinates, such as principal directions or curvature-like\nquantities.\n• Cross-layer invariants that measure how stable certain subspaces are across depth.\n13\n\n-- 13 of 17 --\n\nThese invariants could then be injected into each layer as auxiliary inputs or gates, turning the\narchitecture into a system where local flows are guided by global constraints. This would echo the\ninterplay between local and global structures in information geometry, where local metrics (e.g.,\nFisher information) and global curvature jointly shape inference.\nIn this paper we deliberately restricted ourselves to a minimal design—k = 2, no explicit global\ninvariants—to keep the core idea clear. But we view “global invariants + local Grassmann flow” as\na promising direction for future work on geometry-aware reasoning.\n7 Related Work\n7.1 Efficient and long-context Transformers\nA large body of work aims to reduce the quadratic cost of self-attention, including linearized or\nkernelized attention, sparse and local attention patterns, and memory-augmented or retrieval-based\narchitectures. These approaches typically:\n• approximate or sparsify the QK⊤ computation,\n• restrict attention to local windows or structured patterns, or\n• offload parts of the context into external memories or caches.\nAll of them, however, retain the same core operation: the model still computes (or approximates)\nan L × L matrix of pairwise weights. Our work is orthogonal: we remove attention entirely and\nexplore whether a geometric mixing rule based on Grassmann flows can fill its role.\n7.2 State-space models and structured sequence models\nState-space models and related architectures interpret sequences as signals governed by linear\ndynamical systems, often combined with non-linear readout. These models excel at long-context\nmodeling with linear complexity in sequence length and have strong ties to control theory and signal\nprocessing.\nGrassmann mixing shares with state-space models the idea of maintaining a structured latent\nstate that evolves over time, but the emphasis is different:\n• SSMs focus on temporal evolution of a latent state; their geometry is often implicit.\n• Grassmann mixing focuses on geometric evolution in representation space, with time entering\nthrough causal windows.\nThe two perspectives are complementary, and hybrid architectures that combine SSM-style\ntemporal dynamics with Grassmann constraints on hidden representations are an interesting direction\nfor future work.\n7.3 Geometric and manifold-based representation learning\nThere is growing interest in learning on non-Euclidean spaces, including hyperbolic, spherical, and\nother Riemannian manifolds. These approaches typically embed data into a curved manifold where\ndistances better capture the underlying structure (e.g., hierarchies, periodicities).\n14\n\n-- 14 of 17 --\n\nGrassmann manifolds have appeared in classical machine learning contexts such as subspace\nclustering, low-rank approximation, and metric learning, where they represent sets of subspaces.\nTheir use as a primary mixing mechanism in sequence models has been less explored. Our contribution\nis to integrate a Grassmann–Pl¨ucker pipeline directly into a Transformer-like block, turning subspace\ngeometry into a core part of the sequence interaction mechanism.\n7.4 Interpretability and attention analysis\nAttention maps are often used as a proxy for interpretability in Transformers: we visualize which\ntokens attend to which others. However, attention weights are not guaranteed to align with causal\nimportance, and aggregating them across layers and heads leads to highly complex patterns.\nOur work does not introduce a new interpretability method per se, but it changes the object of\nanalysis. Instead of trying to make sense of attention tensors, we propose to analyze the evolution\nof Grassmann features. These are finite-dimensional, structured objects that may lend themselves\nmore naturally to geometric or algebraic analysis.\n8 Conclusion and Future Work\nWe revisited a simple but fundamental question: is explicit self-attention, as commonly implemented\nin Transformers, really necessary for strong sequence modeling and reasoning?\nBy reinterpreting attention as a form of tensor lifting, we argued that its power comes at the cost\nof mathematical traceability: the core of the model lives in a high-dimensional tensor space whose\nglobal behavior is difficult to summarize with explicit invariants. We then proposed an alternative\nin which the sequence interaction is governed by flows on a Grassmann manifold rather than by an\nL × L attention matrix.\nThe resulting Causal Grassmann architecture:\n• is fully attention-free, yet competitive with a Transformer baseline on Wikitext-2 at 13–18M\nparameters,\n• slightly outperforms a Transformer-based classification head on SNLI when plugging into a\nfixed DistilBERT backbone, and\n• has an asymptotic complexity that is linear in sequence length for fixed reduced dimension\nand window sizes.\nBeyond these empirical results, the main contribution is conceptual: Grassmann flows offer a\nconcrete example of how one can design sequence models whose core operations live on a finite-\ndimensional manifold with explicit structure, rather than in an unstructured tensor space. This\nopens the door to a more geometric understanding of reasoning in neural networks.\nThere are many directions for future work:\n• Global and long-range invariants. Develop sequence-level invariants of the Grassmann\nflow—e.g., averaged subspaces, curvature-like measures, or cross-layer stability statistics—and\ninject them as features or constraints on local mixing.\n• Richer Grassmann structures. Move beyond k = 2 subspaces, explore higher-dimensional\nsubspaces, and study regularizers that encourage smooth trajectories on Gr (k, r) across layers.\n15\n\n-- 15 of 17 --\n\n• Hybrid architectures. Combine Grassmann mixing with state-space models, kernelized\nattention, or convolutional modules to better balance local, global, and temporal information.\n• Interpretability studies. Systematically investigate correlations between Pl¨ucker coordi-\nnates, model behavior, and human-understandable patterns, with the goal of defining more\nstable invariants than raw attention maps.\n• Scaling and engineering. Implement fused Grassmann kernels and optimized GPU operators\nto realize the theoretical linear scaling in practice and test the architecture at larger scales\nand on more challenging reasoning benchmarks.\nIn summary, our results suggest that what we fundamentally need for strong sequence modeling is\nnot attention as such, but a principled way for representations to move on the manifolds they inhabit.\nGrassmann flows provide one concrete instantiation of this idea, and we hope they will encourage\nfurther exploration of geometric alternatives to attention in the design of neural architectures.\nReferences\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin, “Attention Is All You Need,” in Advances in Neural Information Processing\nSystems (NeurIPS), vol. 30, 2017.\n[2] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding,” in Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics (NAACL), pp. 4171–4186,\n2019.\n[3] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language Models are\nUnsupervised Multitask Learners,” OpenAI technical report, 2019.\n[4] T. Brown, B. Mann, N. Ryder et al., “Language Models are Few-Shot Learners,” Advances in\nNeural Information Processing Systems 33 (NeurIPS), 2020.\n[5] A. Dosovitskiy, L. Beyer, A. Kolesnikov et al., “An Image is Worth 16x16 Words: Transformers\nfor Image Recognition at Scale,” International Conference on Learning Representations (ICLR),\n2021.\n[6] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer Sentinel Mixture Models,”\narXiv:1609.07843, 2016.\n[7] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal et al., “Language Models\nare Few-Shot Learners,” in Advances in Neural Information Processing Systems (NeurIPS),\nvol. 33, pp. 1877–1901, 2020.\n[8] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer Sentinel Mixture Models,” in 5th\nInternational Conference on Learning Representations (ICLR), 2017. (WikiText-2 dataset.)\n[9] S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning, “A Large Annotated Corpus for\nLearning Natural Language Inference,” in Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pp. 632–642, 2015. (SNLI dataset.)\n16\n\n-- 16 of 17 --\n\n[10] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Kuvekas, and D. Tran, “Image\nTransformer,” in Proceedings of the 35th International Conference on Machine Learning (ICML),\npp. 4055–4064, 2018.\n[11] R. Hartshorne, Algebraic Geometry. Springer, 1977.\n[12] J. M. Lee, Introduction to Riemannian Manifolds, 2nd ed. Springer, 2018.\n[13] A. Edelman, T. A. Arias, and S. T. Smith, “The Geometry of Algorithms with Orthogonality\nConstraints,” SIAM Journal on Matrix Analysis and Applications, vol. 20, no. 2, pp. 303–353,\n1998.\n[14] P.-A. Absil, R. Mahony, and R. Sepulchre, Optimization Algorithms on Matrix Manifolds.\nPrinceton University Press, 2008.\n17\n\n-- 17 of 17 --\n\n",
    "analysis_json": "{\"title\":\"Attention Is Not What You Need: Grassmann Flows as an Attention-Free Alternative for Sequence Modeling\",\"summary\":\"This research paper challenges the assumption that self-attention is the indispensable primitive for sequence modeling. It frames attention as an opaque 'tensor lifting' mechanism that creates uninterpretable high-dimensional interactions. As an alternative, the author introduces 'Grassmann flows,' a geometric approach where token pairs are treated as 2D subspaces on a Grassmann manifold Gr(2, r). By embedding these subspaces using Pl ücker coordinates and fusing them through gated mixing, the model achieves linear scaling in sequence length and competitive performance on Wikitext-2 and SNLI benchmarks without any explicit attention weights.\",\"key_claims\":[\"Self-attention's uninterpretability stems from a high-dimensional tensor lift that lacks simple mathematical invariants.\",\"Sequence modeling can be effectively performed by controlled deformations of low-rank subspaces on a Grassmann manifold.\",\"Grassmann mixing scales linearly O(L) in sequence length for fixed rank, compared to the quadratic O(L^2) scaling of self-attention.\",\"Geometric features derived from Pl ücker embeddings can replace pairwise compatibility weights in reasoning tasks.\",\"A pure Grassmann-based language model achieves validation perplexities within 10–15% of size-matched Transformers.\"],\"simulation_possibilities\":[{\"title\":\"Pl ücker Coordinate Geometric Encoder\",\"description\":\"A simulation of the Grassmann encoding process where two low-dimensional vectors (reduced token states) are mapped to a 2D subspace. It visualizes how the resulting Pl ücker coordinates change based on the angle and magnitude of the input tokens.\",\"complexity\":\"Medium\",\"variables\":[\"reduced_dimension_r\",\"vector_theta_1\",\"vector_theta_2\",\"normalization_epsilon\"]},{\"title\":\"Multi-Scale Local Window Flow\",\"description\":\"An interactive sequence visualizer showing how information propagates through the hidden states via specific window offsets (e.g., 1, 2, 4, 8) instead of a dense attention matrix. It tracks the path of a single token's influence across layers.\",\"complexity\":\"Low\",\"variables\":[\"window_offsets\",\"sequence_length\",\"num_layers\",\"stride_size\"]},{\"title\":\"Gated Grassmann Fusion Dynamics\",\"description\":\"A prototype of the mixing block that allows users to adjust the gating parameter alpha. It demonstrates the balance between preserving original hidden state identity and incorporating geometric subspace information.\",\"complexity\":\"Medium\",\"variables\":[\"gate_bias\",\"feature_projection_scale\",\"dropout_probability\",\"hidden_dimension_d\"]},{\"title\":\"Asymptotic Complexity Benchmarker\",\"description\":\"A comparative simulation tool that plots memory and compute requirements for Grassmann Mixing versus Standard Attention as sequence length L increases towards infinity.\",\"complexity\":\"Low\",\"variables\":[\"max_sequence_length\",\"attention_heads\",\"grassmann_rank_r\",\"num_windows_m\"]}]}",
    "created_at": 1767024089070
  }
}